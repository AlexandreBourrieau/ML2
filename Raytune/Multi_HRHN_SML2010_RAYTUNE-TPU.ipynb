{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_HRHN_SML2010_RAYTUNE-TPU.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/Raytune/Multi_HRHN_SML2010_RAYTUNE-TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPK5y4fIYKJt"
      },
      "source": [
        "!pip install -q 'ray[tune]' 'ray[default]'\n",
        "!pip install -q --upgrade aioredis==1.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSRRDMXaK_YA"
      },
      "source": [
        "Les TPU sont des calculateurs qui sont différents du processeur local exécutant le programme Python. Pour travailler avec les TPU, il faut donc commencer par se connecter au cluster distant et les initialiser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Ce dataset est utilisé pour effectuer la prédiction de la température d'une pièce en fonction de plusieurs paramètres mesurés. La fréquence originale des données est d'une minute, puis a été modifiée à 15minutes avec un filtrage. L'ensemble correspond environ à une durée de 40 jours.  \n",
        "Nous allons utiliser ici la température de la chambre comme cible et sélectionner 18 séries exogènes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/SMLselected_VSURF_pred.csv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_etude = pd.read_csv(\"SMLselected_VSURF_pred.csv\")\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oElnLHPailF"
      },
      "source": [
        "df_etude = df_etude.drop(['Unnamed: 0'],axis=1)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INH5D4lncQRY"
      },
      "source": [
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igyc5qUTcdXo"
      },
      "source": [
        "Modifie les type en float32 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_0svvF8chHQ"
      },
      "source": [
        "df_etude = df_etude.astype(dtype='float32')\n",
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude.values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128                 \n",
        "longueur_sequence = 20\n",
        "longueur_sortie = 1\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "def Create_train(dataset):\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "\n",
        "  # Extrait les X,Y du dataset\n",
        "  x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                          # y=43x(BS,1,1)\n",
        "  for i in range(len(x)):\n",
        "    X1.append(x[i][0])          \n",
        "    X2.append(x[i][1])\n",
        "\n",
        "  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "  # Recombine les données\n",
        "  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "  x_train = [X1,X2]\n",
        "  y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "  return x_train,y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVbVOFDwERm-"
      },
      "source": [
        "x_train, y_train = Create_train(dataset)\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "def Create_val(dataset_val):\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "\n",
        "  # Extrait les X,Y du dataset\n",
        "  x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                          # y=43x(BS,1,1)\n",
        "  for i in range(len(x)):\n",
        "    X1.append(x[i][0])          \n",
        "    X2.append(x[i][1])\n",
        "\n",
        "  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "  # Recombine les données\n",
        "  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "  x_val = [X1,X2]\n",
        "  y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "  return x_val,y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTzC0c34Enxr"
      },
      "source": [
        "x_val,y_val = Create_val(dataset_val)\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZTF58ZnmpCo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle HRHN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBvAzS8KtFlp"
      },
      "source": [
        "Le modèle HRHN est décrit dans ce document de recherche : [Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction](https://arxiv.org/pdf/1806.00685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKq0mqg2ts2w"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Mod%C3%A8leHRHN1.png?raw=true' width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "**1. Création de l'encodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tn5xnfnuehY"
      },
      "source": [
        "L'encodeur a pour but de créer des représentations cachées des séries exogènes qui prennent en compte les relations spatiales entre ces séries ainsi que les relations temporelles.  \n",
        "Les relations spatiales sont extraitent à l'aide d'un ensemble de réseaux de convolution qui produisent des représentations w1, w2... w(T-1).  \n",
        "Ces représentations sont ensuites codées par un réseau RHN à 3 couches afin d'en extraire les relations temporelles. En sortie de ce réseau RHN, on extrait 3 tenseurs dont chacun contient les (T-1) états cachés de chaque couche du réseau RHN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0mq5BSauQUq"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-boowSGDp3"
      },
      "source": [
        "***a. Création des CNN parallèlisés***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dF5Cp9vzWB"
      },
      "source": [
        "La structure d'un réseau de convolution est composée de trois couches CNN-1D + Max-pooling :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jga5_ZzKv6CI"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN1.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CMXl2tgwJ76"
      },
      "source": [
        "L'intégration de caque réseau dans Keras est parallélisée :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtR-u7ZqwjTL"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ETqqvAIGKLi"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "\n",
        "class Encodeur_CNN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling,dim_motif):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  # Création de Tin réseaux de convolution + max_pooling en //\n",
        "  ############################################################\n",
        "  def build(self,input_shape):\n",
        "    convs = []\n",
        "    input_cnns = []\n",
        "\n",
        "    # Création des Tin entrées des réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "        input_cnns.append(tf.keras.Input(shape=(input_shape[2],1)))       # input = Tin*(batch_size,#dim,1)\n",
        "\n",
        "    # Création des Tin réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "      conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[0],      # conv : (batch_size,#dim,16)\n",
        "                                    kernel_size=self.dim_filtres_cnn[0],\n",
        "                                    activation='relu',\n",
        "                                    padding='same',\n",
        "                                    strides=1)(input_cnns[i])\n",
        "      conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[0],      # conv : (batch_size,#pooling1,16)\n",
        "                                       padding='same')(conv)\n",
        "      for n in range(1,len(self.dim_filtres_cnn)):\n",
        "        conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                      kernel_size=self.dim_filtres_cnn[n],\n",
        "                                      activation='relu',\n",
        "                                      padding='same',\n",
        "                                      strides=1)(conv)\n",
        "        conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                         padding='same')(conv)\n",
        "      convs.append(conv)\n",
        "    \n",
        "    # Création de la sortie concaténée des Tin réseaux CNN\n",
        "    out = tf.convert_to_tensor(convs)                                     # out : (Tin,batch_size,#pooling,64)\n",
        "    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,Tin,#pooling,64)\n",
        "    out = tf.keras.layers.Reshape(                                        # out : (batch_size,Tin,#pooling*64)\n",
        "        target_shape=(out.shape[1],out.shape[2]*out.shape[3]))(out)\n",
        "\n",
        "    if self.dim_motif == 0:\n",
        "      out = tf.keras.layers.Dense(units=out.shape[2])(out)                  # out : (batch_size,Tin,dim_motif = #pooling*64) \n",
        "    else:\n",
        "      out = tf.keras.layers.Dense(units=self.dim_motif)(out)                # out : (batch_size,Tin,dim_motif) \n",
        "\n",
        "    # Création du modèle global\n",
        "    self.conv_model = tf.keras.Model(inputs=input_cnns,outputs=out)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:  Entrée séries exogènes  : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     w:      Sorties des motifs CNN  : (batch_size,Tin,#dim_motif)\n",
        "  #                                       (taille dernier filtre=64)\n",
        "  def call(self, input):\n",
        "    # Coupes temporelles sur les séries exogènes\n",
        "    # au format : Tin*(batch_size,#dim,1)\n",
        "    input_list = []\n",
        "    for i in range(input.shape[1]):\n",
        "      input_list.append(tf.transpose(input[:,i:i+1,:],perm=[0,2,1]))      # (batch_size,#dim,1)\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.conv_model(input_list)                                       # (batch_size,Tin,dim_motif)\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_y68mmiRpR1"
      },
      "source": [
        "***b. Création des cellules RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FQ47zOsxHpx"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_RHN.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvNrS-wQ1B5C"
      },
      "source": [
        "On crée une cellule RHN en reprenant le code précédent auquel :  \n",
        "- On ajoute la possibilité de retourner tous les états cachés de chaque couche\n",
        "- On ajoute la prise en compte de la dimension d'entrée correspondant à la dimension des motifs en sortie des réseaux CNN (dim_motif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CygD9DXbBTDJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Structure_RHN4.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HldCwl9z3fY"
      },
      "source": [
        "class Cellule_RHN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_RHN, nbr_couches, return_all_states = False, dim_input=1):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches = nbr_couches\n",
        "    self.dim_input = dim_input\n",
        "    self.return_all_states = return_all_states\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wh = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wh\")       # (#dim, #RHN)\n",
        "    self.Wt = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wt\")       # (#dim, #RHN)\n",
        "    self.Wc = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wc\")       # (#dim, #RHN)\n",
        "\n",
        "    self.Rh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rh\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rt\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rc\")      # (n_couches,#RHN, #RHN)\n",
        "\n",
        "    self.bh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bh\")        # (n_couches,#RHN, 1)\n",
        "    self.bt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bt\")        # (n_couches,#RHN, 1)\n",
        "    self.bc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bc\")        # (n_couches,#RHN, 1)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "    # Initialisation des masques de dropout\n",
        "  def InitMasquesDropout(self,drop=0.0):\n",
        "    self.Wh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Rh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X[t]        : (batch_size,1,#dim)\n",
        "  #     init_hidden:    Etat caché Init.    : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     sL:             Etat caché de la dernière couche       : (batch_size,#RHN) \n",
        "  #           ou        Etats cachés de chaque couche SL[t]    : (batch_size,nbr_couches,#RHN)\n",
        "  def call(self, input, init_hidden=None):\n",
        "    # Construction d'un vecteur d'état nul si besoin\n",
        "    if init_hidden == None:\n",
        "      init_hidden = tf.matmul(tf.zeros(shape=(self.dim_RHN,input.shape[2])), # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "                              tf.transpose(input,perm=[0,2,1]))\n",
        "      init_hidden = tf.squeeze(init_hidden,-1)                               # (batch_size,#RHN,1) => (batch_size,#RHN)\n",
        "  \n",
        "    liste_sl = []                                                            # Liste pour  enregistrer les états cachés de chaque couche\n",
        "    # Calcul de hl, tl et cl\n",
        "    for i in range(self.nbr_couches):\n",
        "      if i==0:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[0,:,:],self.Rh[0,:,:])                      # (#RHN,1)_x_(#RHN,#RHN) = (#RHN,#RHN)\n",
        "        Rt = tf.multiply(self.Rt_[0,:,:],self.Rt[0,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[0,:,:],self.Rc[0,:,:])\n",
        "\n",
        "        Wh = tf.multiply(self.Wh_,self.Wh)                                    # (#dim,1)_x_(#dim,#RHN) = (#dim,#RHN)\n",
        "        Wt = tf.multiply(self.Wt_,self.Wt)\n",
        "        Wc = tf.multiply(self.Wc_,self.Wc)\n",
        "   \n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + tf.matmul(tf.transpose(Wh),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + tf.matmul(tf.transpose(Wt),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + tf.matmul(tf.transpose(Wc),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "\n",
        "      else:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[i,:,:],self.Rh[i,:,:])\n",
        "        Rt = tf.multiply(self.Rt_[i,:,:],self.Rt[i,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[i,:,:],self.Rc[i,:,:])\n",
        "\n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "      \n",
        "      # Calcul de sl\n",
        "      sl = tf.keras.layers.multiply([hl,tl])                                # (batch_size,#RHN)\n",
        "      sl = sl + tf.keras.layers.multiply([init_hidden,cl])                  # (batch_size,#RHN)\n",
        "      liste_sl.append(sl)       # Sauvegarde l'état caché de la couche courante\n",
        "      init_hidden = sl\n",
        "    if self.return_all_states == False:\n",
        "      return sl\n",
        "    else:\n",
        "      liste_sl = tf.convert_to_tensor(liste_sl)                             # (nbr_couches,batch_size,#RHN)\n",
        "      liste_sl = tf.transpose(liste_sl,perm=[1,0,2])                        # (batch_size,nbr_couches,#RHN)\n",
        "      return liste_sl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJY-TY7W55ZF"
      },
      "source": [
        "***c. Création de l'encodeur : Convolutions + RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMT8C8-UVe2O"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYQu67IfBdel"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "#   dim_motif         :   dimension du motif en sortie du CNN\n",
        "#   dim_RHN           :   dimension du vecteur caché RHN\n",
        "#   nbr_couches_RHN   :   nombre de couches du RHN\n",
        "#   dropout           :   dropout variationnel pour le RHN ex: [0.1]\n",
        "\n",
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling, dim_motif,dim_RHN,nbr_couches_RHN, dropout=0.0):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches_RHN = nbr_couches_RHN\n",
        "    self.dropout = dropout\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.encodeur_cnn = Encodeur_CNN(dim_filtres_cnn=self.dim_filtres_cnn,nbr_filtres_cnn=self.nbr_filtres_cnn,dim_max_pooling=self.dim_max_pooling,dim_motif=self.dim_motif)\n",
        "    self.RHN = Cellule_RHN(dim_RHN=self.dim_RHN,nbr_couches=self.nbr_couches_RHN,return_all_states=True,dim_input=self.dim_motif)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:          Entrées X         : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     hidden_states   Vecteurs cachés   : (batch_size,nbr_couches,Tin,#RHN)\n",
        "  def call(self, input):\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.encodeur_cnn(input)      #  (batch_size,Tin,dim_motif)\n",
        "\n",
        "    # Encodage des motifs CNN avec les cellules RHN\n",
        "    sequence = []\n",
        "    hidden = None\n",
        "\n",
        "    # Initialisation des masques de dropout pour tous les pas de temps\n",
        "    self.RHN.InitMasquesDropout(self.dropout)\n",
        "\n",
        "    # Applique la cellule RHN à chaque pas de temps\n",
        "    for i in range(input.shape[1]):\n",
        "      hidden = self.RHN(w[:,i:i+1,:],hidden)          # Envoie (batch_size,1,dim_motif)\n",
        "      sequence.append(hidden)                         # Sauve (batch_size,nbr_couches,#RHN)\n",
        "\n",
        "      # Le premier état caché du prochain instant\n",
        "      # est l'état caché de la dernière couche précédente\n",
        "      hidden = hidden[:,self.nbr_couches_RHN-1,:]       # (batch_size,#RHN)\n",
        "\n",
        "    # Traite le format des vecteurs cachés de l'encodeur\n",
        "    sequence = tf.convert_to_tensor(sequence)               # (Tin,batch_size,nbr_couches,#RHN)\n",
        "    hidden_states = tf.transpose(sequence,perm=[1,2,0,3])   # (batch_size,nbr_couches,Tin,#RHN)  \n",
        "\n",
        "    return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__CJ4O7yJne3"
      },
      "source": [
        "**2. Création du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2yWQeaJwNn"
      },
      "source": [
        "Le décodeur prend en entrée et à chaque pas de temps :  \n",
        "- Le tenseur en sortie de l'encodeur RHN qui contient l'ensemble des vecteurs cachés des différentes couches : (batch_size,Nbr_couches,Tin,#RHN)\n",
        "- L'état caché de la dernière couche du décodeur RHN précédent : (batch_size,#RHN)\n",
        "- La valeur de la série cible à l'instant courant : (batch_size,1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtYLxAoIK8Xn"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_VueEnsembleDecodeur2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjHiRZZbLief"
      },
      "source": [
        "**a. Création de la couche d'attention hiérarchique**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JX5hGeWNN8w"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_AttentionHierarchique.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9p7ylHmY6gS"
      },
      "source": [
        "On commence par créer la fonction permettant de calculer les scores. Cette fonction sera appelée avec la méthode TimeDistributed de Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvaGAb0uY1XL"
      },
      "source": [
        "class CalculScore(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.T = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"T\")            # (#RHN, #RHN)\n",
        "    self.U = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"U\")            # (#RHN, #RHN)\n",
        "    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"b\")                         # (#RHN, 1)\n",
        "    self.v = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"v\")                         # (#RHN, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  #     hid_state:  Etat initial RHN          : (batch_size,#RHN)\n",
        "  def SetInitState(self,hid_state):\n",
        "    self.hid_state = hid_state\n",
        "\n",
        "  def compute_output_shape(self,input_shape):\n",
        "    return(input_shape[0],1)\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      1 sortie encodeur RHN     : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     score:      score                     : (batch_size,1,1)\n",
        "  def call(self, input):\n",
        "    score = tf.matmul(self.U,tf.expand_dims(input,-1))                      # (#RHN,#RHN)x(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "    score = score + tf.matmul(self.T,tf.expand_dims(self.hid_state,-1))     # (batch_size,#RHN,1)\n",
        "    score = score + self.b                                                  # (batch_size,#RHN,1)\n",
        "    score = K.tanh(score)\n",
        "    score = tf.matmul(tf.transpose(self.v),score)                           # (1,#RHN)x(batch_size,#RHN,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                             # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pF02ysdbxWU"
      },
      "source": [
        "On crée maintenant la couche d'attention hiérarchique :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kxnR9fSVXDC"
      },
      "source": [
        "class AttentionHierarchique(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_score = CalculScore()\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:      Sorties d'une couche encodeur RHN       : (batch_size,Tin,#RHN)\n",
        "  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     vc:         SousVecteur contexte                    : (batch_size,1,RHN)\n",
        "  def call(self, input, hid_state):\n",
        "    # Calcul des scores\n",
        "    self.couche_score.SetInitState(hid_state)\n",
        "    scores = tf.keras.layers.TimeDistributed(self.couche_score)(input)        # (batch_size,Tin,#RHN) : Timestep = Tin\n",
        "                                                                              # (batch_size,#RHN) envoyé Tin fois\n",
        "                                                                              # (batch_size,Tin,1) retourné\n",
        "    scores = tf.keras.activations.softmax(scores,axis=1)                      # (batch_size,Tin,1)\n",
        "\n",
        "    # Applique les scores aux sorties de la couche RHN\n",
        "    poids = tf.multiply(input,scores)             # (batch_size,Tin,#RHN)_x_(batch_size,Tin,1) = (batch_size,Tin,#RHN)\n",
        "\n",
        "    # Calcul le sous-vecteur contexte\n",
        "    vc = K.sum(poids,axis=1)                      # (batch_size,#RHN)\n",
        "    return tf.expand_dims(vc,1)                   # (batch_size,1,#RHN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slCSUTmyifEY"
      },
      "source": [
        "**b. Création du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_efbOikfRwt"
      },
      "source": [
        "Dans le décodeur, on parallélise autant de couches d'attention que nécessaire afin de créer un modèle d'attention multi-entrées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCElCxBcnUHj"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ParaDecodeur.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2nZG3Rrjv3O"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_RHN,nbr_couches_RHN,dropout=0.0):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches_RHN = nbr_couches_RHN\n",
        "    self.dropout = dropout\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    attentions = []\n",
        "    inputs_attention = []\n",
        "\n",
        "    # Création des \"nbr_couches\" entrées des attentions\n",
        "    # Chaque entrée est une liste : [input,init_state] = [((batch_size,Tin,#RHN)),((batch_size,#RHN))]\n",
        "    for i in range(input_shape[1]):\n",
        "      inputs_attention.append([tf.keras.Input(shape=(input_shape[2],input_shape[3])),          # input = \"nbr_couches\"*(batch_size,Tin,#RHN)\n",
        "                                 tf.keras.Input(shape=(input_shape[3]))])                      # init_state = \"nbr_couches\"*(batch_size,#RHN)\n",
        "\n",
        "    # Création des \"nbr_couches\" couches d'attentions hierarchiques\n",
        "    for i in range(input_shape[1]):\n",
        "      att = AttentionHierarchique()(inputs_attention[i][0],                 # inputs_attention[i][0] : (batch_size,Tin,#RHN)\n",
        "                                    inputs_attention[i][1])                 # inputs_attention[i][1] : (batch_size,#RHN)\n",
        "      attentions.append(att)\n",
        "\n",
        "    # Création de la sortie concaténée des \"nbr_couches\" couches d'attentions\n",
        "    out = tf.convert_to_tensor(attentions)                                # out : (nbr_couches,batch_size,1,#RHN)\n",
        "    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,nbr_couches,1,#RHN)\n",
        "\n",
        "    # Création du modèle global\n",
        "    self.att_model = tf.keras.Model(inputs=inputs_attention,outputs=out)\n",
        "\n",
        "    # Création des poids\n",
        "    self.Wtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n",
        "    self.Vtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n",
        "\n",
        "    # Création du décodeur RHN\n",
        "    self.dec_RHN = Cellule_RHN(dim_RHN=self.dim_RHN,nbr_couches=self.nbr_couches_RHN,return_all_states=False,dim_input=1)\n",
        "   \n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:      Sorties des couches de l'encodeur RHN   : (batch_size,nbr_couches,Tin,#RHN)\n",
        "  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN)\n",
        "  #     Y:          Valeur de la série cible                : (batch_size,1)\n",
        "  #     only_att    Si =True ne calcul que le vecteur ctx   : True/False\n",
        "  # Sorties :\n",
        "  #     d:          Vecteur contexte                        : (batch_size,nbr_couches*RHN)\n",
        "  #     s:          Vecteur caché décodeur RHN              : (batch_size,#RHN)\n",
        "  def call(self, input, hid_state, Y,only_att):\n",
        "    # Initialisation de l'état caché à 0 si besoin\n",
        "    # Construit le tenseur nul au format (batch_size,#RHN)\n",
        "    if hid_state == None:\n",
        "      coef = tf.expand_dims(input[:,0,0,0],-1)                        # (batch_size,1)\n",
        "      coef = tf.expand_dims(coef,-1)                                  # (batch_size,1,1)\n",
        "      hid_state = tf.matmul(coef,tf.zeros(shape=(1,input.shape[3])))  # (batch_size,1,1)X(1,#RHN) = (batch_size,1,#RHN)\n",
        "      hid_state = tf.squeeze(hid_state,axis=1)                        # (batch_size,#RHN)\n",
        "\n",
        "    # Construction de l'entrée du modèle\n",
        "    # nbr_couches*[((batch_size,Tin,#RHN)),((batch_size,#RHN))]\n",
        "    input_model = []\n",
        "    for i in range(input.shape[1]):\n",
        "      input_model.append([input[:,i,:,:],hid_state])    # [((batch_size,Tin,#RHN)),((batch_size,#RHN))]\n",
        "    \n",
        "    # Calcul des sous-vecteurs contextes\n",
        "    # avec le modèle d'attention hiérarchique parallélisé\n",
        "    d = self.att_model(input_model)                     # d : (batch_size,nbr_couches,1,#RHN)\n",
        "\n",
        "    # Concaténation des sous-vecteurs contextes\n",
        "    d = tf.squeeze(d,axis=2)                            # (batch_size,nbr_couches,#RHN)\n",
        "    d = tf.keras.layers.Flatten()(d)                    # (batch_size,nbr_couches*RHN)\n",
        "\n",
        "    if only_att == False :\n",
        "      # Calcul de y_tilda\n",
        "      ytilda = self.Wtilda(Y)                             # (batch_size,1)\n",
        "      ytilda = ytilda + self.Vtilda(d)                    # (batch_size,1)\n",
        "\n",
        "      # Initialisation des masques de dropout pour tous les pas de temps\n",
        "      self.dec_RHN.InitMasquesDropout(self.dropout)\n",
        "\n",
        "      # Décodage avec le réseau RHN\n",
        "      s = self.dec_RHN(tf.expand_dims(ytilda,-1),hid_state)                  # (batch_size,#RHN)\n",
        "      return d,s\n",
        "    else:\n",
        "      return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYOTdM7fZT65"
      },
      "source": [
        "**3. Création de la couche HRHN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhML2b5bFsZB"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Gene_HRHN.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PCEHUDEZ1bt"
      },
      "source": [
        "class Net_HRHN(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, dim_RHN, regul=0.0, drop = 0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    self.dim_RHN = dim_RHN\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n",
        "    self.V = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     output_seq:     Sortie séquence Y   : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction Y        : (batch_size,1,1)\n",
        "  def call(self,input,output_seq):\n",
        "    # Appel de l'encodeur\n",
        "    # Récupère l'ensemble des états cachés de l'encodeur RHN\n",
        "    H = self.encodeur(input)                # (batch_size,nbr_couches,Tin,#RHN)\n",
        "\n",
        "    # Décodage\n",
        "    hidden_state = None\n",
        "    for t in range(input.shape[1]):\n",
        "      vc, hidden_state = self.decodeur(H,hidden_state,output_seq[:,t:t+1,0],only_att = False)\n",
        "    \n",
        "    # Couche d'attention finale\n",
        "    vc = self.decodeur(H,hidden_state,output_seq[:,0,0],only_att=True)\n",
        "\n",
        "    # Génération de la prédiction\n",
        "    sortie = self.W(hidden_state) + self.V(vc)        # (batch_size,1)\n",
        "    return tf.expand_dims(sortie,-1)                  # (batch_size,1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj12PHWgPTjD"
      },
      "source": [
        "# Mise en place du modèle HRHN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlAMNrC2W935"
      },
      "source": [
        "def get_model(config):\n",
        "  dim_cnn_ = config['dim_filtres_cnn']\n",
        "  nbr_cnn_ = config['nbr_filtres_cnn']\n",
        "  max_pool_ = config['max_pooling_cnn']\n",
        "\n",
        "  dim_cnn=[]\n",
        "  nbr_cnn=[]\n",
        "  max_pool=[]\n",
        "\n",
        "  val = dim_cnn_[0].split(',')\n",
        "  for c in val:\n",
        "    dim_cnn.append(int(c))\n",
        "    \n",
        "  val = nbr_cnn_[0].split(',')\n",
        "  for c in val:\n",
        "    nbr_cnn.append(int(c))\n",
        "\n",
        "  val = max_pool_[0].split(',')\n",
        "  for c in val:\n",
        "    max_pool.append(int(c))\n",
        "\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(config['longueur_sequence'],x_train[0].shape[2]))\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(config['longueur_sequence'],1))\n",
        "\n",
        "  dim_motif = Encodeur_CNN(dim_filtres_cnn=dim_cnn,nbr_filtres_cnn=nbr_cnn,dim_max_pooling=max_pool,dim_motif=0)(x_train[0][0:1,:,:]).shape[2]\n",
        "\n",
        "  encodeur = Encodeur(dim_filtres_cnn=dim_cnn,nbr_filtres_cnn=nbr_cnn,dim_max_pooling=max_pool,dim_motif=dim_motif,dim_RHN=config['dim_RHN'],nbr_couches_RHN=config['nbr_couches_RHN'],dropout=config['drop'])\n",
        "  decodeur = Decodeur(dim_RHN=config['dim_RHN'],nbr_couches_RHN=config['nbr_couches_RHN'],dropout=config['drop'])\n",
        "\n",
        "  sortie = Net_HRHN(encodeur,decodeur,longueur_sequence=config['longueur_sequence'],drop=config['drop'],dim_RHN=config['dim_RHN'])(entrees_sequences,sorties_sequence)\n",
        "\n",
        "  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdSoHUDnkpgu"
      },
      "source": [
        "# Configuration de l'optimiseur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQdgXWaTkx0w"
      },
      "source": [
        "**1. Espace des hyperparamètres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhHTHSdgnEEP"
      },
      "source": [
        "liste_dim_filtres_cnn = [\"1\",\"2\",\"3\",\"4\"]\n",
        "liste_nbr_filtres_cnn = [\"2\",\"4\",\"8\",\"16\",\"32\",\"64\",\"128\",\"256\"]\n",
        "liste_rapport_max_pooling_cnn = [\"1\",\"2\",\"3\",\"4\"]\n",
        "\n",
        "def create_filtres_x2():\n",
        "  dim_filtres_cnn_x2 = []\n",
        "  nbr_filtres_cnn_x2 = []\n",
        "  liste_rapport_max_pooling_cnn_x2 = []\n",
        "\n",
        "  for i in liste_dim_filtres_cnn:\n",
        "    for j in liste_dim_filtres_cnn:\n",
        "      dim_filtres_cnn_x2.append([i,j])\n",
        "  for i in liste_nbr_filtres_cnn:\n",
        "    for j in liste_nbr_filtres_cnn:\n",
        "      nbr_filtres_cnn_x2.append([i,j])\n",
        "  for i in liste_rapport_max_pooling_cnn:\n",
        "    for j in liste_rapport_max_pooling_cnn:\n",
        "      liste_rapport_max_pooling_cnn_x2.append([i,j])\n",
        "  \n",
        "  return dim_filtres_cnn_x2,nbr_filtres_cnn_x2,liste_rapport_max_pooling_cnn_x2\n",
        "\n",
        "def create_filtres_x3():\n",
        "  dim_filtres_cnn_x3 = []\n",
        "  nbr_filtres_cnn_x3 = []\n",
        "  liste_rapport_max_pooling_cnn_x3 = []\n",
        "\n",
        "  for i in liste_dim_filtres_cnn:\n",
        "    for j in liste_dim_filtres_cnn:\n",
        "      for k in liste_dim_filtres_cnn:\n",
        "        dim_filtres_cnn_x3.append([i,j,k])\n",
        "  for i in liste_nbr_filtres_cnn:\n",
        "    for j in liste_nbr_filtres_cnn:\n",
        "      for k in liste_nbr_filtres_cnn:\n",
        "        nbr_filtres_cnn_x3.append([i,j,k])\n",
        "  for i in liste_rapport_max_pooling_cnn:\n",
        "    for j in liste_rapport_max_pooling_cnn:\n",
        "      for k in liste_rapport_max_pooling_cnn:\n",
        "        liste_rapport_max_pooling_cnn_x3.append([i,j,k])\n",
        "  \n",
        "  return dim_filtres_cnn_x3,nbr_filtres_cnn_x3,liste_rapport_max_pooling_cnn_x3\n",
        "\n",
        "def create_liste_filtres():\n",
        "  all_dim_filtres_cnn = [[],[]]\n",
        "  all_nbr_filtres_cnn = [[],[]]\n",
        "  all_max_pooling_cnn = [[],[]]\n",
        "\n",
        "  liste_dim_filtres_x2,liste_nbr_filtres_x2,liste_rapport_max_pooling_cnn_x2 = create_filtres_x2()\n",
        "  liste_dim_filtres_x3,liste_nbr_filtres_x3,liste_rapport_max_pooling_cnn_x3 = create_filtres_x3()\n",
        "\n",
        "  for i in liste_dim_filtres_x2:\n",
        "      all_dim_filtres_cnn[0].append(i)\n",
        "\n",
        "  for i in liste_dim_filtres_x3:\n",
        "      all_dim_filtres_cnn[1].append(i)\n",
        "\n",
        "  for i in liste_nbr_filtres_x2:\n",
        "      all_nbr_filtres_cnn[0].append(i)\n",
        "\n",
        "  for i in liste_nbr_filtres_x3:\n",
        "      all_nbr_filtres_cnn[1].append(i)\n",
        "\n",
        "  for i in liste_rapport_max_pooling_cnn_x2:\n",
        "      all_max_pooling_cnn[0].append(i)\n",
        "\n",
        "  for i in liste_rapport_max_pooling_cnn_x3:\n",
        "      all_max_pooling_cnn[1].append(i)\n",
        "\n",
        "\n",
        "  return all_dim_filtres_cnn,all_nbr_filtres_cnn,all_max_pooling_cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx-aUxmEnGKi"
      },
      "source": [
        "all_dim_filtres_cnn = [[],[]]\n",
        "all_nbr_filtres_cnn = [[],[]]\n",
        "all_max_pooling_cnn = [[],[]]\n",
        "\n",
        "\n",
        "all_dim_filtres_cnn_, all_nbr_filtres_cnn_ , all_max_pooling_cnn_= create_liste_filtres()\n",
        "\n",
        "val_str = \"\"\n",
        "\n",
        "for i in all_dim_filtres_cnn_[0]:\n",
        "  for j in i:\n",
        "    val_str = val_str+str(j)+\",\"\n",
        "  val_str = val_str[:-1]\n",
        "  all_dim_filtres_cnn[0].append(val_str)\n",
        "  val_str = \"\"\n",
        "\n",
        "for i in all_nbr_filtres_cnn_[0]:\n",
        "  for j in i:\n",
        "    val_str = val_str+j+\",\"\n",
        "  val_str = val_str[:-1]\n",
        "  all_nbr_filtres_cnn[0].append(val_str)\n",
        "  val_str = \"\"\n",
        "\n",
        "for i in all_dim_filtres_cnn_[1]:\n",
        "  for j in i:\n",
        "    val_str = val_str+str(j)+\",\"\n",
        "  val_str = val_str[:-1]\n",
        "  all_dim_filtres_cnn[1].append(val_str)\n",
        "  val_str = \"\"\n",
        "\n",
        "for i in all_nbr_filtres_cnn_[1]:\n",
        "  for j in i:\n",
        "    val_str = val_str+j+\",\"\n",
        "  val_str = val_str[:-1]\n",
        "  all_nbr_filtres_cnn[1].append(val_str)\n",
        "  val_str = \"\"\n",
        "\n",
        "for i in all_max_pooling_cnn_[0]:\n",
        "  for j in i:\n",
        "    val_str = val_str+str(j)+\",\"\n",
        "  val_str = val_str[:-1]\n",
        "  all_max_pooling_cnn[0].append(val_str)\n",
        "  val_str = \"\"\n",
        "\n",
        "for i in all_max_pooling_cnn_[1]:\n",
        "  for j in i:\n",
        "    val_str = val_str+str(j)+\",\"\n",
        "  val_str = val_str[:-1]\n",
        "  all_max_pooling_cnn[1].append(val_str)\n",
        "  val_str = \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qWo0E2ynILr"
      },
      "source": [
        "all_nbr_filtres_cnn[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r8pgY-FBm8l"
      },
      "source": [
        "from ray import tune\n",
        "\n",
        "def create_search_space():\n",
        "  config = {\n",
        "      \"longueur_sequence\": tune.choice([3,4,5,6,7,8,9,10,15,20,25,30]),\n",
        "      \"dim_filtres_cnn\": tune.choice(all_dim_filtres_cnn[1][:]),\n",
        "      \"nbr_filtres_cnn\": tune.choice(all_nbr_filtres_cnn[1][:]),\n",
        "      \"max_pooling_cnn\": tune.choice(all_max_pooling_cnn[1][:]),\n",
        "      \"dim_RHN\": tune.choice([8,16,32,64,128,256]),\n",
        "      \"nbr_couches_RHN\": tune.choice([1,2,3,4]),\n",
        "      \"drop\": tune.choice([0.0,0.01,0.1,0.3,0.6]),\n",
        "      \"batch_size\": tune.choice([128,256,512]),\n",
        "      \"lr\": tune.loguniform(1e-4, 1e-1)\n",
        "      }\n",
        "  \n",
        "  initial_best_config = [{\n",
        "      \"longueur_sequence\":20,\n",
        "      \"dim_filtres_cnn\": \"3,3,3\",\n",
        "      \"nbr_filtres_cnn\": \"16,32,64\",\n",
        "      \"max_pooling_cnn\": \"3,3,3\",\n",
        "      \"dim_RHN\": 64,\n",
        "      \"nbr_couches_RHN\": 3,\n",
        "      \"drop\": 0.0,\n",
        "      \"batch_size\": 128,\n",
        "      \"lr\": 1e-3\n",
        "      }]\n",
        "\n",
        "  return config,initial_best_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylc12PPtPGIv"
      },
      "source": [
        "class TuneReporter(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, reporter=None, freq=\"batch\", logs=None):\n",
        "        self.iteration = 0\n",
        "        logs = logs or {}\n",
        "        if freq not in [\"batch\", \"epoch\"]:\n",
        "            raise ValueError(\"{} not supported as a frequency.\".format(freq))\n",
        "        self.freq = freq\n",
        "        super(TuneReporter, self).__init__()\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        from ray import tune\n",
        "        logs = logs or {}\n",
        "        if not self.freq == \"batch\":\n",
        "            return\n",
        "        self.iteration += 1\n",
        "        for metric in list(logs):\n",
        "            if \"loss\" in metric and \"neg_\" not in metric:\n",
        "                logs[\"neg_\" + metric] = -logs[metric]\n",
        "        if \"acc\" in logs:\n",
        "            tune.report(keras_info=logs, mean_accuracy=logs[\"acc\"])\n",
        "        else:\n",
        "            tune.report(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"))\n",
        "    \n",
        "    def on_epoch_end(self, batch, logs=None):\n",
        "        from ray import tune\n",
        "        logs = logs or {}\n",
        "        if not self.freq == \"epoch\":\n",
        "            return\n",
        "        self.iteration += 1\n",
        "        for metric in list(logs):\n",
        "            if \"loss\" in metric and \"neg_\" not in metric:\n",
        "                logs[\"neg_\" + metric] = -logs[metric]\n",
        "        if \"acc\" in logs:\n",
        "            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs[\"acc\"])\n",
        "        else:\n",
        "            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs.get(\"accuracy\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1llAfcqwOjpR"
      },
      "source": [
        "def create_callbacks():\n",
        "    callbacks = []\n",
        "    tune_reporter = TuneReporter(freq=\"epoch\")\n",
        "    callbacks.append(tune_reporter)\n",
        "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='loss', patience=150))\n",
        "    return callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W2wP-rtLIG4"
      },
      "source": [
        "def train_model(config,):\n",
        "  dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], config['longueur_sequence'],longueur_sortie,config['batch_size'],shift)\n",
        "  dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],config['longueur_sequence'],longueur_sortie,config['batch_size'],shift)\n",
        "  x_train, y_train = Create_train(dataset)\n",
        "  x_val, y_val = Create_train(dataset_val)\n",
        "  \n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "  with strategy.scope():\n",
        "    model = get_model(config)\n",
        "    model.build(input_shape=([(int(config['batch_size']/8),config['longueur_sequence'],x_train[0].shape[2]),(int(config['batch_size']/8),config['longueur_sequence'],1)]))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config['lr']),loss='mse')\n",
        "  \n",
        "  callbacks = create_callbacks()\n",
        "  history = model.fit(x=[x_train[0],x_train[1]],y=y_train,epochs=1000,callbacks=callbacks,validation_data=([x_val[0],x_val[1]],y_val),batch_size=config['batch_size'])\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw6VpKTZYzkx"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/googledrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXAsjyGYbRr"
      },
      "source": [
        "from ray.tune import Callback\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "class SendFileToGoogleDrive(Callback):\n",
        "  def on_trial_complete(self,iteration,trials,trial,**info):\n",
        "    os.system(\"zip -r /content/googledrive/MyDrive/RayTuneHRHN_SML2010.zip /content/ray_results\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXnzuTH0uiZU"
      },
      "source": [
        "# Téléchargement des résultats précédents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHhMs-fd_ZKW"
      },
      "source": [
        "results_dir = os.path.abspath(\"ray_results\")\n",
        "\n",
        "!rm -r \"/content/ray_results\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XvAMmojZrTg"
      },
      "source": [
        "os.system(\"unzip /content/googledrive/MyDrive/RayTuneHRHN_SML2010.zip -d /\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl-0TUJeloJC"
      },
      "source": [
        "# Lancement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6b5eCeMorVe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/ray_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrdnOWfFaIwb"
      },
      "source": [
        "results_dir = os.path.abspath(\"ray_results\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiRfpaXTB_YM"
      },
      "source": [
        "import ray\n",
        "import os\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.suggest import ConcurrencyLimiter\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "\n",
        "ray.init(configure_logging=False,ignore_reinit_error=True)\n",
        "config, initial_best_config = create_search_space()\n",
        "\n",
        "scheduler = AsyncHyperBandScheduler(time_attr='training_iteration',metric=\"val_loss\",mode=\"min\",grace_period=200,max_t=1000)\n",
        "search_alg = HyperOptSearch(metric=\"val_loss\",mode=\"min\",points_to_evaluate=initial_best_config)\n",
        "search_alg = ConcurrencyLimiter(search_alg, max_concurrent=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRa7aHc7lvV-"
      },
      "source": [
        "analysis = tune.run(train_model, \n",
        "                    local_dir=results_dir,\n",
        "                    name=\"RayTuneHRHN_SML2010\",\n",
        "                    verbose=0,\n",
        "                    num_samples=10000,\n",
        "                    scheduler=scheduler,\n",
        "                    search_alg=search_alg,\n",
        "                    raise_on_failed_trial=False,\n",
        "                    resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
        "                    config=config,\n",
        "                    checkpoint_freq = 1,\n",
        "                    checkpoint_at_end=True,\n",
        "#                    resume=True,\n",
        "                    callbacks=[SendFileToGoogleDrive()])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}