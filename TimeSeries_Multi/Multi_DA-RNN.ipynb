{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_DA-RNN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/TimeSeries_Multi/Multi_DA-RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Le dataset utilisé contient les prix des stocks des 81 principales companies du NASDAQ100. La valeur de l'index du NASDAQ est utilisé comme cible.  \n",
        "La fréquence des information est par minute, depuis le 26 juillet 2016 jusqu'au 22 décembre 2016, soit 105 jours au total (les samedi et dimanche ne sont pas comptés, ainsi que le 25 novembre qui ne possède que 210 données et le 22 décembre qui n'en possède que 180)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/nasdaq100_padding.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_etude = pd.read_csv(\"nasdaq100_padding.csv\",dtype=\"float32\")\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INH5D4lncQRY"
      },
      "source": [
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['NDX'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLPlWzLJlNhA"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02JYhFxflROo"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude.values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6HRdVZmRM6"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JhXRi2DnAY"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19_-vTkFg3dg"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0pc1lYwf3p5"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44PYyswAlXFI"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoiHptoQyTxL"
      },
      "source": [
        "**1. Tests**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQQTRH_5wjrc"
      },
      "source": [
        "X1 = np.linspace(1,100,100)\n",
        "X2 = np.linspace(101,200,100)\n",
        "X3 = np.linspace(201,300,100)\n",
        "Y = np.linspace(301,400,100)\n",
        "\n",
        "X1 = tf.expand_dims(X1,-1)\n",
        "X2 = tf.expand_dims(X2,-1)\n",
        "X3 = tf.expand_dims(X3,-1)\n",
        "Y = tf.expand_dims(Y,-1)\n",
        "\n",
        "Serie_X = tf.concat([X1,X2,X3],axis=1)\n",
        "Serie_Y = Y\n",
        "print(Serie_X.shape)\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),((X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT-1)}\n",
        "# Y = YT\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence-1][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "test_dataset = prepare_dataset_XY(Serie_X,Serie_Y,10,1,1,1)\n",
        "\n",
        "print(len(list(test_dataset.as_numpy_iterator())))\n",
        "for element in test_dataset.take(1):\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLEjFxNnyWse"
      },
      "source": [
        "**2. Préparation des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT-1)}\n",
        "# Y = YT\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence-1][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 512\n",
        "longueur_sequence = 10\n",
        "longueur_sortie = 1\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT-1)\n",
        "  print(element[1].shape)               # YT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT-1\n",
        "  print(element[1].shape)               # YT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train = [X1,X2]\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_val = [X1,X2]\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9aPVTcDn6TQ"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(np.linspace(0,longueur_sequence,longueur_sequence),x_train[0][0,:,0:3],label=\"X_train (X)\")\n",
        "ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence-1),x_train[1][0,:,:],label=\"X_train (Y)\")\n",
        "\n",
        "ax.plot(np.linspace(longueur_sequence,longueur_sequence+1,1),y_train[0,:,:],label=\"Y_train\",marker=\"*\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW5Q7ElFKhbR"
      },
      "source": [
        "# Création du modèle DA-RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvo4sAcIKhbS"
      },
      "source": [
        "**1. Création de la couche d'encodeur à base de LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KxDejuzKhbS"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Encodeur\")\n",
        "    self.We = self.add_weight(shape=(input_shape[1],2*self.dim_LSTM),initializer=\"normal\",name=\"We\")    # (Tin, 2x#LSTM)\n",
        "    self.Ue = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"Ue\")     # (Tin, Tin)\n",
        "    self.be = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"be\")                  # (Tin, 1)\n",
        "    self.ve = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"ve\")                  # (Tin, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  #     index:          index série         : (1)\n",
        "  # Sorties :\n",
        "  #     out_hid : Sortie vecteur caché      : (batch_size,#LSTM)\n",
        "  #     out_cell: Sortie cell state         : (btach_size,#LSTM)\n",
        "  def call(self, input, hidd_state, cell_state, index):\n",
        "    # Calcul des poids normalisés\n",
        "    scores = []\n",
        "    for i in range(input.shape[2]):\n",
        "      if hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([hidd_state,cell_state],axis=1)        # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.We,hs)                                               # (Tin,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,Tin,1)\n",
        "        e = e + tf.matmul(self.Ue,tf.expand_dims(input[:,:,i],-1))              # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.be                                                         # (batch_size,Tin,1)\n",
        "      else:\n",
        "        e = tf.matmul(self.Ue,tf.expand_dims(input[:,:,i],-1))                  # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.be                                                         # (batch_size,Tin,1)\n",
        " \n",
        "      e = K.tanh(e)\n",
        "      scores.append(tf.matmul(tf.transpose(self.ve),e))                       # (1,Tin)x(batch_size,Tin,1) = (batch_size,1,1)\n",
        "    \n",
        "    a = tf.convert_to_tensor(scores)                                          # (#dim,batch_size,1,1)\n",
        "    a = tf.keras.activations.softmax(a,axis=0)                                # (#dim,batch_size,1,1)\n",
        "    a = tf.squeeze(a,-1)                                                      # (#dim,batch_size,1)\n",
        "    a = tf.transpose(a,perm=[1,0,2])                                          # (batch_size,#dim,1)\n",
        "\n",
        "    # Applique les poids normalisés à la série\n",
        "    x_tilda = tf.multiply(tf.expand_dims(input[:,index,:],-1),a)              # (batch_size,#dim,1) _x_ (batch_size,#dim,1) = (batch_size,#dim,1)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,1,#dim)\n",
        "\n",
        "    # Applique x_tilda à la cellule LSTM\n",
        "    out_dec, out_hid, out_cell = self.couche_LSTM(x_tilda)                    # out_dec et out_cell : (batch_size,#LSTM)\n",
        "\n",
        "    return out_hid, out_cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csuz7pbm1Q_Q"
      },
      "source": [
        "out, hid, cell = tf.keras.layers.LSTM(units=128,return_sequences=False,return_state=True)(tf.expand_dims(x_train[0][:,:,0],1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUDOCLZF33IH"
      },
      "source": [
        "tf.keras.layers.concatenate([hid,cell],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGhfKlEw3LeB"
      },
      "source": [
        "hid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFqAwllb0spp"
      },
      "source": [
        "Encodeur(dim_LSTM=128)(x_train[0],hid,cell,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEeMjICjyvRw"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDLNIs1y8V3"
      },
      "source": [
        "x_train[0][:,0,:].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mDkx8SuKhbT"
      },
      "source": [
        "**2. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7WjKKsuKhbT"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Decodeur\")\n",
        "    self.Wd = self.add_weight(shape=(self.dim_LSTM,2*self.dim_LSTM),initializer=\"normal\",name=\"Wd\")     # (#LSTM, 2x#LSTM)\n",
        "    self.Ud = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ud\")       # (#LSTM, #LSTM)\n",
        "    self.bd = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bd\")                   # (#LSTM, 1)\n",
        "    self.vd = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"vd\")                   # (#LSTM, 1)\n",
        "    self.W = self.add_weight(shape=(self.dim_LSTM+1,1),initializer=\"normal\",name=\"W\")                   # (#LSTM+1, 1)\n",
        "    self.b = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"b\")                                 # (1, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:        Entrée décodeur       : (batch_size,Tin,#LSTM)\n",
        "  #     Y:            Yt                    : (batch_size,1,1)\n",
        "  #     hid_state:    hidden state          : (batch_size,#LSTM)\n",
        "  #     cell_state:   cell_state            : (batch_size,#LSTM)\n",
        "  # Sorties :\n",
        "  #     out_hid :     hidden_state          : (batch_size,#LSTM)\n",
        "  #     out_cell :    cell_state            : (batch_size,#LSTM)\n",
        "  #     v_contexte:   vecteur contexte      : (batch_size,#LSTM)\n",
        "  def call(self,input,Y,hid_state,cell_state):\n",
        "    # Calcul des poids normalisés beta\n",
        "    scores = []\n",
        "    for i in range(input.shape[1]):\n",
        "      if hid_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([hid_state,cell_state],axis=1)         # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.Wd,hs)                                               # (#LSTM,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + tf.matmul(self.Ud,tf.expand_dims(input[:,i,:],-1))              # (#LSTM,#LSTM)x(batch_size,#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + self.bd                                                         # (batch_size,Tin,1)\n",
        "      else:\n",
        "        e = tf.matmul(self.Ud,tf.expand_dims(input[:,i,:],-1))                  # (#LSTM,#LSTM)x(batch_size,#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + self.bd                                                         # (batch_size,#LSTM,1)\n",
        " \n",
        "      e = K.tanh(e)\n",
        "      scores.append(tf.matmul(tf.transpose(self.vd),e))                         # (1,#LSTM)x(batch_size,#LSTM,1) = (batch_size,1,1)\n",
        "    \n",
        "    b = tf.convert_to_tensor(scores)                                            # (Tin,batch_size,1,1)\n",
        "    b = tf.keras.activations.softmax(b,axis=0)                                  # (Tin,batch_size,1,1)\n",
        "    b = tf.squeeze(b,-1)                                                        # (Tin,batch_size,1)\n",
        "    b = tf.transpose(b,perm=[1,0,2])                                            # (batch_size,Tin,1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    C = tf.multiply(input,b)        # (batch_size,Tin,#LSTM)_x_(batch_size,Tin,1) = (batch_size,Tin,#LSTM)\n",
        "    C = K.sum(C,axis=1)             # (batch_size,#LSTM)\n",
        "    C = tf.expand_dims(C,1)         # (batch_size,1,#LSTM)\n",
        "\n",
        "    # Calcul de Y_tilda\n",
        "    add = tf.keras.layers.concatenate([Y,C],axis=2)     # (batch_size,1,#LSTM+1)\n",
        "    add = tf.transpose(add,perm=[0,2,1])                # (batch_size,#LSTM+1,1)\n",
        "    Y_tilda = tf.matmul(tf.transpose(self.W),add)       # (1,#LSTM+1) x (batch_size,#LSTM+1,1) = (batch_size,1,1)\n",
        "    Y_tilda = Y_tilda + self.b\n",
        "\n",
        "    # Calcul des hidden state et cell state\n",
        "    if hid_state is not None:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda,initial_state=[hid_state,cell_state])\n",
        "    else:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda)\n",
        "\n",
        "    return out_hid,out_cell, C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBezNXNIKhbU"
      },
      "source": [
        "**3. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYg30e2tKhbU"
      },
      "source": [
        "class Net_DARNN(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, dim_LSTM, regul=0.0, drop = 0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wy = self.add_weight(shape=(self.dim_LSTM,2*self.dim_LSTM),initializer=\"normal\",name=\"Wy\")     # (#LSTM, 2x#LSTM)\n",
        "    self.bw = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bw\")                   # (#LSTM, 1)\n",
        "    self.vy = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"vy\")                   # (#LSTM, 1)\n",
        "    self.bv = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"bv\")                               # (1, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     output_seq:     Sortie séquence Y   : (batch_size,Tin-1,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction Y        : (batch_size,1,1)\n",
        "  def call(self,input,output_seq):\n",
        "    # Phase d'encodage\n",
        "    # Calcul les représentations cachées spatialles\n",
        "    # des séries en entrée\n",
        "    hid = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state = self.encodeur(input,hid_state,cell_state,i)\n",
        "      hid.append(hid_state)\n",
        "    hid = tf.convert_to_tensor(hid)               # (Tin,batch_size,#LSTM)\n",
        "    hid = tf.transpose(hid,perm=[1,0,2])          # (batch_size,Tin,#LSTM)\n",
        "\n",
        "    # Phase de décodage\n",
        "    # Calcul du vecteur contexte de sortie et du dernier\n",
        "    # hidden state à partir des représentations cachées\n",
        "    # précédentes et des (T-1) premieres sortie Y\n",
        "    hid_, cell_, vc = self.decodeur(hid,output_seq[:,0:1,:]*0,None,None)\n",
        "    for i in range(0,output_seq.shape[1]):\n",
        "      hid_, cell_, vc = self.decodeur(hid,output_seq[:,i:i+1,:],hid_,cell_)\n",
        "    \n",
        "    # Estimation de la sortie\n",
        "    # hid_ : (batch_size,#LSTM)\n",
        "    # vc   : (batch_size,1,#LSTM)\n",
        "    add = tf.keras.layers.concatenate([tf.expand_dims(hid_,1),vc],axis=2)       # (batch_size,1,2x#LSTM)\n",
        "    add = tf.transpose(add,perm=[0,2,1])                                        # (batch_size,2x#LSTM,1)\n",
        "    sortie = tf.matmul(self.Wy,add)                                             # (#LSTM,2x#LSTM) x (batch_size,2x#LSTM+1,1) = (batch_size,#LSTM,1)\n",
        "    sortie = sortie + self.bw                                                   # (batch_size,#LSTM,1)\n",
        "    sortie = tf.matmul(tf.transpose(self.vy),sortie)                            # (1,#LSTM)x(batch_size,#LSTM,1) = (batch_size,1,1)\n",
        "    sortie = sortie + self.bv                                                   # (batch_size,1,1)\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i03XW8dYMrIA"
      },
      "source": [
        "x_train[0].shape[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNEcGHfSAetw"
      },
      "source": [
        "x_train[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HL0kALoKhbU"
      },
      "source": [
        "**4. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKJkOpXvKhbV"
      },
      "source": [
        "dim_LSTM = 128\n",
        "drop=0.0\n",
        "l2reg=0.0\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,x_train[0].shape[2]))\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(longueur_sequence-1,1))\n",
        "\n",
        "  encodeur = Encodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "\n",
        "  sortie = Net_DARNN(encodeur,decodeur,longueur_sequence=longueur_sequence,dim_LSTM=dim_LSTM,regul=l2reg,drop=drop)(entrees_sequences,sorties_sequence)\n",
        "\n",
        "  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 1000\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.001)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train[0],x_train[1]],y=y_train,validation_data=([x_val[0],x_val[1]],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfFTzSYfyIf"
      },
      "source": [
        "model.evaluate(x=[x_train[0],x_train[1]],y=y_train)\n",
        "model.evaluate(x=[x_val[0],x_val[1]],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOahfnJ-yX1"
      },
      "source": [
        "**DA-RNN128_DR0_L2_0_BS512_EP1000_10 :**  \n",
        "\t- Vecteur LSTM : 128 \n",
        "\t- Longueur entrée : 10  \n",
        "\t- Longueur sortie : 1\n",
        "\t- Drop : 0.0\n",
        "\t- L2 : 0.00  \n",
        "\t- Batch Size : 512  \n",
        "\t- Périodes : 1000  \n",
        "\t=> mse :  3.3184e-05 / 4.6733e-05\n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Train_DARNN1.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XKdrbK-022"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05bqnRr-pyf"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Models/DA-RNN128_DR0_L2_0_BS512_EP1000_10.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"DA-RNN128_DR0_L2_0_BS512_EP1000_10.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions multi-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train[0],x_train[1]],verbose=1)\n",
        "pred_val = model.predict([x_val[0],x_val[1]],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,-1],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}