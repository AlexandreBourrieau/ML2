{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LorenzMap_Wavenet_Univarié_Multistep.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/WaveNet/LorenzMap_Wavenet_Multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFKhwq5wg7ZM"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVuCzMYKDp3z"
      },
      "source": [
        "# Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOdetggHqKE"
      },
      "source": [
        "# Récupération des données au format .csv\n",
        "!rm *.csv\n",
        "!wget --no-check-certificate --content-disposition \"https://raw.githubusercontent.com/AlexandreBourrieau/ML/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/data/LorenzMap.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJ6nnrRsUBm"
      },
      "source": [
        "Charge la série sous Pandas et affiche les informations du fichier :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEB_JjihEYTP"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "serie = pd.read_csv(\"LorenzMap.csv\", names=['X','Y','Z'])\n",
        "serie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyDl3H3UhD7K"
      },
      "source": [
        "serie = serie.astype(dtype='float32')\n",
        "serie.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1QKMBThNQni"
      },
      "source": [
        "# Affiche la série\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(serie.index, serie['X'],serie['Y'])\n",
        "plt.plot(serie.index, serie['Z'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(serie.values) * pourcentage)\n",
        "date_separation = serie.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(serie.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(serie.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(serie.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(serie.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(serie.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHfiXLFZhrPs"
      },
      "source": [
        "# Création du modèle type Wavenet Univarié - Multipas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd9AzWFtjnjs"
      },
      "source": [
        "**1. Création du réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIX-o-lMzVK0"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def get_model(longueur_sequence, longueur_sortie, nbr_filtres, dim_filtres, prodondeur_dilation, use_bias, res_l2, final_l2,batch_size):\n",
        "  input_shape = tf.keras.Input(shape=(longueur_sequence, 1), name='input_part')\n",
        "\n",
        "  entree = input_shape\n",
        "  sorties = []\n",
        "\n",
        "  for t in range(longueur_sortie):\n",
        "    # Convolution causale d'entrée\n",
        "    out = K.temporal_padding(entree, padding=((dim_filtres - 1), 0))\n",
        "    out = tf.keras.layers.Conv1D(filters=nbr_filtres,kernel_size=dim_filtres,dilation_rate=1,use_bias=True,activation=None,\n",
        "                              name='convolution_causale_%i' %t,kernel_regularizer=tf.keras.regularizers.l2(res_l2))(out)\n",
        "          \n",
        "    # Convolutions dilatées intermédiaires\n",
        "    res_x = out\n",
        "    skip = []\n",
        "    for i in range(1, prodondeur_dilation + 1):\n",
        "      z = K.temporal_padding(res_x, padding=(2**i * (dim_filtres - 1), 0))\n",
        "      z = tf.keras.layers.Conv1D(filters=nbr_filtres,kernel_size=dim_filtres,dilation_rate=2**i,use_bias=True,activation=\"relu\",\n",
        "                              name='convolution_causale_%i_%i' %(i,t), kernel_regularizer=tf.keras.regularizers.l2(res_l2))(z)\n",
        "\n",
        "      z = tf.keras.layers.Conv1D(1, 1, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(res_l2))(z)\n",
        "      skip_x = tf.keras.layers.Conv1D(1, 1, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(res_l2))(z)\n",
        "      res_x = tf.keras.layers.Conv1D(1, 1, padding='same', use_bias=False, kernel_regularizer=tf.keras.regularizers.l2(res_l2))(z)\n",
        "\n",
        "      res_x = tf.keras.layers.Add()([out, res_x])\n",
        "      out = res_x\n",
        "      skip.append(skip_x)\n",
        "    \n",
        "    # Addition des sorties intermédiaires\n",
        "    out = tf.keras.layers.Add()(skip)\n",
        "\n",
        "    # Couche de sortie\n",
        "    out = tf.keras.layers.Activation('linear', name=\"output_linear_%i\" %t)(out)\n",
        "    out = tf.keras.layers.Conv1D(1, 1, padding='same', kernel_regularizer=tf.keras.regularizers.l2(final_l2))(out)\n",
        "    sorties.append(out[:,-1:,:])\n",
        "\n",
        "    # Ajustement de l'entree à (t+i+1)\n",
        "    entree = tf.concat([entree,out[:,-1:,:]],axis=1)\n",
        "    entree = tf.slice(entree,[0,1,0],[batch_size,input_shape.shape[1],1])\n",
        "  \n",
        "  sorties = tf.convert_to_tensor(sorties)\n",
        "  sorties = tf.squeeze(sorties,-1)\n",
        "  sorties = tf.transpose(sorties,perm=[1,0,2])\n",
        "\n",
        "  model = tf.keras.Model(input_shape, sorties)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH9rsD5UzZgM"
      },
      "source": [
        "**2. Construction du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19zmYCgzTvV"
      },
      "source": [
        "def calcul_longueur_sequence(prodondeur_dilation):\n",
        "  return (2 ** prodondeur_dilation * 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln6eVTFezYlq"
      },
      "source": [
        "nbr_filtres = 2\n",
        "dim_filtres = 2\n",
        "profondeur_dilation = 3         # longueur_sequence = 16\n",
        "use_bias = False\n",
        "res_l2 = 0\n",
        "final_l2 = 0\n",
        "\n",
        "batch_size = 128\n",
        "longueur_sortie = 5\n",
        "\n",
        "longueur_sequence = calcul_longueur_sequence(profondeur_dilation)\n",
        "print(longueur_sequence)\n",
        "\n",
        "model = get_model(longueur_sequence, longueur_sortie, nbr_filtres, dim_filtres, profondeur_dilation, use_bias, res_l2, final_l2,batch_size=batch_size)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-longueur_sortie:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a575nB6tpUfj"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train = [X1,X2]\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tJ2U3QmpXWL"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_val = [X1,X2]\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 500\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model(longueur_sequence, longueur_sortie, nbr_filtres, dim_filtres, profondeur_dilation, use_bias, res_l2, final_l2, batch_size=int(batch_size/8))\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.01,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=x_train[1],y=y_train, validation_data=(x_val[1],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=200)],batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jheNXKz5O3aO"
      },
      "source": [
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vve7jENFlN3"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxjwVyvJFmtx"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plI7hKwCPWnm"
      },
      "source": [
        "start = 400\n",
        "\n",
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[start:])),erreur_entrainement[start:], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[start:])),erreur_validation[start:], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOahfnJ-yX1"
      },
      "source": [
        "**LorenzMap_Wavenet_Univarié Multistep**  \n",
        "  - Single step  \n",
        "  - profondeur dilation : 3 (longueur_sequence = 16)\n",
        "  - dim_filtres = 2\n",
        "  - nbr_filtres = 2\n",
        "  - régul = 0\n",
        "\n",
        "\t=> mse :  0.00014400063 / 0.00013114203\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05bqnRr-pyf"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Wavenet/LorenzMap_Wavenet_Univari%C3%A9_Multistep.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XKdrbK-022"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"LorenzMap_Wavenet_Univari%C3%A9_Multistep.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p71-LDZA4kre"
      },
      "source": [
        "# Prédictions Multistep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIMlZ-Ge4kre"
      },
      "source": [
        "pred_ent = model.predict(x_train[1],verbose=1,batch_size=batch_size)\n",
        "pred_val = model.predict(x_val[1],verbose=1,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVazFZhi4krf"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Affiche les courbes originales\n",
        "fig.add_trace(go.Scatter(x=serie.index,y=tf.squeeze(serie_entrainement_X_norm[:,-1:],-1),line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie.index[temps_separation:],y=tf.squeeze(serie_test_X_norm[:,-1:],-1),line=dict(color='red', width=1)))\n",
        "\n",
        "#Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "#max = 10\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(serie.index[longueur_sequence+i*longueur_sortie:longueur_sequence+(i+1)*longueur_sortie])\n",
        "  step_val.append(pred_ent[i*longueur_sortie,0,0])\n",
        "  step_time.append(serie.index[longueur_sequence+i*longueur_sortie])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pred_index,y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "#max = 10\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(serie.index[temps_separation+i*decalage+longueur_sequence:temps_separation+i*decalage+longueur_sequence+longueur_sortie])\n",
        "  step_val.append(pred_val[i*longueur_sortie,0,0])\n",
        "  step_time.append(serie.index[temps_separation+i*decalage+longueur_sequence])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pred_index,y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "# Affiche les prédictions\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnEWS_Ru4krf"
      },
      "source": [
        "**Erreurs en multi step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqAdHqUu4krf"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "#Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=serie.index[longueur_sequence:],y=serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie.index[longueur_sequence:],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_ent = tf.keras.losses.mse(serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGWZv5pe4krf"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=serie.index[temps_separation+longueur_sequence::],y=serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=serie.index[temps_separation+longueur_sequence::],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_test = tf.keras.losses.mse(serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLAbTvVJ4krf"
      },
      "source": [
        "print(mse_ent)\n",
        "print(mse_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}