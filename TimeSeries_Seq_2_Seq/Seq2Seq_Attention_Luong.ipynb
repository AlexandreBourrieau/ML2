{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_Attention_Luong.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/TimeSeries_Seq_2_Seq/Seq2Seq_Attention_Luong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-pPkMQp1JI"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place une attention de type Bahdanau dans notre modèle de prédiction de séries temporelles Séquence vers Séquence.  \n",
        "Le papier de recherche sur lequel s'appuie ce modèle est disponible ici : [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Data/Power_PV.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_data = pd.read_csv(\"Power_PV.csv\")\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU5e7LEN5o23"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdlO2gPJ9B0t"
      },
      "source": [
        "Converison des types `object` en `float32` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbdm1hcH53tO"
      },
      "source": [
        "df_data.iloc[:,1:] = pd.DataFrame.replace(df_data.iloc[:,1:],\"?\",\"NaN\")\n",
        "df_data.iloc[:,1:] = df_data.iloc[:,1:].astype(np.float32)\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgPxFdsd5Ybv"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ShrX00CM5z"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2018-07-01 00:00:00\"\n",
        "date_fin = \"2019-04-14 00:00:00\"\n",
        "\n",
        "# Place l'index du dataframe sur la colonne Date\n",
        "df_data = df_data.rename(columns={'Unnamed: 0': \"Date\"})\n",
        "df_data = df_data.set_index(df_data['Date'])\n",
        "\n",
        "# Copie des données dans le dataframe d'étude sur l'intervalle d'étude\n",
        "df_etude = df_data.loc[date_debut:date_fin].copy()\n",
        "\n",
        "# Conversion de la colonne Date au format datetime\n",
        "df_etude.index = pd.to_datetime(df_etude.index)\n",
        "\n",
        "# Suppression de la colonne Unnamed:0\n",
        "df_etude = df_etude.drop(\"Date\", axis=1)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oOlwpLd9NdW"
      },
      "source": [
        "Vérification des données et correction des anomalies :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZGgjlBr8qMH"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXje2Zaill9l"
      },
      "source": [
        "df_etude = df_etude.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWNilOlvmChW"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=df_etude['watts'], line=dict(color='blue', width=1),name=\"Puissance (W)\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = df_etude['watts'].values\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,500))\n",
        "ax1.set_title(\"Autocorrélation\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 500))\n",
        "ax2.set_title(\"Autocorrélation partielle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLPlWzLJlNhA"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02JYhFxflROo"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude['watts'].values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude['watts'].values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude['watts'].values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6HRdVZmRM6"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JhXRi2DnAY"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsOFDQV_mVkf"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "serie_entrainement_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X),1)))\n",
        "serie_test_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_test_X,shape=(len(serie_test_X),1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44PYyswAlXFI"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm, label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm, label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  dataset = dataset.map(lambda x: (x[0:longueur_sequence][:,:],tf.expand_dims(x[-longueur_sortie:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 256\n",
        "longueur_sequence = 5*24*4      # 5 jours (288*15min)\n",
        "longueur_sortie = 12*4          # 12h\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hppDt9HnNQU"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWR9V0WnOyR"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUAqFIpmnY8Y"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9aPVTcDn6TQ"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,3):\n",
        "  ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence),x_train[i],label=\"X_train\")\n",
        "  ax.plot(np.linspace(longueur_sequence,longueur_sequence+longueur_sortie-1,longueur_sortie),y_train[i],label=\"Y_train\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7dkj0dK2S7P"
      },
      "source": [
        "# Codage des couches du modèle Seq2Seq avec attention de Bahadanau (avec encodeur simple GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqeBGpRB2S7r"
      },
      "source": [
        "Notre modèle Séquence vers Séquence est composé :\n",
        "- D'un encodeur, qui comprend une couche GRU uni-directionnelle\n",
        "- D'un décodeur, qui comprend une couche GRU uni-directionnelle.\n",
        "- D'un système d'attention de type Bahdanau dont le but est de décider quelle est la partie de la source la plus importante à chaque étape du décodage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqrQ2THM2S7s"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_GRU2_.png?raw=true' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK9B2ezPasjh"
      },
      "source": [
        "**1. Création de la couche d'attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzt2zFcVawlg"
      },
      "source": [
        "L'attention est calculée à chaque étape du décodage. Le but est de décider quelles sont les parties les plus importante dans la source.  \n",
        "Pour cela, à chaque étape du décodage le calcul d'attention est effectué de la manière suivante :\n",
        "- L'attention reçoit l'ensemble des états cachés de l'encodeur Enc_out (batch_size, Tin, GRU) ainsi que l'état caché du décodeur à l'étape précédente Hid_state (batch_size,GRU)\n",
        "- A partir de ces deux informations, elle calcule un score d'attention alpha. Ce score représente l'importance de l'état caché issu du décodeur à l'étape de décodage courante. Dans la méthode de Bahdanau, ce score est calculé en appliquant une couche dense sur la concaténation de ces deux informations, avec une fonction d'activation de type tanh.\n",
        "- A partir des scores d'attention, elle calcule les poids d'attention à attribuer à chaque état caché de l'encodeur (Enc_out#1, Enc_out#2, ...). Ces poids représentent des probabilités et sont obtenus en appliquant une fonction de type Softmax sur les scores.\n",
        "- Enfin, elle applique les poids d'attention sur les états cachés de l'encodeur en effectuant la somme pondérée de ces états cachés avec les poids.\n",
        "- Le résultat obtenu est le vecteur contexte C qui sera ensuite concaténé avec l'entrée et injecté dans la cellule GRU de décodage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfhrXQJ6Fo-"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/CalculAttention_Bahdanau2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOiyFNN8sq56"
      },
      "source": [
        "# Classe d'attention de Bahdanau\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_att):\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W1 = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W1\")   # (#Att, #GRU)\n",
        "    self.W2 = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W2\")   # (#Att, #GRU)\n",
        "    self.va = self.add_weight(shape=(self.dim_att,1),initializer=\"normal\",name=\"va\")                # (#Att, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Entrées : enc_out : Etats cachés de l'encodeur (batch_size, Tin, #GRU)\n",
        "  #           hid_dec : Etat caché du décodeur (batch_size, #GRU)\n",
        "  # Sortie :  vc :      Vecteur contexte (batch_size,#GRU) \n",
        "  def call(self,enc_out,hid_dec):\n",
        "    # Calcul de Xh contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de l'encodeur\n",
        "    xt = tf.transpose(enc_out,perm=[0,2,1])     # (batch_size,Tin,#GRU) => (batch_size,#GRU,Tin)\n",
        "    Xh = tf.matmul(self.W1,xt)                  # (#Att,#GRU)x(batch_size,#GRU,Tin) = (batch_size,#Att,Tin)\n",
        "\n",
        "    # Calcul de la représentation cachée S\n",
        "    # du vecteur caché issu du décodeur\n",
        "    hid_dec = tf.expand_dims(hid_dec,-1)      # (batch_size,#GRU) => (batch_size,#GRU,1)\n",
        "    S = tf.matmul(self.W2,hid_dec)            # (#Att,#GRU)x(batch_size,#GRU,1) = (batch_size,#Att,1)\n",
        "\n",
        "    # Addition terme à terme des représentations\n",
        "    # cachées contenues dans les matarices Xh et\n",
        "    # de la représentation cachées contenue dans\n",
        "    # le vecteur S\n",
        "    Yh = tf.add(Xh,S)                         # (batch_size,#Att,Tin) _+_ (batch_size,#Att,1) = (batch_size,#Att,Tin)\n",
        "    Yh = K.tanh(Yh)                           # Yh = (batch_size,#Att,Tin)\n",
        "\n",
        "    # Calcul des poids d'attention normalisés\n",
        "    Yh = tf.transpose(Yh,perm=[0,2,1])        # Yh = (batch_size,#Att,Tin) => (batch_size,Tin,#Att)\n",
        "    a = tf.matmul(Yh,self.va)                 # (batch_size,Tin,#Att)x(#Att,1) = (batch_size,Tin,1)\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    xa = tf.multiply(enc_out,a)               # (batch_size,Tin,#GRU)_x_(batch_size,Tin,1) = (batch_size,Tin,#GRU)\n",
        "    vc = K.sum(xa,axis=1)                     # vc = (batch_size,#GRU)\n",
        "    return vc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqqmIcUg2S7s"
      },
      "source": [
        "**2. Création de la couche d'encodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ8pgIJ4YE1d"
      },
      "source": [
        "Dans le document de recherche de Bahdanau, l'encodeur utilisé est de type RNN bi-directionnel. Ici on utilise un encodeur GRU unidurectionnel.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFpxmR0mxSq"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Encodeur_Bahdanau.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCPfeJq22S7s"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=True,return_state=True,dropout=self.drop,name=\"GRU_Encodeur\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée encodeur       : (batch_size,Tin,1) \n",
        "  # Sorties :\n",
        "  #     out_enc : Sortie encodeur       : (batch_size,Tin,#GRU)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,#GRU)\n",
        "  def call(self, input):\n",
        "    out_enc, out_hid = self.couche_GRU(input)\n",
        "    return out_enc, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__rii3mc2S7t"
      },
      "source": [
        "**3. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-jGzrstYSd0"
      },
      "source": [
        "Dans le document de recherche de Bahdanau, le décodeur utilise un réseau RNN unidirectionnel agrémenté de deux matrices de poids pour prendre en compte la combinaison linéaire du vecteur contexte et de l'état caché. Ici, on utilise un réseau de type GRU unidirectionnel et on réalise cette combinaison linéaire à l'extérieur du réseau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Pey2RHm5FE"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Decodeur_Bahdanau_2.png?raw=true' width=900/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnMiDjg12S7t"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"GRU_Decodeur\")\n",
        "    self.couche_Dense = tf.keras.layers.Dense(units=1,kernel_regularizer=tf.keras.regularizers.l2(self.regul),input_dim=self.dim_GRU)\n",
        "    self.Attention = Couche_Attention(self.dim_GRU)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     dec_input:    Entrée décodeur       : (batch_size,1,1) \n",
        "  #     enc_out:      Sortie encodeur       : (batch_size,Tin,#GRU)\n",
        "  #     hidden:       Vecteur caché         : (batch_size,#GRU)\n",
        "  # Sorties :\n",
        "  #     out_dec : Sortie décodeur       : (batch_size,1)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,#GRU)\n",
        "  def call(self,dec_input, enc_out,hidden):\n",
        "    # Calcul du vecteur contexte\n",
        "    vect_contexte = self.Attention(enc_out,hidden)          # (batch_size,#GRU)\n",
        "\n",
        "    # Concaténation du vecteur contexte\n",
        "    # et de l'entrée du décodeur\n",
        "\n",
        "    vect_contexte = tf.expand_dims(vect_contexte,1)          # (batch_size,#GRU) => (batch_size,1,#GRU)\n",
        "    add = tf.concat([vect_contexte,dec_input],axis=2)        # Concat {(batch_size,1,#GRU),(batch_size,1,1)} = ((batch_size,1,#GRU+1))\n",
        "\n",
        "    # Calcul de la sortie\n",
        "    out_dec, out_hid = self.couche_GRU(add,initial_state=hidden)\n",
        "    out_dec = self.couche_Dense(out_dec)\n",
        "    return out_dec, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSspq38H2S7u"
      },
      "source": [
        "**4. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20vrAwPY7Wc"
      },
      "source": [
        "Dans le document de recherche, l'état caché initial du décodeur est calculé à partir du premier état caché (sens retour) de l'encodeur. Ici, on calcule cet état initial en utlilisant le dernier état caché de l'encodeur. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Jd51Jhnc97"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_GRU_2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ9tC3nJ2S7u"
      },
      "source": [
        "class Net_GRU(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Ws = tf.keras.layers.Dense(units=self.dim_GRU,activation=\"tanh\",use_bias=False)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size, Tin, 1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    # Encodage de la séquence d'entrée\n",
        "    enc_out, enc_hid = self.encodeur(input)       # (batch_size,Tin,#GRU), (batch_size,#GRU)\n",
        "\n",
        "    # Calcul de l'état caché initial du décodeur\n",
        "    dec_hid = self.Ws(enc_hid)                    # s0 : (batch_size,#GRU)\n",
        "\n",
        "    # La première entrée du décodeur est la\n",
        "    # dernière entrée de l'encodeur\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)   # Y0 : (batch_size,1,1)\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, enc_out, dec_hid)\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, enc_out, dec_hid)\n",
        "        dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFQB6v98B-MN"
      },
      "source": [
        "**5. Création du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn-R4Ik2CKd4"
      },
      "source": [
        "Notre modèle doit avoir deux comportements différents suivant qu'il soit en entrainement ou en prédiction. Pour différencier ce que réalisent ces deux modes, nous devons écrire nos propres fonctions d'entrainement et de test.  \n",
        "\n",
        "Pour faire cela, nous allons dériver la classe `CustomModel` qui permet d'implanter ses propres fonctions :\n",
        "- train_step : Définit la fonction d'entrainement.\n",
        "- test_step : Définit la fonction de test.  \n",
        "  \n",
        "Lors de la phase d'entrainement, l'optimiseur doit appliquer l'algorithme de rétropropagation du gradient sur la fonction d'erreur. Il doit donc calculer les dérivées partielles de l'erreur par rapport aux différents poids du modèle. Ce calcul est effectué par la ligne `self.optimizer.apply_gradients(zip(gradients, trainable_vars))`  \n",
        "\n",
        "L'optimisaeur a donc besoin des gradients de l'erreur. C'est pendant l'application de la fonction d'erreur aux prédictions que ces gradients sont calculés et sauvegardés par différentiation automatique. La ligne qui réalise cela est la ligne : `loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)`\n",
        "\n",
        "Ce calcul est réalisé sous la surveillance de la fonction `GradientTape` qui permet de sauvegarder les résultats du gradients, afin qu'ils soient utilisés par l'optimiseur.  \n",
        "\n",
        "Dans la phase de test, l'optimisation n'est pas utilisée. Les gradients n'ont donc pas besoin d'être sauvegardés et donc le calcul de l'erreur n'est pas appelé sous la surveillance de GradientTape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNh-GvpeCB_s"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Calcul du gradient\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Optimisation des poids\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul des predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Mise à jour des erreurs\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGiXTAMlCUu2"
      },
      "source": [
        "dim_GRU = 128\n",
        "drop=0.2\n",
        "l2reg=0.001\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "  sorties_sequences = tf.keras.layers.Input(shape=(longueur_sortie,1))\n",
        "\n",
        "  encodeur = Encodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "\n",
        "  sortie = Net_GRU(encodeur,decodeur,longueur_sequence=longueur_sequence, longueur_sortie=longueur_sortie)(entrees_sequences,sorties_sequences)\n",
        "\n",
        "  model = CustomModel([entrees_sequences,sorties_sequences],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_2pjIwEc9TH"
      },
      "source": [
        "# Codage des couches du modèle Seq2Seq avec attention de Bahadanau (avec encodeur LSTM-Bidirectionnel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6y_St_0RHmq"
      },
      "source": [
        "Notre modèle Séquence vers Séquence est composé :\n",
        "- D'un encodeur, qui comprend une couche LSTM bi-directionnelle. Les états cachés et les cell-states avants/arrières sont additionés (merge=\"sum\").\n",
        "- D'un décodeur, qui comprend une couche LSTM uni-directionnelle. Une couche denses est utilisée pour recréer la sortie univariée à partir des sorties des cellules LSTM.\n",
        "  \n",
        "L'attention n'utilise que les hidden-states des LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZkh0szAdjIO"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_LSTM_Decodeur_Ah.png?raw=true' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNfT12MM1hE2"
      },
      "source": [
        "**1. Création de la couche d'attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QnHOGdG1v0z"
      },
      "source": [
        "Nous gradons la même structure pour la couche d'attention :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaF7U8_huw91"
      },
      "source": [
        "# Classe d'attention de Bahdanau\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_att):\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W1 = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W1\")   # (#Att, #LSTM)\n",
        "    self.W2 = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W2\")   # (#Att, #LSTM)\n",
        "    self.va = self.add_weight(shape=(self.dim_att,1),initializer=\"normal\",name=\"va\")                # (#Att, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Entrées : enc_out : Etats cachés de l'encodeur (batch_size, Tin, #LSTM)\n",
        "  #           hid_dec : Etat caché du décodeur (batch_size, #LSTM)\n",
        "  # Sortie :  vc :      Vecteur contexte (batch_size,#LSTM) \n",
        "  def call(self,enc_out,hid_dec):\n",
        "    # Calcul de Xh contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de l'encodeur\n",
        "    xt = tf.transpose(enc_out,perm=[0,2,1])     # (batch_size,Tin,#LSTM) => (batch_size,#LSTM,Tin)\n",
        "    Xh = tf.matmul(self.W1,xt)                  # (#Att,#LSTM)x(batch_size,#LSTM,Tin) = (batch_size,#Att,Tin)\n",
        "\n",
        "    # Calcul de la représentation cachée S\n",
        "    # du vecteur caché issu du décodeur\n",
        "    hid_dec = tf.expand_dims(hid_dec,-1)      # (batch_size,#LSTM) => (batch_size,#LSTM,1)\n",
        "    S = tf.matmul(self.W2,hid_dec)            # (#Att,#LSTM)x(batch_size,#LSTM,1) = (batch_size,#Att,1)\n",
        "\n",
        "    # Addition terme à terme des représentations\n",
        "    # cachées contenues dans les matarices Xh et\n",
        "    # de la représentation cachées contenue dans\n",
        "    # le vecteur S\n",
        "    Yh = tf.add(Xh,S)                         # (batch_size,#Att,Tin) _+_ (batch_size,#Att,1) = (batch_size,#Att,Tin)\n",
        "    Yh = K.tanh(Yh)                           # Yh = (batch_size,#Att,Tin)\n",
        "\n",
        "    # Calcul des poids d'attention normalisés\n",
        "    Yh = tf.transpose(Yh,perm=[0,2,1])        # Yh = (batch_size,#Att,Tin) => (batch_size,Tin,#Att)\n",
        "    a = tf.matmul(Yh,self.va)                 # (batch_size,Tin,#Att)x(#Att,1) = (batch_size,Tin,1)\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    xa = tf.multiply(enc_out,a)               # (batch_size,Tin,#LSTM)_x_(batch_size,Tin,1) = (batch_size,Tin,#LSTM)\n",
        "    vc = K.sum(xa,axis=1)                     # vc = (batch_size,#LSTM)\n",
        "    return vc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8J4fmfWc9TV"
      },
      "source": [
        "**2. Création de la couche d'encodeur à base de LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AOxn0upx3-H"
      },
      "source": [
        "La couche Bidirectionnelle appliquée à une couche LSTM retourne 5 tenseurs. Dans le cas où `return_sequence=True` les dimensions sont les duivantes :\n",
        "- La concaténation (ou la somme, en fonction de la méthode `merge` choisie) des états cachés avance / recul : (batch_size,Tin,dim_LSTM*2) si concaténation ou (batch_size,Tin, dim_LSTM) si merge\n",
        "- Le dernier état caché du LSM d'avance : (batch_size, dim_LSTM)\n",
        "- Le derner cell-state du LSTM d'avance : (batch_size, dim_LSTM)\n",
        "- Le dernier état caché du LSTM de recul : (batch_size, dim_LSTM)\n",
        "- Le dernier cell-state du LSTM de recul : (batch_size, dim_LSTM)\n",
        "  \n",
        "Dans notre cas, nous retournons :\n",
        "- La somme (merge=\"sum\") des états cachés avance/recul\n",
        "- La somme du dernier état caché d'avance et du dernier état caché de recul.\n",
        "- La somme du dernier cell-state d'avance et du dernier cell-state de recul."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRHo8AUoeQSB"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_LSTM_Encodeur_Ah2.png?raw=true' width=300/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt7DF6g-2E3V"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                  # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=True,return_state=True,dropout=self.drop,name=\"LSTM_Encodeur\"),merge_mode=\"sum\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée encodeur       : (batch_size,Tin,1) \n",
        "  # Sorties :\n",
        "  #     out_enc :   Sortie encodeur       : (batch_size,Tin,#LSTM)\n",
        "  #     out_hid_x : Sortie vecteur caché  : (batch_size,#LSTM)\n",
        "  #     state_c_x : Sortie cell state     : (btach_size,#LSTM)\n",
        "  def call(self, input):\n",
        "    out_enc, out_hid_1, state_c_1, out_hid_2, state_c_2 = self.couche_LSTM(input)\n",
        "    return out_enc, tf.keras.layers.Add()([out_hid_1, out_hid_2]),tf.keras.layers.Add()([state_c_1, state_c_2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBcCalqUc9TZ"
      },
      "source": [
        "**3. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-omuD8t47GF"
      },
      "source": [
        "Avec l'utilisation de cellules LSTM dans l'encodeur et le décodeur, il faut prendre en compte les nouvelles entrées et sorties du décodeur.  \n",
        "Concernant les entrées, elles sont au nombre de 4 :\n",
        " - La sortie provenant de l'encodeur : (batch_size,Tin,#LSTM)\n",
        " - L'entrée du décodeur : (batch_size,1,1)\n",
        " - L'état caché et le cell state : [(batch_size,#LSTM),(batch_size,#LSTM)]\n",
        "  \n",
        "Concernant les sorties, elles sont au nombre de 2 :\n",
        " - La sortie du décodeur issue du générateur : (batch_size,1)\n",
        " - L'état caché et le cell-satte : [(batch_size,#LSTM),(batch_size,#LSTM)]\n",
        "\n",
        "Remarque : L'attention n'utilise que le hidden-state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNh2Rr_kd9gA"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_LSTM_Decodeur_Ahh.png?raw=true' width=900/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLBza0HI3lhc"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"GRU_Decodeur\")\n",
        "    self.couche_Dense = tf.keras.layers.Dense(units=1,kernel_regularizer=tf.keras.regularizers.l2(self.regul),input_dim=self.dim_LSTM)\n",
        "    self.Attention = Couche_Attention(self.dim_LSTM)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     dec_input:    Entrée décodeur       : (batch_size,1,1) \n",
        "  #     enc_out:      Sortie encodeur       : (batch_size,Tin,#LSTM)\n",
        "  #     hidden:       hidden et cell        : [(batch_size,#LSTM),(batch_size,#LSTM)]\n",
        "  # Sorties :\n",
        "  #     out_dec :     Sortie décodeur       : (batch_size,1)\n",
        "  #     out_state :   Hidden LSTM           : (batch_size,#LSTM)\n",
        "  #     out_cell :    Cell state LSTM       : (batch_size,#LSTM)\n",
        "  def call(self,dec_input,enc_out,hidden):\n",
        "    # Calcul du vecteur contexte\n",
        "    vect_contexte = self.Attention(enc_out,hidden[0])        # hidden[0] = hidden_state : (batch_size,#LSTM)\n",
        "\n",
        "    # Concaténation du vecteur contexte\n",
        "    # et de l'entrée du décodeur\n",
        "    vect_contexte = tf.expand_dims(vect_contexte,1)          # (batch_size,#LSTM) => (batch_size,1,#LSTM)\n",
        "    add = tf.concat([vect_contexte,dec_input],axis=2)        # Concat {(batch_size,1,#LSTM),(batch_size,1,1)} = ((batch_size,1,#LSTM+1))\n",
        "\n",
        "    # Calcul de la sortie\n",
        "    out_dec, out_state, out_cell = self.couche_LSTM(add,initial_state=hidden)\n",
        "    out_dec = self.couche_Dense(out_dec)\n",
        "    return out_dec, out_state, out_cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7AGHa66c9Tc"
      },
      "source": [
        "**4. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dibpLU5veYOd"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_LSTM_Decodeur_Ah.png?raw=true' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUldqMv3-Mus"
      },
      "source": [
        "class Net_LSTM(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Ws_etat = tf.keras.layers.Dense(units=self.dim_LSTM,activation=\"tanh\",use_bias=False)\n",
        "    self.Ws_cell = tf.keras.layers.Dense(units=self.dim_LSTM,activation=\"tanh\",use_bias=False)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size, 1, 1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    # Encodage de la séquence d'entrée\n",
        "    enc_out, out_hid, out_c = self.encodeur(input)\n",
        "\n",
        "    # Calcul de l'état caché initial du décodeur\n",
        "    dec_hid = self.Ws_etat(out_hid)            # h0_state : (batch_size,#LSTM)\n",
        "    dec_state = self.Ws_cell(out_c)            # c0_cell : (batch_size,#LSTM)\n",
        "\n",
        "    # La première entrée du décodeur est la\n",
        "    # dernière entrée de l'encodeur\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)   # Y0 : (batch_size,1,1)\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid, dec_state = self.decodeur(dec_input, enc_out,[dec_hid,dec_state])\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid,dec_state = self.decodeur(dec_input, enc_out, [dec_hid,dec_state])\n",
        "        dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bL8e_pECGE8"
      },
      "source": [
        "**5. Création du modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdTK4wAQ1O4v"
      },
      "source": [
        "Notre modèle doit avoir deux comportements différents suivant qu'il soit en entrainement ou en prédiction. Pour différencier ce que réalisent ces deux modes, nous devons écrire nos propres fonctions d'entrainement et de test.  \n",
        "\n",
        "Pour faire cela, nous allons dériver la classe `CustomModel` qui permet d'implanter ses propres fonctions :\n",
        "- train_step : Définit la fonction d'entrainement.\n",
        "- test_step : Définit la fonction de test.  \n",
        "  \n",
        "Lors de la phase d'entrainement, l'optimiseur doit appliquer l'algorithme de rétropropagation du gradient sur la fonction d'erreur. Il doit donc calculer les dérivées partielles de l'erreur par rapport aux différents poids du modèle. Ce calcul est effectué par la ligne `self.optimizer.apply_gradients(zip(gradients, trainable_vars))`  \n",
        "\n",
        "L'optimisaeur a donc besoin des gradients de l'erreur. C'est pendant l'application de la fonction d'erreur aux prédictions que ces gradients sont calculés et sauvegardés par différentiation automatique. La ligne qui réalise cela est la ligne : `loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)`\n",
        "\n",
        "Ce calcul est réalisé sous la surveillance de la fonction `GradientTape` qui permet de sauvegarder les résultats du gradients, afin qu'ils soient utilisés par l'optimiseur.  \n",
        "\n",
        "Dans la phase de test, l'optimisation n'est pas utilisée. Les gradients n'ont donc pas besoin d'être sauvegardés et donc le calcul de l'erreur n'est pas appelé sous la surveillance de GradientTape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoMrc-_pZ2WE"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Calcul du gradient\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Optimisation des poids\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul des predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Mise à jour des erreurs\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozgilOOUbJmb"
      },
      "source": [
        "dim_LSTM = 128\n",
        "drop=0.4\n",
        "l2reg=0.01\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "  sorties_sequences = tf.keras.layers.Input(shape=(longueur_sortie,1))\n",
        "\n",
        "  encodeur = Encodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "\n",
        "  sortie = Net_LSTM(encodeur,decodeur,longueur_sequence=longueur_sequence, longueur_sortie=longueur_sortie)(entrees_sequences,sorties_sequences)\n",
        "\n",
        "  model = CustomModel([entrees_sequences,sorties_sequences],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "  max_periodes = 1000\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train,y_train],y=y_train,validation_data=([x_val,y_val],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfFTzSYfyIf"
      },
      "source": [
        "model.evaluate(x=[x_train,y_train],y=y_train)\n",
        "model.evaluate(x=[x_val,y_val],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYRtjcGBaB4a"
      },
      "source": [
        "#Mise en place du Schedule Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twg8YFLrrom7"
      },
      "source": [
        "**1. Définition de la fonction de probabilité**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B0yiv2waIdn"
      },
      "source": [
        "Nous mettons ici en place un entrainement aléatoirement forcé ou non basé sur ce papier de recherche : [Scheduled Sampling for Sequence Prediction with\n",
        "Recurrent Neural Networks](https://arxiv.org/pdf/1506.03099.pdf)\n",
        ".  \n",
        "  \n",
        "L'idée est ici de choisir aléatoirement sur chaque entrée si nous effectuons un entrainement forcé ou non :  \n",
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/ScheduleSampling.png?raw=true' width=500/>  \n",
        "  \n",
        "  La probabilité du choix est controllée par une décroissance de type sigmoïde inverse :  \n",
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/ScheduleSamplingProba.png?raw=true' width=500/>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d_UaYf5mM-l"
      },
      "source": [
        "Le taux de décroissance est fixé au nombre de périodes divisé par 10 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhxGTfWmmSQZ"
      },
      "source": [
        "n_periodes = 1000\n",
        "\n",
        "k = n_periodes/10       # Taux de décroissance\n",
        "\n",
        "x = np.linspace(0,n_periodes,n_periodes+1)\n",
        "y = k/(k+np.exp(x/k))\n",
        "\n",
        "plt.plot(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnfJq3mcrsra"
      },
      "source": [
        "**2. Implantation du Schedule Sampling dans le modèle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRUCC1nBkLnj"
      },
      "source": [
        "Pour cela, on ajoute le tirage au sort dans notre classe Net_LSTM. Le tirage au sort se fait sur chaque entrée de la séquence et non sur la séquence entière. Ceci permet d'obtenir des meilleurs résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdexC0ZrkLHQ"
      },
      "source": [
        "class Net_LSTM(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Ws_etat = tf.keras.layers.Dense(units=self.dim_LSTM,activation=\"tanh\",use_bias=False)\n",
        "    self.Ws_cell = tf.keras.layers.Dense(units=self.dim_LSTM,activation=\"tanh\",use_bias=False)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size, 1, 1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False,proba=1.0):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    # Encodage de la séquence d'entrée\n",
        "    enc_out, out_hid, out_c = self.encodeur(input)\n",
        "\n",
        "    # Calcul de l'état caché initial du décodeur\n",
        "    dec_hid = self.Ws_etat(out_hid)            # h0_state : (batch_size,#LSTM)\n",
        "    dec_state = self.Ws_cell(out_c)            # c0_cell : (batch_size,#LSTM)\n",
        "\n",
        "    # La première entrée du décodeur est la\n",
        "    # dernière entrée de l'encodeur\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)   # Y0 : (batch_size,1,1)\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid, dec_state = self.decodeur(dec_input, enc_out,[dec_hid,dec_state])\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid,dec_state = self.decodeur(dec_input, enc_out, [dec_hid,dec_state])\n",
        "        # Tirage au sort\n",
        "        tirage = np.random.binomial(n=1,size=1,p=proba)\n",
        "        if tirage == 1:\n",
        "          dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        else:\n",
        "          dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h_U16RSqDWL"
      },
      "source": [
        "Il faut également modifier notre classe personnelle du modèle afin d'y insérer :\n",
        " - L'initialisation : Permet d'instancier les éléments dont on aura besoin pour appeler notre couche Net_LSTM, ainsi que les variables liées aux calculs des probabilités de tirage au sort pour l'entrainement forcé\n",
        " - La fonction d'appel call qui appelle la couche Net_LSTM\n",
        " - On ajoute le calcul de la probabilité dans la fonction train_step. Ainsi, à chaque nouveau batch, la probabilité est ré-évaluée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf3NDkMih9_Q"
      },
      "source": [
        "dim_LSTM = 128\n",
        "drop=0.4\n",
        "l2reg=0.01\n",
        "max_periodes = 1000\n",
        "k = max_periodes/10\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.periode = 0                  # Période courante\n",
        "        self.k = k                        # Facteur de décroissance\n",
        "        self.proba = 1.0                  # Proba du batch\n",
        "        self.encodeur = Encodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "        self.decodeur = Decodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "        self.Net_LSTM = Net_LSTM(self.encodeur,self.decodeur,longueur_sequence=longueur_sequence, longueur_sortie=longueur_sortie)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        sortie = self.Net_LSTM(inputs[0],inputs[1],training=training, proba=self.proba)\n",
        "        return sortie\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul de la probabilité courante pour ce batch\n",
        "        self.proba = self.k/(self.k+np.exp(self.periode/self.k))\n",
        "        self.periode = self.periode + 1\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Calcul du gradient\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Optimisation des poids\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul des predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Mise à jour des erreurs\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7vV7XXrzp6"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVxeWpO_-y00"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = CustomModel()\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train,y_train],y=y_train,validation_data=([x_val,y_val],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYHKebOZ7T6j"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BvKrp4T7WqO"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEtShsiB7ZcO"
      },
      "source": [
        "model.evaluate(x=[x_train,y_train],y=y_train)\n",
        "model.evaluate(x=[x_val,y_val],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIBUbI1TICU"
      },
      "source": [
        "**1. Seq2Seq Bahdanau - Encodeur unidirectionnel GRU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOahfnJ-yX1"
      },
      "source": [
        "**Seq2Seq_Bahdanau_GRU128_DR02_L2_001_BS128_EP1000_5J_12H :**\n",
        "- Encodeur unidirectionnel GRU #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 12h  \n",
        "- Drop : 0.2  \n",
        "- L2 : 0.01  \n",
        "- Batch Size : 128\n",
        "- Périodes : 1000  \n",
        "- Décroissance LR : 0.001  \n",
        "=> mse : 0.0171 / 0.0239  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Train_TPU_Seq2SeqBahdanau1.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XKdrbK-022"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flEANw1UxQfg"
      },
      "source": [
        "!rm *.hdf5\n",
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Bahdanau_GRU128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5?raw=true\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Bahdanau_GRU128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci9qh0TdTPD2"
      },
      "source": [
        "**2. Seq2Seq Bahdanau - Encodeur LSTM Bidirectionnel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckazHYvhTfeF"
      },
      "source": [
        "**a - Seq2Seq_Bahdanau_LSTMBiDir128_DR03_L2_005_BS128_EP1000_5J_12H :**\n",
        "- Encodeur Bi-directionnel LSTM #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 12h\n",
        "- Drop : 0.3\n",
        "- L2 : 0.005\n",
        "- Batch Size : 128\n",
        "- Périodes : 1000  \n",
        "- Décroissance LR : 0.001  \n",
        "=> mse : 0.0229 / 0.0296\n",
        "  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Train_Bahdanau_LSTM_1.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwHdVXb82Vgn"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4DzoDy42W7-"
      },
      "source": [
        "!rm *.hdf5\n",
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Bahdanau_LSTMBiDir128_DR03_L2_005_BS128_EP1000_5J_12H.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q01gpFP82fRd"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Bahdanau_LSTMBiDir128_DR03_L2_005_BS128_EP1000_5J_12H.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbNfaZp4RSts"
      },
      "source": [
        "**b - Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_DLR_001_BS256_EP1000_5J_12H :**\n",
        "- Encodeur Bi-directionnel LSTM #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 12h\n",
        "- Drop : 0.4\n",
        "- L2 : 0.01\n",
        "- Batch Size : 256\n",
        "- Périodes : 1000  \n",
        "- Décroissance LR : 0.01  \n",
        "=> mse : 0.0144 / 0.0211\n",
        "  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Train_Bahdanau_LSTM_2.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5rM59z2Q_1X"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGAgFNuqRBoV"
      },
      "source": [
        "!rm *.hdf5\n",
        "!wget --no-check-certificate --content-disposition \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTIAqIVGRJDE"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiVGLX3XGnGL"
      },
      "source": [
        "**3. Seq2Seq Bahdanau - Encodeur LSTM Bidirectionnel - Avec Schedule Sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eijTNAC_9Tqj"
      },
      "source": [
        "**Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10 :**\n",
        "- Encodeur Bi-directionnel LSTM #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 12h\n",
        "- Drop : 0.4\n",
        "- L2 : 0.01\n",
        "- Batch Size : 256\n",
        "- Périodes : 1000  \n",
        "- Décroissance LR : 0.01  \n",
        "- Schedule Sampling : k = max_periodes/10  \n",
        "=> mse : 0.0070 / 0.0133\n",
        "  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Train_Bahdanau_LSTM_ScheduleSample2.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tLGoCvbbUlc"
      },
      "source": [
        "model = CustomModel()\n",
        "model.compile(loss=\"mse\",metrics=\"mse\")\n",
        "model.fit(x=[x_train[0:1,:,:],y_train[0:1,:,:]],y=y_train[0:1,:,:], epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--9c2OEwbZhP"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46rSxuZbfEO"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions multi-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train,y_train],verbose=1)\n",
        "pred_val = model.predict([x_val,y_val],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6JOfhb5sukN"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Affiche les courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "# Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[longueur_sequence+i*longueur_sortie:longueur_sequence+(i+1)*longueur_sortie])\n",
        "  step_val.append(pred_ent[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[longueur_sequence+i*longueur_sortie])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[temps_separation+i*decalage+longueur_sequence:temps_separation+i*decalage+longueur_sequence+longueur_sortie])\n",
        "  step_val.append(pred_val[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[temps_separation+i*decalage+longueur_sequence])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "# Affiche les prédictions\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az6iW5lAFZ34"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
