{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_STAM_SML2010-MultiStep.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Ce dataset est utilisé pour effectuer la prédiction de la température d'une pièce en fonction de plusieurs paramètres mesurés. La fréquence originale des données est d'une minute, puis a été modifiée à 15minutes avec un filtrage. L'ensemble correspond environ à une durée de 40 jours.  \n",
        "Nous allons utiliser ici la température de la chambre comme cible et sélectionner 18 séries exogènes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.txt\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/NEW-DATA-1.T15.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_etude = pd.read_csv(\"NEW-DATA-1.T15.txt\",sep=\" \")\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtpJcijfabsQ"
      },
      "source": [
        "Supprime les colonnes non utiles :\n",
        " - Date et heure\n",
        " - Exterior Entalpic 1, 2 et turbo  \n",
        "   \n",
        "Déplace la cible (4:Temperature_Habitacion_Sensor) en dernière colonne :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oElnLHPailF"
      },
      "source": [
        "df_etude = df_etude.drop(['Date','Time','19:Exterior_Entalpic_1', '20:Exterior_Entalpic_2', '21:Exterior_Entalpic_turbo'],axis=1)\n",
        "cible = df_etude.pop(\"4:Temperature_Habitacion_Sensor\")\n",
        "df_etude.insert(len(df_etude.columns),\"Temperature_Habitacion_Sensor\",cible)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INH5D4lncQRY"
      },
      "source": [
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igyc5qUTcdXo"
      },
      "source": [
        "Modifie les type en float32 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_0svvF8chHQ"
      },
      "source": [
        "df_etude = df_etude.astype(dtype='float32')\n",
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['Temperature_Habitacion_Sensor'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude.values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-longueur_sortie:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128\n",
        "longueur_sequence = 20\n",
        "longueur_sortie = 5\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train = [X1,X2]\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_val = [X1,X2]\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle STAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0lDwKjfzzPh"
      },
      "source": [
        "Le modèle STAM implanté est le suivant :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYq66FyZeYV"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/VueGenerale.png?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "**1. Création de la couche de création des motifs spatiaux**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiCdmR0dnXp4"
      },
      "source": [
        "class CreationMotifsSpatiaux(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_motifs):\n",
        "    self.dim_motifs = dim_motifs\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    couches_denses = []\n",
        "    input_couches_denses = []\n",
        "\n",
        "    # Création des N entrées des couches denses\n",
        "    for i in range(input_shape[2]):\n",
        "      input_couches_denses.append(tf.keras.Input(shape=(input_shape[1])))                   # input = N*(batch_size,Tin)\n",
        "\n",
        "    # Création des N couches denses\n",
        "    for i in range(input_shape[2]):\n",
        "      couche_dense = tf.keras.layers.Dense(units=self.dim_motifs)(input_couches_denses[i])    # couche_dense : (batch_size,dim_motif)\n",
        "      couches_denses.append(couche_dense)\n",
        "\n",
        "    # Création de la sortie des N couches denses\n",
        "    out = tf.convert_to_tensor(couches_denses)                  # (N,batch_size,dim_motif)\n",
        "    out = tf.transpose(out,perm=[1,0,2])                        # (batch_size,N,dim_motif)\n",
        "\n",
        "    # Création du modèle global\n",
        "    self.Dense_Motifs = tf.keras.Model(inputs=input_couches_denses,outputs=out)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:            Séries exogènes & cibles  : (batch_size,Tin,N)\n",
        "  # Sorties :\n",
        "  #     space_motifs:     Motifs spatiaux           : (batch_size,N,dim_motif)\n",
        "  def call(self, input):\n",
        "    input_list = tf.unstack(input,axis=2)           # N*(batch_size,Tin)\n",
        "    space_motifs = self.Dense_Motifs(input_list)    # (batch_size,N,dim_motifs)\n",
        "    return space_motifs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM8tVDxynXp4"
      },
      "source": [
        "**2. Création de la couche de création des motifs temporels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8n_qKEnXp4"
      },
      "source": [
        "class CreationMotifsTemporels(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_motifs):\n",
        "    self.dim_motifs = dim_motifs\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_lstm1 = tf.keras.layers.LSTM(units=self.dim_motifs,return_sequences=True)\n",
        "    self.couche_lstm2 = tf.keras.layers.LSTM(units=self.dim_motifs,return_sequences=True)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:            Séries exogènes & cible  : (batch_size,Tin,N)\n",
        "  # Sorties :\n",
        "  #     temp_motifs:      Motifs temporels          : (batch_size,Tin,dim_motif)\n",
        "  def call(self, input):\n",
        "    out_lstm = self.couche_lstm1(input)            # (batch_size,Tin,dim_motif)\n",
        "    out_lstm = self.couche_lstm2(out_lstm)         # (batch_size,Tin,dim_motif)\n",
        "    return out_lstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXeI4FoknXp8"
      },
      "source": [
        "**3. Création de la couche d'attention de l'encodeur spatial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSCbKEKWnXp9"
      },
      "source": [
        "On commence par créer la couche de calcul du score de l'encodeur spatial.  \n",
        "Cette fonction est appellée par la couche d'attention spatiale à l'aide de la méthode TimeDistribued de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y3O02ua1gTp"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/EncodeurSpatialSTAM_CalculScore2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W59Yw8xVnXp9"
      },
      "source": [
        "class CalculScores_EncodeurSpatial(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                  # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.CoucheDenseScoresEncSpatial = tf.keras.layers.Dense(units=1,activation=\"relu\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTMG)\n",
        "  def SetStates(self,hidd_state):\n",
        "    self.hidd_state = hidd_state\n",
        "\n",
        "   # Entrées :\n",
        "  #     motifs :      Motifs spatiaux               : (batch_size,#space)\n",
        "  # Sorties :\n",
        "  #     score :       score                         : (batch_size,1)\n",
        "  def call(self,motifs):\n",
        "    motifs = tf.concat([motifs,self.hidd_state],axis=1)         # (batch_size,#space+#LSTMG)\n",
        "    score = self.CoucheDenseScoresEncSpatial(motifs)      # (batch_size,1)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5IMpHcgnXp9"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S231v-TnXp9"
      },
      "source": [
        "class CalculAttention_EncodeurSpatial(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_CalculScores_EncodeurSpatial = CalculScores_EncodeurSpatial()\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "\n",
        "  # Entrées :\n",
        "  #     input :       Motifs spatiaux               : (batch_size,N,#space)\n",
        "  #     hid_state :   Hidden state décodeur spatial : (batch_size,#LSTMG)\n",
        "  # Sorties :\n",
        "  #     vect_contexte   Vecteur Contexte    : (batch_size,#space)\n",
        "  def call(self, input,hid_state):\n",
        "    # Calcul des scores\n",
        "    self.couche_CalculScores_EncodeurSpatial.SetStates(hid_state)\n",
        "    b = tf.keras.layers.TimeDistributed(\n",
        "        self.couche_CalculScores_EncodeurSpatial)(input)                      # (batch_size,N,#space) : Timestep=N\n",
        "                                                                              # (batch_size,#space) envoyé N fois en //\n",
        "                                                                              # (batch_size,N,1) retourné\n",
        "    # Normalisation des scores\n",
        "    b = tf.keras.activations.softmax(b,axis=1)                                # (batch_size,N,1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    g = tf.multiply(input,b)        # (batch_size,N,#space)_x_(batch_size,N,1) = (batch_size,N,#space)\n",
        "    g = K.sum(g,axis=1)             # (batch_size,#space)\n",
        "    return g\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkRazn04eMLn"
      },
      "source": [
        "**4. Création de la couche d'attention de l'encodeur temporel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrlEgQIBeMLn"
      },
      "source": [
        "On commence par créer la couche de calcul du score de l'encodeur spatial.  \n",
        "Cette fonction est appellée par la couche d'attention spatiale à l'aide de la méthode TimeDistribued de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWBYt8MEeMLo"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/EncodeurTemporelSTAM_CalculScore2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iomd35dFeMLo"
      },
      "source": [
        "class CalculScores_EncodeurTemporel(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                  # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.CoucheDenseScoresEncTemporel = tf.keras.layers.Dense(units=1,activation=\"relu\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTMG)\n",
        "  def SetStates(self,hidd_state):\n",
        "    self.hidd_state = hidd_state\n",
        "\n",
        "   # Entrées :\n",
        "  #      motifs :      Motifs temporels               : (batch_size,#tempo)\n",
        "  # Sorties :\n",
        "  #     score :       score                         : (batch_size,1)\n",
        "  def call(self,motifs):\n",
        "    motifs = tf.concat([motifs,self.hidd_state],axis=1)         # (batch_size,#tempo+#LSTMS)\n",
        "    score = self.CoucheDenseScoresEncTemporel(motifs)           # (batch_size,1)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejtdRwKgeMLo"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOveEOgMeMLo"
      },
      "source": [
        "class CalculAttention_EncodeurTemporel(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_CalculScores_EncodeurTemporel = CalculScores_EncodeurTemporel()\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "\n",
        "  # Entrées :\n",
        "  #     motifs :      Motifs temporels               : (batch_size,T,#tempo)\n",
        "  #     hid_state :   Hidden state décodeur tempo    : (batch_size,#LSTMS)\n",
        "  # Sorties :\n",
        "  #     vect_contexte   Vecteur Contexte    : (batch_size,#tempo)\n",
        "  def call(self, input,hid_state):\n",
        "    # Calcul des scores\n",
        "    self.couche_CalculScores_EncodeurTemporel.SetStates(hid_state)\n",
        "    b = tf.keras.layers.TimeDistributed(\n",
        "        self.couche_CalculScores_EncodeurTemporel)(input)                     # (batch_size,T,#tempo) : Timestep=T\n",
        "                                                                              # (batch_size,#tempo) envoyé T fois en //\n",
        "                                                                              # (batch_size,T,1) retourné\n",
        "    # Normalisation des scores\n",
        "    b = tf.keras.activations.softmax(b,axis=1)                                # (batch_size,T,1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    g = tf.multiply(input,b)        # (batch_size,T,#tempo)_x_(batch_size,T,1) = (batch_size,T,#tempo)\n",
        "    g = K.sum(g,axis=1)             # (batch_size,#tempo)\n",
        "    return g\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9vEoEscnXp-"
      },
      "source": [
        "**5. Création de la couche du décodeur spatial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7o3Qr8g9GC"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/CoucheDecodeurSpatialSTAM.png?raw=true' width=800>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrm3oB_QhGt4"
      },
      "source": [
        "class DecodeurSpatial(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTMG,dim_G,regul,drop):\n",
        "    self.drop = drop\n",
        "    self.regul = regul\n",
        "    self.dim_LSTMG = dim_LSTMG          # Dimension des vecteurs cachés\n",
        "    self.dim_G = dim_G                  # dimension réduite du vecteur contexte\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.CoucheReductionSpatiale = tf.keras.layers.Dense(units=self.dim_G,activation=\"relu\")\n",
        "    self.CoucheLSTMG = tf.keras.layers.LSTM(units=self.dim_LSTMG,return_state=True,kernel_regularizer=tf.keras.regularizers.l2(self.regul),dropout=self.drop,recurrent_dropout=self.drop,name=\"LSTM_DecodeurSpatial\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "\n",
        "  # Entrées :\n",
        "  #     vc_spatial :      Vecteur Contexte spatial                : (batch_size,#space)\n",
        "  #     hid_state_1 :     Hidden state décodeur spatial (t-1)     : (batch_size,#LSTMG)\n",
        "  #     cell_state_1 :    Cell state décodeur spatial (t-1)       : (batch_size,#LSTMG)\n",
        "  #     y_pred_1 :        Prédiction (t-1)                        : (batch_size,1,1)\n",
        "  # Sorties :\n",
        "  #     hid_state :       Hidden state décodeur spatial (t)       : (batch_size,#LSTMG)\n",
        "  #     cell_state :      Cell state décodeur spatial (t)         : (batch_size,#LSTMG)\n",
        "\n",
        "  def call(self, vc_spatial,hid_state_1,cell_state_1,y_pred_1):\n",
        "    # Réduction du vecteur contexte\n",
        "    rg = self.CoucheReductionSpatiale(vc_spatial)                         # (batch_size,dim_G)\n",
        "\n",
        "    # Concaténation du vecteur réduit avec la prédictionà (t-1)\n",
        "    rg = tf.concat([rg,tf.squeeze(y_pred_1,-1)],axis=1)           # (batch_size,dim_G+1)\n",
        "    rg = tf.expand_dims(rg,-1)                                    # (batch_size,dim_G+1,1)\n",
        "\n",
        "    # Applique le résultat au LSTMG\n",
        "    out_dec, hid_state, cell_state = self.CoucheLSTMG(rg,initial_state=[hid_state_1,cell_state_1])         # hid_state / cell_state : (batch_size,#LSTMG)\n",
        "    return hid_state,cell_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3a9uSN9lCGY"
      },
      "source": [
        "**6. Création de la couche du décodeur temporel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwgRkVDHlCGY"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/CoucheDecodeurTemporelSTAM.png?raw=true' width=800>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fICDVgEqlCGZ"
      },
      "source": [
        "class DecodeurTemporel(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTMS,dim_S,regul,drop):\n",
        "    self.drop = drop\n",
        "    self.regul = regul\n",
        "    self.dim_LSTMS = dim_LSTMS          # Dimension des vecteurs cachés\n",
        "    self.dim_S = dim_S                  # dimension réduite du vecteur contexte\n",
        "    super().__init__()                  # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.CoucheReductionTemporelle = tf.keras.layers.Dense(units=self.dim_S,activation=\"relu\")\n",
        "    self.CoucheLSTMS = tf.keras.layers.LSTM(units=self.dim_LSTMS,return_state=True,kernel_regularizer=tf.keras.regularizers.l2(self.regul),dropout=self.drop,recurrent_dropout=self.drop,name=\"LSTM_DecodeurTemporel\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "\n",
        "  # Entrées :\n",
        "  #     vc_tempo :        Vecteur Contexte temporel               : (batch_size,#tempo)\n",
        "  #     hid_state_1 :     Hidden state décodeur tempo (t-1)       : (batch_size,#LSTMS)\n",
        "  #     cell_state_1 :    Cell state décodeur tempo (t-1)         : (batch_size,#LSTMS)\n",
        "  #     y_pred_1 :        Prédiction (t-1)                        : (batch_size,1,1)\n",
        "  # Sorties :\n",
        "  #     hid_state :       Hidden state décodeur tempo (t)         : (batch_size,#LSTMS)\n",
        "  #     cell_state :      Cell state décodeur tempo (t)           : (batch_size,#LSTMS)\n",
        "\n",
        "  def call(self, vc_tempo,hid_state_1,cell_state_1,y_pred_1):\n",
        "    # Réduction du vecteur contexte\n",
        "    rs = self.CoucheReductionTemporelle(vc_tempo)                 # (batch_size,dim_S)\n",
        "\n",
        "    # Concaténation du vecteur réduit avec la prédictionà (t-1)\n",
        "    rs = tf.concat([rs,tf.squeeze(y_pred_1,-1)],axis=1)           # (batch_size,dim_S+1)\n",
        "    rs = tf.expand_dims(rs,-1)                                    # (batch_size,dimg_S+1,1)\n",
        "\n",
        "    # Applique le résultat au LSTMG\n",
        "    out_dec, hid_state, cell_state = self.CoucheLSTMS(rs,initial_state=[hid_state_1,cell_state_1])         # hid_state / cell_state : (batch_size,#LSTMS)\n",
        "    return hid_state,cell_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUjhbMK8nXp-"
      },
      "source": [
        "**4. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqrD7BfM3eK0"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ReseauSTAM2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l25tCidCnXp-"
      },
      "source": [
        "class Net_STAM(tf.keras.layers.Layer):\n",
        "  def __init__(self,creationmotifsspatiaux, creationmotifstemporels, encodeurspatial,encodeurtemporel, decodeurspatial,decodeurtemporel,longueur_sequence, longueur_sortie, regul, drop,dim_LSTMG,dim_LSTMS):\n",
        "    self.creationmotifsspatiaux = creationmotifsspatiaux\n",
        "    self.creationmotifstemporels = creationmotifstemporels\n",
        "    self.encodeurspatial = encodeurspatial\n",
        "    self.encodeurtemporel = encodeurtemporel\n",
        "    self.decodeurspatial = decodeurspatial\n",
        "    self.decodeurtemporel = decodeurtemporel\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    self.dim_LSTMG = dim_LSTMG\n",
        "    self.dim_LSTMS = dim_LSTMS\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.CoucheConcatenation = tf.keras.layers.Dense(units=self.dim_LSTMG+self.dim_LSTMS,activation=\"tanh\")\n",
        "    self.CoucheGenerateur = tf.keras.layers.Dense(units=1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Series exogènes     : (batch_size,Tin,#dim)\n",
        "  #     cible:          Cible               : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction Y        : (batch_size,longueur_sortie,1)\n",
        "  def call(self,input,cible):\n",
        "    # Création des motifs spatiaux et temporels\n",
        "    motifs_spatiaux = self.creationmotifsspatiaux(tf.concat([input,cible],axis=2))      # (batch_size,N,#space) N=#dim+1\n",
        "    motifs_temporels = self.creationmotifstemporels(tf.concat([input,cible],axis=2))    # (batch_size,N,#tempo)\n",
        "\n",
        "    # Initialise les hidden_states et les cells_states\n",
        "    hid_state_spatial_1 = tf.matmul(input[:,0:1,0:1],tf.zeros(shape=(1,self.dim_LSTMG)))    # (batch_size,1,1)x(1,#LSTMG) = (batch_size,1,#LSTMG)\n",
        "    hid_state_spatial_1 = tf.transpose(hid_state_spatial_1,perm=[0,2,1])                    # (batch_size,#LSTMG,1)\n",
        "    hid_state_spatial_1 = tf.squeeze(hid_state_spatial_1,-1)                                # (batch_size,1)\n",
        "    cell_state_spatial_1 = hid_state_spatial_1\n",
        "\n",
        "    hid_state_temporel_1 = tf.matmul(input[:,0:1,0:1],tf.zeros(shape=(1,self.dim_LSTMS)))    # (batch_size,1,1)x(1,#LSTMS) = (batch_size,1,#LSTMS)\n",
        "    hid_state_temporel_1 = tf.transpose(hid_state_temporel_1,perm=[0,2,1])                   # (batch_size,#LSTMS,1)\n",
        "    hid_state_temporel_1 = tf.squeeze(hid_state_temporel_1,-1)                               # (batch_size,1)\n",
        "    cell_state_temporel_1 = hid_state_temporel_1\n",
        "\n",
        "    # Initialise la prédiction à t=0\n",
        "    y_pred_1 = tf.matmul(cible[:,0:1,:],tf.zeros(shape=(1,1)))    # (batch_size,1,1)x(1,1) = (batch_size,1,1)\n",
        "\n",
        "    sorties = []\n",
        "\n",
        "    for t in range(self.longueur_sortie):\n",
        "      # Calcul des vecteurs contextes spatiaux et temporels\n",
        "      g = self.encodeurspatial(motifs_spatiaux,hid_state_spatial_1)                               # (batch_size,#space)\n",
        "      s = self.encodeurtemporel(motifs_temporels,hid_state_temporel_1)                            # (batch_size,#tempo)\n",
        "\n",
        "      # Décodeur spatial & temporel\n",
        "      hid_state_dec_spatial,cell_state_dec_spatial = self.decodeurspatial(                        # (batch_size,#LSTMG)\n",
        "          g,hid_state_spatial_1,cell_state_spatial_1,y_pred_1)\n",
        "      hid_state_dec_temporel,cell_state_dec_temporel = self.decodeurtemporel(                     # (batch_size,#LSTMS)\n",
        "          s,hid_state_temporel_1,cell_state_temporel_1,y_pred_1)\n",
        "      \n",
        "      # Concaténation des hidden states et des vecteurs contextes\n",
        "      out_dec = tf.concat([hid_state_dec_spatial,hid_state_dec_temporel,g,s],axis=1)              # (batch_size,#LSTMG+LSTMS+#space+#tempo)\n",
        "\n",
        "      y_pred_1 = self.CoucheConcatenation(out_dec)                                                # (batch_size,#LSTMG+#LSTMS)\n",
        "      y_pred_1 = self.CoucheGenerateur(y_pred_1)                                                 # (batch_size,1)\n",
        "      y_pred_1 = tf.expand_dims(y_pred_1,-1)                                                      # (batch_size,1,1)\n",
        "\n",
        "      sorties.append(y_pred_1)\n",
        "      hid_state_spatial_1 = hid_state_dec_spatial\n",
        "      cell_state_spatial_1 = cell_state_dec_spatial\n",
        "      hid_state_temporel_1 = hid_state_dec_temporel\n",
        "      cell_state_temporel_1 = cell_state_dec_temporel\n",
        "\n",
        "    sorties = tf.convert_to_tensor(sorties)         # (longueur_sortie,batch_size,1,1)\n",
        "    sorties = tf.squeeze(sorties,-1)                # (longueur_sortie,batch_size,1)\n",
        "    sorties = tf.transpose(sorties,perm=[1,0,2])    # (batch_size,longueur_sortie,1)\n",
        "\n",
        "    return sorties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHECl3KPnXp_"
      },
      "source": [
        "**4. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9KMMJOqnXp_"
      },
      "source": [
        "dim_motifs_spatiaux = 128\n",
        "dim_motifs_temporels = 128\n",
        "\n",
        "dim_LSTMG = 128\n",
        "dim_G = 128\n",
        "dim_LSTMS = 128\n",
        "dim_S = 128\n",
        "drop=0.0\n",
        "l2reg=0.0\n",
        "\n",
        "def get_model():\n",
        "  entrees_exo = tf.keras.layers.Input(shape=(longueur_sequence,x_train[0].shape[2]))\n",
        "  entrees_cible = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "\n",
        "  creationmotifsspatiaux = CreationMotifsSpatiaux(dim_motifs=dim_motifs_spatiaux)\n",
        "  creationmotifstemporels = CreationMotifsTemporels(dim_motifs=dim_motifs_temporels)\n",
        "  encodeurspatial = CalculAttention_EncodeurSpatial()\n",
        "  encodeurtemporel = CalculAttention_EncodeurTemporel()\n",
        "  decodeurspatial = DecodeurSpatial(dim_LSTMG=dim_LSTMG,dim_G=dim_G,regul=l2reg,drop=drop)\n",
        "  decodeurtemporel = DecodeurTemporel(dim_LSTMS=dim_LSTMS,dim_S=dim_S,regul=l2reg,drop=drop)\n",
        "\n",
        "\n",
        "  sortie = Net_STAM(creationmotifsspatiaux,creationmotifstemporels,encodeurspatial,encodeurtemporel,decodeurspatial,decodeurtemporel,longueur_sequence,longueur_sortie,l2reg,drop,dim_LSTMG,dim_LSTMS)(entrees_exo,entrees_cible)\n",
        "\n",
        "  model = tf.keras.Model([entrees_exo,entrees_cible],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 500\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur)\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train[0],x_train[1]],y=y_train,validation_data=([x_val[0],x_val[1]],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=60)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBffY7lfwqJP"
      },
      "source": [
        "start = 400\n",
        "\n",
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[start:])),erreur_entrainement[start:], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[start:])),erreur_validation[start:], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfFTzSYfyIf"
      },
      "source": [
        "model.evaluate(x=[x_train[0],x_train[1]],y=y_train)\n",
        "model.evaluate(x=[x_val[0],x_val[1]],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Tx4V-RtSab"
      },
      "source": [
        "**STAM_SML2010-MultiStep**  \n",
        "\n",
        "  - Vecteurs LSTM : 128  \n",
        "  - Longueur entrée : 20  \n",
        "  - Longueur sortie : 5\n",
        "  - Drop : 0.0\n",
        "  - L2 : 0.00  \n",
        "  - Batch Size : 128  \n",
        "  - Périodes : 500   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqxGwP-jtu65"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNJdKLcVtv75"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Models/Multi_STAM_SML2010-Multistep_L20.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqWQrmr1tygK"
      },
      "source": [
        "model.load_weights(\"Multi_STAM_SML2010-Multistep_L20.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions Multistep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train[0],x_train[1]],verbose=1)\n",
        "pred_val = model.predict([x_val[0],x_val[1]],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Affiche les courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=tf.squeeze(serie_entrainement_X_norm[:,-1:],-1),line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=tf.squeeze(serie_test_X_norm[:,-1:],-1),line=dict(color='red', width=1)))\n",
        "\n",
        "#Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "#max = 10\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[longueur_sequence+i*longueur_sortie:longueur_sequence+(i+1)*longueur_sortie])\n",
        "  step_val.append(pred_ent[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[longueur_sequence+i*longueur_sortie])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pred_index,y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "#max = 10\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[temps_separation+i*decalage+longueur_sequence:temps_separation+i*decalage+longueur_sequence+longueur_sortie])\n",
        "  step_val.append(pred_val[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[temps_separation+i*decalage+longueur_sequence])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pred_index,y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "# Affiche les prédictions\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wCeoEnMEYv"
      },
      "source": [
        "**Erreurs en multi step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2_ceD69Mju6"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "#Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_ent = tf.keras.losses.mse(serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV5JTpnnNApH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_test = tf.keras.losses.mse(serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FShIaJINHBX"
      },
      "source": [
        "print(mse_ent)\n",
        "print(mse_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}