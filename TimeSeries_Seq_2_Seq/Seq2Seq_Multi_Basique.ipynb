{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_Multi_Basique.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/TimeSeries_Seq_2_Seq/Seq2Seq_Multi_Basique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Data/Power_PV.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_data = pd.read_csv(\"Power_PV.csv\")\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU5e7LEN5o23"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdlO2gPJ9B0t"
      },
      "source": [
        "Converison des types `object` en `float32` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbdm1hcH53tO"
      },
      "source": [
        "df_data.iloc[:,1:] = pd.DataFrame.replace(df_data.iloc[:,1:],\"?\",\"NaN\")\n",
        "df_data.iloc[:,1:] = df_data.iloc[:,1:].astype(np.float32)\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgPxFdsd5Ybv"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ShrX00CM5z"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2018-07-01 00:00:00\"\n",
        "date_fin = \"2019-04-14 00:00:00\"\n",
        "\n",
        "# Place l'index du dataframe sur la colonne Date\n",
        "df_data = df_data.rename(columns={'Unnamed: 0': \"Date\"})\n",
        "df_data = df_data.set_index(df_data['Date'])\n",
        "\n",
        "# Copie des données dans le dataframe d'étude sur l'intervalle d'étude\n",
        "df_etude = df_data.loc[date_debut:date_fin].copy()\n",
        "\n",
        "# Conversion de la colonne Date au format datetime\n",
        "df_etude.index = pd.to_datetime(df_etude.index)\n",
        "\n",
        "# Suppression de la colonne Unnamed:0\n",
        "df_etude = df_etude.drop(\"Date\", axis=1)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oOlwpLd9NdW"
      },
      "source": [
        "Vérification des données et correction des anomalies :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZGgjlBr8qMH"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXje2Zaill9l"
      },
      "source": [
        "df_etude = df_etude.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWNilOlvmChW"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=df_etude['watts'], line=dict(color='blue', width=1),name=\"Puissance (W)\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = df_etude['watts'].values\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,500))\n",
        "ax1.set_title(\"Autocorrélation\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 500))\n",
        "ax2.set_title(\"Autocorrélation partielle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLPlWzLJlNhA"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02JYhFxflROo"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude['watts'].values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude['watts'].values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude['watts'].values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6HRdVZmRM6"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JhXRi2DnAY"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsOFDQV_mVkf"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "serie_entrainement_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X),1)))\n",
        "serie_test_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_test_X,shape=(len(serie_test_X),1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44PYyswAlXFI"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm, label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm, label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  dataset = dataset.map(lambda x: (x[0:longueur_sequence][:,:],tf.expand_dims(x[-longueur_sortie:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128\n",
        "longueur_sequence = 5*24*4      # 4 jours (288*15min)\n",
        "longueur_sortie = 12*4        # 12h\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hppDt9HnNQU"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWR9V0WnOyR"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUAqFIpmnY8Y"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9aPVTcDn6TQ"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,3):\n",
        "  ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence),x_train[i],label=\"X_train\")\n",
        "  ax.plot(np.linspace(longueur_sequence,longueur_sequence+longueur_sortie-1,longueur_sortie),y_train[i],label=\"Y_train\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7dkj0dK2S7P"
      },
      "source": [
        "# Création du modèle Seq2Seq basique (avec encodeur simple GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqeBGpRB2S7r"
      },
      "source": [
        "Notre modèle Séquence vers Séquence est composé :\n",
        "- D'un encodeur, qui comprend une couche GRU uni-directionnelle\n",
        "- D'un décodeur, qui comprend une couche GRU uni-directionnelle. De plus, deux couches denses sont utilisées pour recréer la sortie univariée à partir des sorties des cellules GRU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqrQ2THM2S7s"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_basique_2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqqmIcUg2S7s"
      },
      "source": [
        "**1. Création de la couche d'encodeur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCPfeJq22S7s"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"GRU_Encodeur\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée encodeur       : (batch_size,1,1) \n",
        "  #     hidden:   Vecteur caché         : (batch_size,1,128)\n",
        "  # Sorties :\n",
        "  #     out_enc : Sortie encodeur       : (batch_size,1,128)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,1,128)\n",
        "  def call(self, input, hidden=None):\n",
        "    out_enc, out_hid = self.couche_GRU(input,initial_state=hidden)\n",
        "    return out_enc, out_hid\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__rii3mc2S7t"
      },
      "source": [
        "**2. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnMiDjg12S7t"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    self.init_state = True\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"GRU_Decodeur\")\n",
        "    self.couche_Dense = tf.keras.layers.Dense(units=1,kernel_regularizer=tf.keras.regularizers.l2(self.regul),input_dim=self.dim_GRU)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée décodeur       : (batch_size,1,1) \n",
        "  #     hidden:   Vecteur caché         : (batch_size,1,128)\n",
        "  # Sorties :\n",
        "  #     out_dec : Sortie décodeur       : (batch_size,128)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,128)\n",
        "  def call(self,input,hidden=None):\n",
        "    if self.init_state == True:\n",
        "      out_dec, out_hid = self.couche_GRU(input)\n",
        "      self.init_state = False\n",
        "    else:\n",
        "      out_dec, out_hid = self.couche_GRU(input,initial_state=hidden)\n",
        "    out_dec = self.couche_Dense(out_dec)\n",
        "    return out_dec, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSspq38H2S7u"
      },
      "source": [
        "**3. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ9tC3nJ2S7u"
      },
      "source": [
        "class Net_GRU(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size,longueur_sequence,1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    enc_out, enc_hid = self.encodeur(input[:,0:1,:])\n",
        "\n",
        "    for i in range(1,longueur_entree):\n",
        "      enc_out, enc_hid = self.encodeur(input[:,i:i+1,:],enc_hid)\n",
        "\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)\n",
        "    dec_hid = enc_hid\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, dec_hid)\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, dec_hid)\n",
        "        dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-amxhZb2S7u"
      },
      "source": [
        "**4. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foLcsFj72S7v"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Calcul du gradient\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Optimisation des poids\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul des predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Mise à jour des erreurs\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n40l4SabXU5"
      },
      "source": [
        "dim_GRU = 128\n",
        "drop=0.2\n",
        "l2reg=0.001\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "  sorties_sequences = tf.keras.layers.Input(shape=(longueur_sortie,1))\n",
        "  encodeur = Encodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "  sortie = Net_GRU(encodeur,decodeur,longueur_sequence=longueur_sequence, longueur_sortie=longueur_sortie)(entrees_sequences,sorties_sequences)\n",
        "  model = CustomModel([entrees_sequences,sorties_sequences],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_2pjIwEc9TH"
      },
      "source": [
        "# Création du modèle Seq2Seq basique avec encodeur LSTM-Bidirectionnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6y_St_0RHmq"
      },
      "source": [
        "Notre modèle Séquence vers Séquence est composé :\n",
        "- D'un encodeur, qui comprend une couche GRU uni-directionnelle\n",
        "- D'un décodeur, qui comprend une couche GRU uni-directionnelle. De plus, deux couches denses sont utilisées pour recréer la sortie univariée à partir des sorties des cellules GRU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uheOjyuOQ-rg"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_basique_2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8J4fmfWc9TV"
      },
      "source": [
        "**1. Création de la couche d'encodeur à base de LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AOxn0upx3-H"
      },
      "source": [
        "La couche Bidirectionnelle appliquée à une couche LSTM retourne 5 tenseurs :\n",
        "- La concaténation (ou la somme, en fonction de la méthode `merge` choisie) des états cachés avance / recul\n",
        "- Le dernier état caché du LSM d'avance : (batch_size, dim_LSTM*2) si concaténation ou (batch_size,dim_LSTM) si merge\n",
        "- Le derner cell-state du LSTM d'avance : (batch_size, dim_LSTM)\n",
        "- Le dernier état caché du LSTM de recul : (batch_size, dim_LSTM)\n",
        "- Le dernier cell-state du LSTM de recul : (batch_size, dim_LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqSeRbS7c9TW"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"LSTM_Encodeur\"),merge_mode=\"sum\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée encodeur       : (batch_size,1,1) \n",
        "  #     hidden:   Vecteur caché         : (batch_size,1,128)\n",
        "  # Sorties :\n",
        "  #     out_enc :   Sortie encodeur       : (batch_size,128)\n",
        "  #     out_hid_x : Sortie vecteur caché  : (batch_size,128)\n",
        "  #     state_c_x : Sortie cell state     : (btach_size,128)\n",
        "  def call(self, input, hidden=None):\n",
        "    out_enc, out_hid_1, state_c_1, out_hid_2, state_c_2 = self.couche_LSTM(input,initial_state=hidden)\n",
        "    return out_enc, out_hid_1, state_c_1, out_hid_2, state_c_2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBcCalqUc9TZ"
      },
      "source": [
        "**2. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb6xzfpjc9Ta"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    self.init_state = True\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"LSTM_Decodeur\")\n",
        "    self.couche_Dense = tf.keras.layers.Dense(units=1,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée décodeur       : (batch_size,1,1) \n",
        "  #     hidden:   Vecteur caché         : (batch_size,1,128)\n",
        "  # Sorties :\n",
        "  #     out_dec : Sortie décodeur       : (batch_size,1,1)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,1,128)\n",
        "  def call(self,input,hidden=None):\n",
        "    if self.init_state == True:\n",
        "      out_dec, out_hid, state_c = self.couche_LSTM(input)\n",
        "      self.init_state = False\n",
        "    else:\n",
        "      out_dec, out_hid, state_c = self.couche_LSTM(input,initial_state=hidden)\n",
        "    out_dec = self.couche_Dense(out_dec)\n",
        "    return out_dec, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7AGHa66c9Tc"
      },
      "source": [
        "**3. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-xaHB1vc9Tc"
      },
      "source": [
        "class Net_LSTM(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size,longueur_sequence,1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    enc_out, enc_hid_1, enc_state_1, enc_hid_2, enc_state_2 = self.encodeur(input[:,0:1,:])\n",
        "\n",
        "    for i in range(1,longueur_entree):\n",
        "      enc_out, enc_hid_1, enc_state_1, enc_hid_2, enc_state_2 = self.encodeur(input[:,i:i+1,:],[enc_hid_1, enc_state_1, enc_hid_2, enc_state_2])\n",
        "\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)\n",
        "    dec_hid = tf.keras.layers.Add()((enc_hid_1,enc_hid_2))\n",
        "    dec_state = tf.keras.layers.Add()((enc_state_1,enc_state_2))\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, [dec_hid,dec_state])\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, [dec_hid,dec_state])\n",
        "        dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eo-lt6tc9Te"
      },
      "source": [
        "**4. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoMrc-_pZ2WE"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Calcul du gradient\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Optimisation des poids\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul des predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Mise à jour des erreurs\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozgilOOUbJmb"
      },
      "source": [
        "dim_LSTM = 128\n",
        "drop=0.2\n",
        "l2reg=0.001\n",
        "\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "  sorties_sequences = tf.keras.layers.Input(shape=(longueur_sortie,1))\n",
        "\n",
        "\n",
        "  encodeur = Encodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "\n",
        "  sortie = Net_LSTM(encodeur,decodeur,longueur_sequence=longueur_sequence, longueur_sortie=longueur_sortie)(entrees_sequences,sorties_sequences)\n",
        "\n",
        "  model = CustomModel([entrees_sequences,sorties_sequences],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "  max_periodes = 1000\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train,y_train],y=y_train,validation_data=([x_val,y_val],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfFTzSYfyIf"
      },
      "source": [
        "model.evaluate(x=[x_train,y_train],y=y_train)\n",
        "model.evaluate(x=[x_val,y_val],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIBUbI1TICU"
      },
      "source": [
        "**1. Seq2Seq Basique - Encodeur unidirectionnel GRU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOahfnJ-yX1"
      },
      "source": [
        "**Seq2Seq_Basique_GRU128_DR02_L2_001_BS128_EP1000_5J_12H :**\n",
        "- Encodeur unidirectionnel GRU #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 24h\n",
        "- Drop : 0.2\n",
        "- L2 : 0.01\n",
        "- Batch Size : 128\n",
        "- Périodes : 1000  \n",
        "=> mse : 0.0227 / 0.0289  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Basique_GRU.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XKdrbK-022"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05bqnRr-pyf"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Basique_GRU128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Basique_GRU128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci9qh0TdTPD2"
      },
      "source": [
        "**2. Seq2Seq Basique - Encodeur LSTM Bidirectionnel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckazHYvhTfeF"
      },
      "source": [
        "**Seq2Seq_Basique_LSTMBiDir128_DR02_L2_001_BS128_EP1000_5J_12H :**\n",
        "- Encodeur Bi-directionnel LSTM #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 24h\n",
        "- Drop : 0.2\n",
        "- L2 : 0.01\n",
        "- Batch Size : 128\n",
        "- Périodes : 1000  \n",
        "=> mse : 0.0072 / 0.0131\n",
        "  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Basique_LSTMBiDir.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tLGoCvbbUlc"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--9c2OEwbZhP"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Basique_LSTMBiDir128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46rSxuZbfEO"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Basique_LSTMBiDir128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions multi-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train,y_train],verbose=1)\n",
        "pred_val = model.predict([x_val,y_val],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6JOfhb5sukN"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[longueur_sequence+i*longueur_sortie:longueur_sequence+(i+1)*longueur_sortie])\n",
        "  step_val.append(pred_ent[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[longueur_sequence+i*longueur_sortie])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[temps_separation+i*decalage+longueur_sequence:temps_separation+i*decalage+longueur_sequence+longueur_sortie])\n",
        "  step_val.append(pred_val[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[temps_separation+i*decalage+longueur_sequence])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az6iW5lAFZ34"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
