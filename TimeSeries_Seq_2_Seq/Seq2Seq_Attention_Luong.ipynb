{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_Attention_Luong.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/TimeSeries_Seq_2_Seq/Seq2Seq_Attention_Luong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-pPkMQp1JI"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place une attention de type Luong dans notre modèle de prédiction de séries temporelles Séquence vers Séquence.  \n",
        "Le papier de recherche sur lequel s'appuie ce modèle est disponible ici : [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Data/Power_PV.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_data = pd.read_csv(\"Power_PV.csv\")\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU5e7LEN5o23"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdlO2gPJ9B0t"
      },
      "source": [
        "Converison des types `object` en `float32` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbdm1hcH53tO"
      },
      "source": [
        "df_data.iloc[:,1:] = pd.DataFrame.replace(df_data.iloc[:,1:],\"?\",\"NaN\")\n",
        "df_data.iloc[:,1:] = df_data.iloc[:,1:].astype(np.float32)\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgPxFdsd5Ybv"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ShrX00CM5z"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2018-07-01 00:00:00\"\n",
        "date_fin = \"2019-04-14 00:00:00\"\n",
        "\n",
        "# Place l'index du dataframe sur la colonne Date\n",
        "df_data = df_data.rename(columns={'Unnamed: 0': \"Date\"})\n",
        "df_data = df_data.set_index(df_data['Date'])\n",
        "\n",
        "# Copie des données dans le dataframe d'étude sur l'intervalle d'étude\n",
        "df_etude = df_data.loc[date_debut:date_fin].copy()\n",
        "\n",
        "# Conversion de la colonne Date au format datetime\n",
        "df_etude.index = pd.to_datetime(df_etude.index)\n",
        "\n",
        "# Suppression de la colonne Unnamed:0\n",
        "df_etude = df_etude.drop(\"Date\", axis=1)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oOlwpLd9NdW"
      },
      "source": [
        "Vérification des données et correction des anomalies :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZGgjlBr8qMH"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXje2Zaill9l"
      },
      "source": [
        "df_etude = df_etude.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWNilOlvmChW"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=df_etude['watts'], line=dict(color='blue', width=1),name=\"Puissance (W)\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = df_etude['watts'].values\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,500))\n",
        "ax1.set_title(\"Autocorrélation\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 500))\n",
        "ax2.set_title(\"Autocorrélation partielle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLPlWzLJlNhA"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02JYhFxflROo"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude['watts'].values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude['watts'].values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude['watts'].values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6HRdVZmRM6"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JhXRi2DnAY"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsOFDQV_mVkf"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "serie_entrainement_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X),1)))\n",
        "serie_test_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_test_X,shape=(len(serie_test_X),1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44PYyswAlXFI"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm, label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm, label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  dataset = dataset.map(lambda x: (x[0:longueur_sequence][:,:],tf.expand_dims(x[-longueur_sortie:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 256\n",
        "longueur_sequence = 5*24*4      # 5 jours (288*15min)\n",
        "longueur_sortie = 12*4          # 12h\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hppDt9HnNQU"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWR9V0WnOyR"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUAqFIpmnY8Y"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9aPVTcDn6TQ"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,3):\n",
        "  ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence),x_train[i],label=\"X_train\")\n",
        "  ax.plot(np.linspace(longueur_sequence,longueur_sequence+longueur_sortie-1,longueur_sortie),y_train[i],label=\"Y_train\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7dkj0dK2S7P"
      },
      "source": [
        "# Codage des couches du modèle Seq2Seq avec attention de Luong (avec encodeur simple GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqeBGpRB2S7r"
      },
      "source": [
        "Notre modèle Séquence vers Séquence est composé :\n",
        "- D'un encodeur, qui comprend une couche GRU uni-directionnelle\n",
        "- D'un décodeur, qui comprend une couche GRU uni-directionnelle.\n",
        "- D'un système d'attention de type Luong dont le but est de décider quelle est la partie de la source la plus importante à chaque étape du décodage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqrQ2THM2S7s"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Luong_GRU2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK9B2ezPasjh"
      },
      "source": [
        "**1. Création de la couche d'attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzt2zFcVawlg"
      },
      "source": [
        "L'attention est calculée à chaque étape du décodage. Le but est de décider quelles sont les parties les plus importante dans la source.  \n",
        "Pour cela, à chaque étape du décodage le calcul d'attention est effectué de la manière suivante :\n",
        "- L'attention reçoit l'ensemble des états cachés de l'encodeur Enc_out (batch_size, Tin, GRU) ainsi que l'état caché du décodeur à l'étape précédente Hid_state (batch_size,GRU)\n",
        "- A partir de ces deux informations, elle calcule un score d'attention alpha. Ce score représente l'importance de l'état caché issu du décodeur à l'étape de décodage courante. Dans la méthode de Luong, ce score est calculé en recherchant les similarités entre le vecteur caché et les représentations cachées des différents vecteurs issus de l'encodeur.\n",
        "- A partir des scores d'attention, elle calcule les poids d'attention à attribuer à chaque état caché de l'encodeur (Enc_out#1, Enc_out#2, ...). Ces poids représentent des probabilités et sont obtenus en appliquant une fonction de type Softmax sur les scores.\n",
        "- Enfin, elle applique les poids d'attention sur les états cachés de l'encodeur en effectuant la somme pondérée de ces états cachés avec les poids.\n",
        "- Le résultat obtenu est le vecteur contexte C qui sera ensuite concaténé avec l'entrée et injecté dans la cellule GRU de décodage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfhrXQJ6Fo-"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/CalculAttentionLuong2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOiyFNN8sq56"
      },
      "source": [
        "# Classe d'attention de Luong\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self):\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = self.add_weight(shape=(input_shape[2],input_shape[2]),initializer=\"normal\",name=\"W\")   # (#GRU, #GRU)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Entrées : enc_out : Etats cachés de l'encodeur (batch_size, Tin, #GRU)\n",
        "  #           hid_dec : Etat caché du décodeur (batch_size, #GRU)\n",
        "  # Sortie :  vc :      Vecteur contexte (batch_size,#GRU) \n",
        "  def call(self,enc_out,hid_dec):\n",
        "    # Calcul de Xh contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de l'encodeur\n",
        "    xt = tf.transpose(enc_out,perm=[0,2,1])    # (batch_size,Tin,#GRU) => (batch_size,#GRU,Tin)\n",
        "    Xh = tf.matmul(self.W,xt)                  # (#GRU,#GRU)x(batch_size,#GRU,Tin) = (batch_size,#GRU,Tin)\n",
        "\n",
        "    # Calcul des poids d'attention normalisés\n",
        "    Xh = tf.transpose(Xh,perm=[0,2,1])        # Xh = (batch_size,#GRU,Tin) => (batch_size,Tin,#GRU)\n",
        "    hid_dec = tf.expand_dims(hid_dec,-1)      # (batch_size,#GRU) => (batch_size,#GRU,1)\n",
        "    a = tf.matmul(Xh,hid_dec)                 # (batch_size,Tin,#GRU)x(batch_size,#GRU,1) = (batch_size,Tin,1)\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    xa = tf.multiply(enc_out,a)               # (batch_size,Tin,#GRU)_x_(batch_size,Tin,1) = (batch_size,Tin,#GRU)\n",
        "    vc = K.sum(xa,axis=1)                     # vc = (batch_size,#GRU)\n",
        "    return vc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqqmIcUg2S7s"
      },
      "source": [
        "**2. Création de la couche d'encodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ8pgIJ4YE1d"
      },
      "source": [
        "Dans le document de recherche de Luong, l'encodeur utilisé est de type GRU unidirectionnel.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFpxmR0mxSq"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Encodeur_Luong.png?raw=true' width=400/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCPfeJq22S7s"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=True,return_state=True,dropout=self.drop,name=\"GRU_Encodeur\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée encodeur       : (batch_size,Tin,1) \n",
        "  # Sorties :\n",
        "  #     out_enc : Sortie encodeur       : (batch_size,Tin,#GRU)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,#GRU)\n",
        "  def call(self, input):\n",
        "    out_enc, out_hid = self.couche_GRU(input)\n",
        "    return out_enc, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__rii3mc2S7t"
      },
      "source": [
        "**3. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-jGzrstYSd0"
      },
      "source": [
        "On utilise un réseau de type GRU unidirectionnel et on réalise cette combinaison linéaire à l'extérieur du réseau. Dans le document de recherche de Luong, le vecteur contexte et le vecteur caché issu de la cellule GRU sont combinés ensemble en y appliquant un poids et une fonction tanh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Pey2RHm5FE"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Decodeur_Luong.png?raw=true'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnMiDjg12S7t"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"GRU_Decodeur\")\n",
        "    self.Wc = tf.keras.layers.Dense(units=self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),input_dim=2*self.dim_GRU,activation=\"tanh\")\n",
        "    self.couche_Dense_Generateur = tf.keras.layers.Dense(units=1,kernel_regularizer=tf.keras.regularizers.l2(self.regul),input_dim=self.dim_GRU)\n",
        "    self.Attention = Couche_Attention()\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     dec_input:    Entrée décodeur       : (batch_size,1,1) \n",
        "  #     enc_out:      Sortie encodeur       : (batch_size,Tin,#GRU)\n",
        "  #     hidden:       Vecteur caché         : (batch_size,#GRU)\n",
        "  # Sorties :\n",
        "  #     out_dec : Sortie décodeur       : (batch_size,1)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,#GRU)\n",
        "  def call(self,dec_input, enc_out,hidden):\n",
        "    # Calcul du hidden state courant\n",
        "    out_dec, out_hid = self.couche_GRU(dec_input,initial_state=hidden)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    vect_contexte = self.Attention(enc_out,out_hid)          # (batch_size,#GRU)\n",
        "\n",
        "    # Concaténation du vecteur contexte\n",
        "    # et du hidden state courant\n",
        "    add = tf.concat([vect_contexte,out_hid],axis=1)        # Concat {(batch_size,#GRU),(batch_size,#GRU)} = ((batch_size,#2*GRU))\n",
        "\n",
        "    # Application de la couche dense Wc\n",
        "    out_dec = self.Wc(add)                                 # (batch_size,GRU)\n",
        "\n",
        "    # Calcul de la sortie\n",
        "    out_dec = self.couche_Dense_Generateur(out_dec)\n",
        "    return out_dec, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSspq38H2S7u"
      },
      "source": [
        "**4. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20vrAwPY7Wc"
      },
      "source": [
        "Dans le document de recherche, l'état caché initial du décodeur est calculé à partir du premier état caché (sens retour) de l'encodeur. Ici, on calcule cet état initial en utlilisant le dernier état caché de l'encodeur. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Jd51Jhnc97"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Luong_GRU2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ9tC3nJ2S7u"
      },
      "source": [
        "class Net_GRU(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Ws = tf.keras.layers.Dense(units=self.dim_GRU,activation=\"tanh\",use_bias=False)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size, Tin, 1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False, proba=1.0):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    # Encodage de la séquence d'entrée\n",
        "    enc_out, enc_hid = self.encodeur(input)       # (batch_size,Tin,#GRU), (batch_size,#GRU)\n",
        "\n",
        "    # Calcul de l'état caché initial du décodeur\n",
        "    dec_hid = self.Ws(enc_hid)                    # s0 : (batch_size,#GRU)\n",
        "\n",
        "    # La première entrée du décodeur est la\n",
        "    # dernière entrée de l'encodeur\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)   # Y0 : (batch_size,1,1)\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, enc_out, dec_hid)\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, enc_out, dec_hid)\n",
        "        # Tirage au sort\n",
        "        tirage = np.random.binomial(n=1,size=1,p=proba)\n",
        "        if tirage == 1:\n",
        "          dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        else:\n",
        "          dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFQB6v98B-MN"
      },
      "source": [
        "**5. Création du modèle avec Schedule Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fAtwB8KcygR"
      },
      "source": [
        "n_periodes = 1000\n",
        "\n",
        "k = n_periodes/10       # Taux de décroissance\n",
        "\n",
        "x = np.linspace(0,n_periodes,n_periodes+1)\n",
        "y = k/(k+np.exp(x/k))\n",
        "\n",
        "plt.plot(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNh-GvpeCB_s"
      },
      "source": [
        "dim_GRU = 128\n",
        "drop=0.4\n",
        "l2reg=0.01\n",
        "max_periodes = 1000\n",
        "k = max_periodes/10\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.periode = 0                  # Période courante\n",
        "        self.k = k                        # Facteur de décroissance\n",
        "        self.proba = 1.0                  # Proba du batch\n",
        "        self.encodeur = Encodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "        self.decodeur = Decodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "        self.Net_GRU = Net_GRU(self.encodeur,self.decodeur,longueur_sequence=longueur_sequence, longueur_sortie=longueur_sortie)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        sortie = self.Net_GRU(inputs[0],inputs[1],training=training, proba=self.proba)\n",
        "        return sortie\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Calcul du gradient\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Optimisation des poids\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul des predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Mise à jour des erreurs\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = CustomModel()\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train,y_train],y=y_train,validation_data=([x_val,y_val],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfFTzSYfyIf"
      },
      "source": [
        "model.evaluate(x=[x_train,y_train],y=y_train)\n",
        "model.evaluate(x=[x_val,y_val],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eijTNAC_9Tqj"
      },
      "source": [
        "**Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10 :**\n",
        "- Encodeur Bi-directionnel LSTM #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 12h\n",
        "- Drop : 0.4\n",
        "- L2 : 0.01\n",
        "- Batch Size : 256\n",
        "- Périodes : 1000  \n",
        "- Décroissance LR : 0.01  \n",
        "- Schedule Sampling : k = max_periodes/10  \n",
        "=> mse : 0.0070 / 0.0133\n",
        "  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Train_Bahdanau_LSTM_ScheduleSample2.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tLGoCvbbUlc"
      },
      "source": [
        "model = CustomModel()\n",
        "model.compile(loss=\"mse\",metrics=\"mse\")\n",
        "model.fit(x=[x_train[0:1,:,:],y_train[0:1,:,:]],y=y_train[0:1,:,:], epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--9c2OEwbZhP"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46rSxuZbfEO"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Bahdanau_LSTMBiDir128_DR04_L2_01_BS256_EP1000_DLR_001_5J_12H_SS10.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions multi-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train,y_train],verbose=1)\n",
        "pred_val = model.predict([x_val,y_val],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6JOfhb5sukN"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Affiche les courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "# Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[longueur_sequence+i*longueur_sortie:longueur_sequence+(i+1)*longueur_sortie])\n",
        "  step_val.append(pred_ent[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[longueur_sequence+i*longueur_sortie])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[temps_separation+i*decalage+longueur_sequence:temps_separation+i*decalage+longueur_sequence+longueur_sortie])\n",
        "  step_val.append(pred_val[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[temps_separation+i*decalage+longueur_sequence])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "# Affiche les prédictions\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az6iW5lAFZ34"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}