{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_HRHN_VAR_PID_SML2010_MultiStep.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrOOJabrUaZ2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **HRHN + VAR + Correcteur PID**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPwh4S_BN__E"
      },
      "source": [
        "!pip install -q 'ray[tune]' 'ray[default]'\n",
        "!pip install -q --upgrade aioredis==1.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Le dataset utilisé contient les prix des actions des 81 principales compagnies du NASDAQ100. La valeur de l'index du NASDAQ est utilisé comme cible.  \n",
        "La fréquence des information est d'une minute, depuis le 26 juillet 2016 jusqu'au 22 décembre 2016, soit 105 jours au total (les samedi et dimanche ne sont pas comptés, ainsi que le 25 novembre qui ne possède que 210 données et le 22 décembre qui n'en possède que 180)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/Stock_complet.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_etude = pd.read_csv(\"Stock_complet.csv\")\n",
        "df_etude = df_etude.drop(columns='DateTime')\n",
        "df_etude = df_etude.astype('float32')\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INH5D4lncQRY"
      },
      "source": [
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['C_AAPL'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "# 80% / 10% / 10%\n",
        "\n",
        "temps_separation = int(len(df_etude.values) * 0.8)\n",
        "temps_separation_PID = int(len(df_etude.values) * 0.9)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "date_separation_PID = df_etude.index[temps_separation_PID]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "serie_entrainement_PID_X = np.array(df_etude.values[temps_separation:temps_separation_PID],dtype=np.float32)\n",
        "serie_test_PID_X = np.array(df_etude.values[temps_separation_PID:],dtype=np.float32)\n",
        "\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))\n",
        "print(\"Taille de l'entrainement PID : %d\" %len(serie_entrainement_PID_X))\n",
        "print(\"Taille de la validation PID: %d\" %len(serie_test_PID_X))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "serie_entrainement_PID_X_norm = []\n",
        "serie_test_PID_X_norm = []\n",
        "\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "  serie_entrainement_PID_X_norm.append(serie_entrainement_PID_X[:,i])\n",
        "  serie_test_PID_X_norm.append(serie_test_PID_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "serie_entrainement_PID_X_norm = tf.convert_to_tensor(serie_entrainement_PID_X_norm)\n",
        "serie_entrainement_PID_X_norm = tf.transpose(serie_entrainement_PID_X_norm)\n",
        "serie_test_PID_X_norm = tf.convert_to_tensor(serie_test_PID_X_norm)\n",
        "serie_test_PID_X_norm = tf.transpose(serie_test_PID_X_norm)\n",
        "\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)\n",
        "serie_entrainement_PID_X_norm = min_max_scaler.transform(serie_entrainement_PID_X_norm)\n",
        "serie_test_PID_X_norm = min_max_scaler.transform(serie_test_PID_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)\n",
        "print(serie_entrainement_PID_X_norm.shape)\n",
        "print(serie_test_PID_X_norm.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:temps_separation_PID].values,serie_entrainement_PID_X_norm[:,0:5], label=\"X_PID_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation_PID:].values,serie_test_PID_X_norm[:,0:5], label=\"X_PID_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i817Q5rwIL_x"
      },
      "source": [
        "Les datasets sont créés de la manière suivante :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwssikbyIEzc"
      },
      "source": [
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ConstructionDataset.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLEjFxNnyWse"
      },
      "source": [
        "**1. Préparation des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 512\n",
        "longueur_sequence = 30\n",
        "longueur_sortie = 1\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "\n",
        "dataset_PID = prepare_dataset_XY(serie_entrainement_PID_X_norm[:,0:-1],serie_entrainement_PID_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_PID_val = prepare_dataset_XY(serie_test_PID_X_norm[:,0:-1],serie_test_PID_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "def Create_train(dataset):\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "\n",
        "  # Extrait les X,Y du dataset\n",
        "  x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                          # y=43x(BS,1,1)\n",
        "  for i in range(len(x)):\n",
        "    X1.append(x[i][0])          \n",
        "    X2.append(x[i][1])\n",
        "\n",
        "  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "  # Recombine les données\n",
        "  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "  x_train = [X1,X2]\n",
        "  y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "  return x_train,y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVbVOFDwERm-"
      },
      "source": [
        "x_train, y_train = Create_train(dataset)\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "def Create_val(dataset_val):\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "\n",
        "  # Extrait les X,Y du dataset\n",
        "  x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                          # y=43x(BS,1,1)\n",
        "  for i in range(len(x)):\n",
        "    X1.append(x[i][0])          \n",
        "    X2.append(x[i][1])\n",
        "\n",
        "  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "  # Recombine les données\n",
        "  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "  x_val = [X1,X2]\n",
        "  y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "  return x_val,y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTzC0c34Enxr"
      },
      "source": [
        "x_val,y_val = Create_val(dataset_val)\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle HRHN ENC/DEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBvAzS8KtFlp"
      },
      "source": [
        "Le modèle HRHN est décrit dans ce document de recherche : [Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction](https://arxiv.org/pdf/1806.00685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKq0mqg2ts2w"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Mod%C3%A8leHRHN1.png?raw=true' width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "**1. Création de l'encodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tn5xnfnuehY"
      },
      "source": [
        "L'encodeur a pour but de créer des représentations cachées des séries exogènes qui prennent en compte les relations spatiales entre ces séries ainsi que les relations temporelles.  \n",
        "Les relations spatiales sont extraitent à l'aide d'un ensemble de réseaux de convolution qui produisent des représentations w1, w2... w(T-1).  \n",
        "Ces représentations sont ensuites codées par un réseau RHN à 3 couches afin d'en extraire les relations temporelles. En sortie de ce réseau RHN, on extrait 3 tenseurs dont chacun contient les (T-1) états cachés de chaque couche du réseau RHN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0mq5BSauQUq"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-boowSGDp3"
      },
      "source": [
        "***a. Création des CNN parallèlisés***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dF5Cp9vzWB"
      },
      "source": [
        "La structure d'un réseau de convolution est composée de trois couches CNN-1D + Max-pooling :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jga5_ZzKv6CI"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN1.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CMXl2tgwJ76"
      },
      "source": [
        "L'intégration de caque réseau dans Keras est parallélisée :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtR-u7ZqwjTL"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ETqqvAIGKLi"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "\n",
        "class Encodeur_CNN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling,dim_motif):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  # Création de Tin réseaux de convolution + max_pooling en //\n",
        "  ############################################################\n",
        "  def build(self,input_shape):\n",
        "    convs = []\n",
        "    input_cnns = []\n",
        "\n",
        "    # Création des Tin entrées des réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "        input_cnns.append(tf.keras.Input(shape=(input_shape[2],1)))       # input = Tin*(batch_size,#dim,1)\n",
        "\n",
        "    # Création des Tin réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "      conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[0],      # conv : (batch_size,#dim,16)\n",
        "                                    kernel_size=self.dim_filtres_cnn[0],\n",
        "                                    activation='relu',\n",
        "                                    padding='same',\n",
        "                                    strides=1)(input_cnns[i])\n",
        "      conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[0],      # conv : (batch_size,#pooling1,16)\n",
        "                                       padding='same')(conv)\n",
        "      for n in range(1,len(self.dim_filtres_cnn)):\n",
        "        conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                      kernel_size=self.dim_filtres_cnn[n],\n",
        "                                      activation='relu',\n",
        "                                      padding='same',\n",
        "                                      strides=1)(conv)\n",
        "        conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                         padding='same')(conv)\n",
        "      convs.append(conv)\n",
        "    \n",
        "    # Création de la sortie concaténée des Tin réseaux CNN\n",
        "    out = tf.convert_to_tensor(convs)                                     # out : (Tin,batch_size,#pooling,64)\n",
        "    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,Tin,#pooling,64)\n",
        "    out = tf.keras.layers.Reshape(                                        # out : (batch_size,Tin,#pooling*64)\n",
        "        target_shape=(out.shape[1],out.shape[2]*out.shape[3]))(out)\n",
        "\n",
        "    if self.dim_motif == 0:\n",
        "      out = tf.keras.layers.Dense(units=out.shape[2])(out)                  # out : (batch_size,Tin,dim_motif = #pooling*64) \n",
        "    else:\n",
        "      out = tf.keras.layers.Dense(units=self.dim_motif)(out)                # out : (batch_size,Tin,dim_motif) \n",
        "\n",
        "    # Création du modèle global\n",
        "    self.conv_model = tf.keras.Model(inputs=input_cnns,outputs=out)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:  Entrée séries exogènes  : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     w:      Sorties des motifs CNN  : (batch_size,Tin,#dim_motif)\n",
        "  #                                       (taille dernier filtre=64)\n",
        "  def call(self, input):\n",
        "    # Coupes temporelles sur les séries exogènes\n",
        "    # au format : Tin*(batch_size,#dim,1)\n",
        "    input_list = []\n",
        "    for i in range(input.shape[1]):\n",
        "      input_list.append(tf.transpose(input[:,i:i+1,:],perm=[0,2,1]))      # (batch_size,#dim,1)\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.conv_model(input_list)                                       # (batch_size,Tin,dim_motif)\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_y68mmiRpR1"
      },
      "source": [
        "***b. Création des cellules RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FQ47zOsxHpx"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_RHN.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvNrS-wQ1B5C"
      },
      "source": [
        "On crée une cellule RHN en reprenant le code précédent auquel :  \n",
        "- On ajoute la possibilité de retourner tous les états cachés de chaque couche\n",
        "- On ajoute la prise en compte de la dimension d'entrée correspondant à la dimension des motifs en sortie des réseaux CNN (dim_motif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CygD9DXbBTDJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Structure_RHN4.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HldCwl9z3fY"
      },
      "source": [
        "class Cellule_RHN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_RHN, nbr_couches, return_all_states = False, dim_input=1):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches = nbr_couches\n",
        "    self.dim_input = dim_input\n",
        "    self.return_all_states = return_all_states\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wh = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wh\")       # (#dim, #RHN)\n",
        "    self.Wt = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wt\")       # (#dim, #RHN)\n",
        "    self.Wc = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wc\")       # (#dim, #RHN)\n",
        "\n",
        "    self.Rh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rh\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rt\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rc\")      # (n_couches,#RHN, #RHN)\n",
        "\n",
        "    self.bh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bh\")        # (n_couches,#RHN, 1)\n",
        "    self.bt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bt\")        # (n_couches,#RHN, 1)\n",
        "    self.bc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bc\")        # (n_couches,#RHN, 1)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "    # Initialisation des masques de dropout\n",
        "  def InitMasquesDropout(self,drop=0.0):\n",
        "    self.Wh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Rh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X[t]        : (batch_size,1,#dim)\n",
        "  #     init_hidden:    Etat caché Init.    : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     sL:             Etat caché de la dernière couche       : (batch_size,#RHN) \n",
        "  #           ou        Etats cachés de chaque couche SL[t]    : (batch_size,nbr_couches,#RHN)\n",
        "  def call(self, input, init_hidden=None):\n",
        "    # Construction d'un vecteur d'état nul si besoin\n",
        "    if init_hidden == None:\n",
        "      init_hidden = tf.matmul(tf.zeros(shape=(self.dim_RHN,input.shape[2])), # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "                              tf.transpose(input,perm=[0,2,1]))\n",
        "      init_hidden = tf.squeeze(init_hidden,-1)                               # (batch_size,#RHN,1) => (batch_size,#RHN)\n",
        "  \n",
        "    liste_sl = []                                                            # Liste pour  enregistrer les états cachés de chaque couche\n",
        "    # Calcul de hl, tl et cl\n",
        "    for i in range(self.nbr_couches):\n",
        "      if i==0:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[0,:,:],self.Rh[0,:,:])                      # (#RHN,1)_x_(#RHN,#RHN) = (#RHN,#RHN)\n",
        "        Rt = tf.multiply(self.Rt_[0,:,:],self.Rt[0,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[0,:,:],self.Rc[0,:,:])\n",
        "\n",
        "        Wh = tf.multiply(self.Wh_,self.Wh)                                    # (#dim,1)_x_(#dim,#RHN) = (#dim,#RHN)\n",
        "        Wt = tf.multiply(self.Wt_,self.Wt)\n",
        "        Wc = tf.multiply(self.Wc_,self.Wc)\n",
        "   \n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + tf.matmul(tf.transpose(Wh),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + tf.matmul(tf.transpose(Wt),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + tf.matmul(tf.transpose(Wc),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "\n",
        "      else:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[i,:,:],self.Rh[i,:,:])\n",
        "        Rt = tf.multiply(self.Rt_[i,:,:],self.Rt[i,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[i,:,:],self.Rc[i,:,:])\n",
        "\n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "      \n",
        "      # Calcul de sl\n",
        "      sl = tf.keras.layers.multiply([hl,tl])                                # (batch_size,#RHN)\n",
        "      sl = sl + tf.keras.layers.multiply([init_hidden,cl])                  # (batch_size,#RHN)\n",
        "      liste_sl.append(sl)       # Sauvegarde l'état caché de la couche courante\n",
        "      init_hidden = sl\n",
        "    if self.return_all_states == False:\n",
        "      return sl\n",
        "    else:\n",
        "      liste_sl = tf.convert_to_tensor(liste_sl)                             # (nbr_couches,batch_size,#RHN)\n",
        "      liste_sl = tf.transpose(liste_sl,perm=[1,0,2])                        # (batch_size,nbr_couches,#RHN)\n",
        "      return liste_sl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJY-TY7W55ZF"
      },
      "source": [
        "***c. Création de l'encodeur : Convolutions + RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMT8C8-UVe2O"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYQu67IfBdel"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "#   dim_motif         :   dimension du motif en sortie du CNN\n",
        "#   dim_RHN_enc       :   dimension du vecteur caché RHN\n",
        "#   nbr_couches_RHN   :   nombre de couches du RHN\n",
        "#   dropout           :   dropout variationnel pour le RHN ex: [0.1]\n",
        "\n",
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling, dim_motif,dim_RHN_enc,nbr_couches_RHN, dropout=0.0):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    self.dim_RHN_enc = dim_RHN_enc\n",
        "    self.nbr_couches_RHN = nbr_couches_RHN\n",
        "    self.dropout = dropout\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.encodeur_cnn = Encodeur_CNN(dim_filtres_cnn=self.dim_filtres_cnn,nbr_filtres_cnn=self.nbr_filtres_cnn,dim_max_pooling=self.dim_max_pooling,dim_motif=self.dim_motif)\n",
        "    self.RHN = Cellule_RHN(dim_RHN=self.dim_RHN_enc,nbr_couches=self.nbr_couches_RHN,return_all_states=True,dim_input=self.dim_motif)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:          Entrées X         : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     hidden_states   Vecteurs cachés   : (batch_size,nbr_couches,Tin,#RHN)\n",
        "  def call(self, input):\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.encodeur_cnn(input)      #  (batch_size,Tin,dim_motif)\n",
        "\n",
        "    # Encodage des motifs CNN avec les cellules RHN\n",
        "    sequence = []\n",
        "    hidden = None\n",
        "\n",
        "    # Initialisation des masques de dropout pour tous les pas de temps\n",
        "    self.RHN.InitMasquesDropout(self.dropout)\n",
        "\n",
        "    # Applique la cellule RHN à chaque pas de temps\n",
        "    for i in range(input.shape[1]):\n",
        "      hidden = self.RHN(w[:,i:i+1,:],hidden)          # Envoie (batch_size,1,dim_motif)\n",
        "      sequence.append(hidden)                         # Sauve (batch_size,nbr_couches,#RHN)\n",
        "\n",
        "      # Le premier état caché du prochain instant\n",
        "      # est l'état caché de la dernière couche précédente\n",
        "      hidden = hidden[:,self.nbr_couches_RHN-1,:]       # (batch_size,#RHN)\n",
        "\n",
        "    # Traite le format des vecteurs cachés de l'encodeur\n",
        "    sequence = tf.convert_to_tensor(sequence)               # (Tin,batch_size,nbr_couches,#RHN)\n",
        "    hidden_states = tf.transpose(sequence,perm=[1,2,0,3])   # (batch_size,nbr_couches,Tin,#RHN)  \n",
        "\n",
        "    return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__CJ4O7yJne3"
      },
      "source": [
        "**2. Création du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2yWQeaJwNn"
      },
      "source": [
        "Le décodeur prend en entrée et à chaque pas de temps :  \n",
        "- Le tenseur en sortie de l'encodeur RHN qui contient l'ensemble des vecteurs cachés des différentes couches : (batch_size,Nbr_couches,Tin,#RHN)\n",
        "- L'état caché de la dernière couche du décodeur RHN précédent : (batch_size,#RHN)\n",
        "- La valeur de la série cible à l'instant courant : (batch_size,1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtYLxAoIK8Xn"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_VueEnsembleDecodeur2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjHiRZZbLief"
      },
      "source": [
        "**a. Création de la couche d'attention hiérarchique**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JX5hGeWNN8w"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_AttentionHierarchique.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9p7ylHmY6gS"
      },
      "source": [
        "On commence par créer la fonction permettant de calculer les scores. Cette fonction sera appelée avec la méthode TimeDistributed de Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvaGAb0uY1XL"
      },
      "source": [
        "class CalculScore(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_RHN_dec):\n",
        "    self.dim_RHN_dec = dim_RHN_dec\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.T = self.add_weight(shape=(input_shape[1],self.dim_RHN_dec),initializer=\"normal\",name=\"T\")   # (#RHN_enc, #RHN_dec)\n",
        "    self.U = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"U\")     # (#RHN_enc, #RHN_enc)\n",
        "    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"b\")                  # (#RHN_enc, 1)\n",
        "    self.v = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"v\")                  # (#RHN_enc, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  #     hid_state:  Etat initial RHN          : (batch_size,#RHN_dec)\n",
        "  def SetInitState(self,hid_state):\n",
        "    self.hid_state = hid_state\n",
        "\n",
        "  def compute_output_shape(self,input_shape):\n",
        "    return(input_shape[0],1)\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      1 sortie encodeur RHN     : (batch_size,#RHN_enc)\n",
        "  # Sorties :\n",
        "  #     score:      score                     : (batch_size,1,1)\n",
        "  def call(self, input):\n",
        "    score = tf.matmul(self.U,tf.expand_dims(input,-1))                      # (#RHN_enc,#RHN_enc)x(batch_size,#RHN_enc,1) = (batch_size,#RHN_enc,1)\n",
        "    score = score + tf.matmul(self.T,tf.expand_dims(self.hid_state,-1))     # (#RHN_enc,#RHN_dec)(batch_size,#RHN_dec,1) = (batch_size,#RHN_enc,1)\n",
        "    score = score + self.b                                                  # (batch_size,#RHN_enc,1)\n",
        "    score = K.tanh(score)\n",
        "    score = tf.matmul(tf.transpose(self.v),score)                           # (1,#RHN_enc)x(batch_size,#RHN_enc,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                             # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pF02ysdbxWU"
      },
      "source": [
        "On crée maintenant la couche d'attention hiérarchique :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kxnR9fSVXDC"
      },
      "source": [
        "class AttentionHierarchique(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_RHN_dec):\n",
        "    self.dim_RHN_dec = dim_RHN_dec\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_score = CalculScore(dim_RHN_dec=self.dim_RHN_dec)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:      Sorties d'une couche encodeur RHN       : (batch_size,Tin,#RHN_enc)\n",
        "  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN_dec)\n",
        "  # Sorties :\n",
        "  #     vc:         SousVecteur contexte                    : (batch_size,1,RHN_enc)\n",
        "  def call(self, input, hid_state):\n",
        "    # Calcul des scores\n",
        "    self.couche_score.SetInitState(hid_state)\n",
        "    scores = tf.keras.layers.TimeDistributed(self.couche_score)(input)        # (batch_size,Tin,#RHN_enc) : Timestep = Tin\n",
        "                                                                              # (batch_size,#RHN_enc) envoyé Tin fois\n",
        "                                                                              # (batch_size,Tin,1) retourné\n",
        "    scores = tf.keras.activations.softmax(scores,axis=1)                      # (batch_size,Tin,1)\n",
        "\n",
        "    # Applique les scores aux sorties de la couche RHN\n",
        "    poids = tf.multiply(input,scores)             # (batch_size,Tin,#RHN_enc)_x_(batch_size,Tin,1) = (batch_size,Tin,#RHN_enc)\n",
        "\n",
        "    # Calcul le sous-vecteur contexte\n",
        "    vc = K.sum(poids,axis=1)                      # (batch_size,#RHN_enc)\n",
        "    return tf.expand_dims(vc,1)                   # (batch_size,1,#RHN_enc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slCSUTmyifEY"
      },
      "source": [
        "**b. Création du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_efbOikfRwt"
      },
      "source": [
        "Dans le décodeur, on parallélise autant de couches d'attention que nécessaire afin de créer un modèle d'attention multi-entrées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCElCxBcnUHj"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ParaDecodeur.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2nZG3Rrjv3O"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_RHN_dec,nbr_couches_RHN,dropout=0.0):\n",
        "    self.dim_RHN_dec = dim_RHN_dec\n",
        "    self.nbr_couches_RHN = nbr_couches_RHN\n",
        "    self.dropout = dropout\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    attentions = []\n",
        "    inputs_attention = []\n",
        "\n",
        "    # Création des \"nbr_couches\" entrées des attentions\n",
        "    # Chaque entrée est une liste : [input,init_state] = [((batch_size,Tin,#RHN_enc)),((batch_size,#RHN_dec))]\n",
        "    for i in range(input_shape[1]):\n",
        "      inputs_attention.append([tf.keras.Input(shape=(input_shape[2],input_shape[3])),          # input = \"nbr_couches\"*(batch_size,Tin,#RHN_enc)\n",
        "                                 tf.keras.Input(shape=(self.dim_RHN_dec))])                    # init_state = \"nbr_couches\"*(batch_size,#RHN_dec)\n",
        "\n",
        "    # Création des \"nbr_couches\" couches d'attentions hierarchiques\n",
        "    for i in range(input_shape[1]):\n",
        "      att = AttentionHierarchique(dim_RHN_dec=self.dim_RHN_dec)(\n",
        "          inputs_attention[i][0],                 # inputs_attention[i][0] : (batch_size,Tin,#RHN_enc)\n",
        "          inputs_attention[i][1])                 # inputs_attention[i][1] : (batch_size,#RHN_dec)\n",
        "      attentions.append(att)\n",
        "\n",
        "    # Création de la sortie concaténée des \"nbr_couches\" couches d'attentions\n",
        "    out = tf.convert_to_tensor(attentions)                                # out : (nbr_couches,batch_size,1,#RHN_enc)\n",
        "    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,nbr_couches,1,#RHN_enc)\n",
        "\n",
        "    # Création du modèle global\n",
        "    self.att_model = tf.keras.Model(inputs=inputs_attention,outputs=out)\n",
        "\n",
        "    # Création des poids\n",
        "    self.Wtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n",
        "    self.Vtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n",
        "\n",
        "    # Création du décodeur RHN\n",
        "    self.dec_RHN = Cellule_RHN(dim_RHN=self.dim_RHN_dec,nbr_couches=self.nbr_couches_RHN,return_all_states=False,dim_input=1)\n",
        "   \n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:      Sorties des couches de l'encodeur RHN   : (batch_size,nbr_couches,Tin,#RHN_enc)\n",
        "  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN_dec)\n",
        "  #     Y:          Valeur de la série cible                : (batch_size,1)\n",
        "  #     only_att    Si =True ne calcul que le vecteur ctx   : True/False\n",
        "  # Sorties :\n",
        "  #     d:          Vecteur contexte                        : (batch_size,nbr_couches*RHN_enc)\n",
        "  #     s:          Vecteur caché décodeur RHN              : (batch_size,#RHN_dec)\n",
        "  def call(self, input, hid_state, Y,only_att):\n",
        "    # Initialisation de l'état caché à 0 si besoin\n",
        "    # Construit le tenseur nul au format (batch_size,#RHN)\n",
        "    if hid_state == None:\n",
        "      coef = tf.expand_dims(input[:,0,0,0],-1)                          # (batch_size,1)\n",
        "      coef = tf.expand_dims(coef,-1)                                    # (batch_size,1,1)\n",
        "      hid_state = tf.matmul(coef,tf.zeros(shape=(1,self.dim_RHN_dec)))  # (batch_size,1,1)X(1,#RHN_dec) = (batch_size,1,#RHN_dec)\n",
        "      hid_state = tf.squeeze(hid_state,axis=1)                          # (batch_size,#RHN_dec)\n",
        "\n",
        "    # Construction de l'entrée du modèle\n",
        "    # nbr_couches*[((batch_size,Tin,#RHN_enc)),((batch_size,#RHN_dec))]\n",
        "    input_model = []\n",
        "    for i in range(input.shape[1]):\n",
        "      input_model.append([input[:,i,:,:],hid_state])    # [((batch_size,Tin,#RHN_enc)),((batch_size,#RHN_dec))]\n",
        "    \n",
        "    # Calcul des sous-vecteurs contextes\n",
        "    # avec le modèle d'attention hiérarchique parallélisé\n",
        "    d = self.att_model(input_model)                     # d : (batch_size,nbr_couches,1,#RHN_enc)\n",
        "\n",
        "    # Concaténation des sous-vecteurs contextes\n",
        "    d = tf.squeeze(d,axis=2)                            # (batch_size,nbr_couches,#RHN_enc)\n",
        "    d = tf.keras.layers.Flatten()(d)                    # (batch_size,nbr_couches*RHN_enc)\n",
        "\n",
        "    if only_att == False :\n",
        "      # Calcul de y_tilda\n",
        "      ytilda = self.Wtilda(Y)                             # (batch_size,1)\n",
        "      ytilda = ytilda + self.Vtilda(d)                    # (batch_size,1)\n",
        "\n",
        "      # Initialisation des masques de dropout pour tous les pas de temps\n",
        "      self.dec_RHN.InitMasquesDropout(self.dropout)\n",
        "\n",
        "      # Décodage avec le réseau RHN\n",
        "      s = self.dec_RHN(tf.expand_dims(ytilda,-1),hid_state)                  # (batch_size,#RHN_dec)\n",
        "      return d,s\n",
        "    else:\n",
        "      return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYOTdM7fZT65"
      },
      "source": [
        "**3. Création de la couche HRHN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhML2b5bFsZB"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Gene_HRHN.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PCEHUDEZ1bt"
      },
      "source": [
        "class Net_HRHN(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, regul=0.0, drop = 0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n",
        "    self.V = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Séries exogènes X   : (batch_size,Tin,#dim)\n",
        "  #     output_seq:     Série cible Y       : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction HRHN Y   : (batch_size,1,1)\n",
        "  def call(self,input,output_seq):\n",
        "    # Appel de l'encodeur\n",
        "    # Récupère l'ensemble des états cachés de l'encodeur RHN\n",
        "    H = self.encodeur(input)                # (batch_size,nbr_couches,Tin,#RHN_enc)\n",
        "\n",
        "    # Décodage\n",
        "    hidden_state = None\n",
        "    for t in range(input.shape[1]):\n",
        "      vc, hidden_state = self.decodeur(H,hidden_state,output_seq[:,t:t+1,0],only_att = False)\n",
        "    \n",
        "    # Couche d'attention finale\n",
        "    vc = self.decodeur(H,hidden_state,output_seq[:,0,0],only_att=True)    # (batch_size,#RHN_dec)\n",
        "\n",
        "    # Génération de la prédiction\n",
        "    sortie = self.W(hidden_state) + self.V(vc)        # (batch_size,1)\n",
        "    return sortie                                     # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Zmh_AfNbgC"
      },
      "source": [
        "# Ajout du prédicteur initial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nwZjtlOoUUc"
      },
      "source": [
        "**1. Création de la couche VAR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_DWfLohe8jn"
      },
      "source": [
        "class VAR(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Avar = self.add_weight(shape=(input_shape[1],input_shape[2]+1,input_shape[2]+1),initializer=\"normal\",name=\"Avar\")          # (Tin,#dim+1,#dim+1)\n",
        "    self.Apred = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"Apred\")                                                     # (1,1)\n",
        "    self.zeta = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"zeta\")                                                       # (1,1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "  \n",
        "  # Entrées\n",
        "  #     series_X  : Séries exogènes X           : (batch_size,Tin,#dim)\n",
        "  #     cible_Y   : Série cible Y               : (batch_size,Tin,1)\n",
        "  #     y_pred_1  : Prédiction globale à (t-1)  : (batch_size,1)\n",
        "  # Sorties\n",
        "  #     phi       : Prédiction VAR              : (batch_size,1)\n",
        "  def call(self,series_X,cible_Y,y_pred_1):\n",
        "    entree = tf.keras.layers.concatenate([series_X,cible_Y],axis=2)                 # (batch_size,Tin,#dim+1)\n",
        "    entree = tf.transpose(entree,perm=[0,2,1])                                      # (batch_size,#dim+1,Tin)\n",
        "\n",
        "    # Algorithme VAR\n",
        "    sortie = []\n",
        "    for i in range(series_X.shape[1]):\n",
        "      sortie.append(tf.matmul(self.Avar[i,:,:],entree[:,:,i:i+1]))                  # (#dim+1,#dim+1)x(batch_size,#dim+1,1)=(batch_size,#dim+1,1)\n",
        "    sortie = tf.convert_to_tensor(sortie)                                           # (Tin,batch_size,#dim+1,1)\n",
        "    sortie = tf.transpose(sortie,perm=[1,0,2,3])                                    # (batch_size,Tin,#dim+1,1)\n",
        "    sortie = K.sum(sortie,axis=1)                                                   # (batch_size,#dim+1,1)\n",
        "\n",
        "    # Récupère la prédiction de la cible\n",
        "    sortie = sortie[:,-1,:]                                                         # (batch_size,1)\n",
        "\n",
        "    # Ajoute la combinaison linéaire de la prédiction globale à (t-1)\n",
        "    sortie = tf.expand_dims(sortie,-1) + tf.matmul(self.Apred,tf.expand_dims(y_pred_1,-1))             # (batch_size,1,1) + (1,1)x(batch_size,1,1) = (batch_size,1,1)\n",
        "    sortie = tf.squeeze(sortie,-1) + self.zeta                                                         # (batch_size,1)\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlBVjAN6oYsE"
      },
      "source": [
        "**2. Couche du prédicteur initial**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Q-CUqBod3M"
      },
      "source": [
        "class Predicteur_Initial(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,predicteurHRHN,predicteurVAR,regul,drop):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.predicteurHRHN = predicteurHRHN\n",
        "    self.predicteurVAR = predicteurVAR\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Couche_DenseI = tf.keras.layers.Dense(units=1,use_bias=True,name=\"CoucheFNNI\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "  \n",
        "  # Entrées\n",
        "  #     series_X  : Séries exogènes X           : (batch_size,Tin,#dim)\n",
        "  #     cible_Y   : Série cible Y               : (batch_size,Tin,1)\n",
        "  #     y_pred_1  : Prédiction globale à (t-1)  : (batch_size,1)\n",
        "  # Sorties\n",
        "  #     y_tilda   : Prédiction initiale à t     : (batch_size,1)\n",
        "  def call(self,series_X,cible_Y,y_pred_1):\n",
        "    psi = self.predicteurHRHN(series_X,cible_Y)           # (batch_size,1)\n",
        "    phi = self.predicteurVAR(series_X,cible_Y,y_pred_1)   # (batch_size,1)\n",
        "    return self.Couche_DenseI(tf.keras.layers.Concatenate(axis=1)([psi,phi]))        # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWxDydrYgzi"
      },
      "source": [
        "# Ajout du correcteur PID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qofproTuWmlW"
      },
      "source": [
        "class Correcteur_PID(tf.keras.layers.Layer):\n",
        "  def __init__(self,batch_size,couche):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "    self.erreur_1 = tf.Variable(shape=(batch_size,1),trainable=False,initial_value=tf.zeros(shape=(batch_size,1)),name=\"erreur_1\")               # (BS,1) = 0\n",
        "    self.num_iteration = tf.Variable(shape=(batch_size,1),trainable=False,initial_value=tf.ones(shape=(batch_size,1)),name=\"iteration\")          # (BS,1) = 1\n",
        "    self.couche = couche\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Couche_Dense_P = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Ones(),trainable=True,name=\"CoucheP\")\n",
        "    self.Couche_Dense_I = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Zeros(),trainable=True,name=\"CoucheI\")\n",
        "    self.Couche_Dense_D = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Zeros(),trainable=True,name=\"CoucheD\")\n",
        "    self.Couche_DenseII = tf.keras.layers.Dense(units=1,use_bias=True,kernel_initializer=tf.keras.initializers.Ones(),trainable=True,activation=self.couche,name=\"CoucheFNNII\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "  \n",
        "\n",
        "  # Entrées\n",
        "  #     y_tilda_1 :   Prédiction initiale à (t-1) :   (batch_size,1)\n",
        "  #     y_pred_1  :   Prédiction globale à (t-1)  :   (batch_size,1)\n",
        "  # Sorties\n",
        "  #     PID(erreur) : Erreur traitée par PID      :   (batch_size,1)\n",
        "  def call(self, y_tilda_1,y_pred_1):\n",
        "    # Calcul de l'erreur courante\n",
        "    erreur = y_pred_1-y_tilda_1                                                   # (batch_size,1)\n",
        "\n",
        "    # Calcul de la partie proportionnelle\n",
        "    P = self.Couche_Dense_P(erreur)                                               # (batch_size,1)\n",
        "\n",
        "    # Calcul de la partie intégrale\n",
        "    I = (1/self.num_iteration)*self.Couche_Dense_I(self.erreur_1+erreur)          # (batch_size,1)\n",
        "\n",
        "    # Calcul de la partie dérivée\n",
        "    D = self.Couche_Dense_D(erreur-self.erreur_1)                                 # (batch_size,1)\n",
        "    \n",
        "    # Sauvegarde des erreurs et #itérations\n",
        "    self.num_iteration.assign(self.num_iteration+1)                               # (batch_size,1)\n",
        "    self.erreur_1.assign(erreur + self.erreur_1)                                  # (batch_size,1)\n",
        "\n",
        "    return self.Couche_DenseII(tf.keras.layers.Concatenate(axis=1)([P,I,D]))        # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh5MO6QCZbfv"
      },
      "source": [
        "#Création du modèle global"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKu1vn97Zbfv"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def __init__(self,predicteurInitial,correcteurPID,batch_size):\n",
        "      super(CustomModel, self).__init__()\n",
        "      self.predicteurInitial = predicteurInitial\n",
        "      self.CorrecteurPID = correcteurPID\n",
        "      self.y_pred_1 = tf.Variable(shape=(batch_size,1),trainable=False,initial_value=tf.zeros(shape=(batch_size,1)),name=\"ypred_1\")                   # (BS, 1)\n",
        "      self.y_pred_ini_1 = tf.Variable(shape=(batch_size,1),trainable=False,initial_value=tf.zeros(shape=(batch_size,1)),name=\"ypred_ini_1\")       # (BS, 1)\n",
        "      self.y_pred_1.assign(tf.zeros(shape=(batch_size,1)))\n",
        "      self.y_pred_ini_1.assign(tf.zeros(shape=(batch_size,1)))\n",
        "\n",
        "    # inputs:   inputs[0] : Séries exogènes     : (batch_size,Tin,#dim)\n",
        "    #           inputs[1] : Cible               : (batch_size,1,1)\n",
        "    # sortie:   sortie    : Prédiction          : (batch_size,1,1)\n",
        "    def call(self, inputs, training=True):\n",
        "      y_pred_ini = self.predicteurInitial(inputs[0],inputs[1],self.y_pred_1)       # (batch_size,1)\n",
        "      erreur = self.CorrecteurPID(self.y_pred_ini_1,self.y_pred_1)                 # (batch_size,1)\n",
        "      y_pred = y_pred_ini + erreur\n",
        "      self.y_pred_ini_1.assign(y_pred_ini)\n",
        "      self.y_pred_1.assign(y_pred)                                                 # Enregistre la prédiction\n",
        "      return tf.expand_dims(y_pred,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtbX0QyB8Rvr"
      },
      "source": [
        "# Entrainement TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cNt57Q8RIT"
      },
      "source": [
        "  dim_cnn=[4,1,2]\n",
        "  nbr_cnn=[32,32,16]\n",
        "  max_pool=[1,1,1]\n",
        "  dim_RHN_enc=8\n",
        "  dim_RHN_dec=64\n",
        "  nbr_couches_RHN_dec=1\n",
        "  nbr_couches_RHN_enc=2\n",
        "  drop=0.0\n",
        "  regull2=0.0001\n",
        "\n",
        "\n",
        "  dim_cnn=[3,3,3]\n",
        "  nbr_cnn=[16,32,64]\n",
        "  max_pool=[3,3,3]\n",
        "  dim_RHN_enc=128\n",
        "  dim_RHN_dec=128\n",
        "  nbr_couches_RHN_dec=3\n",
        "  nbr_couches_RHN_enc=3\n",
        "  drop=0.0\n",
        "  regull2=0.0001\n",
        "\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "  with strategy.scope():\n",
        "    dim_motif = Encodeur_CNN(dim_filtres_cnn=dim_cnn,nbr_filtres_cnn=nbr_cnn,dim_max_pooling=max_pool,dim_motif=0)(x_train[0][0:1,:,:]).shape[2]\n",
        "    encodeur = Encodeur(dim_filtres_cnn=dim_cnn,nbr_filtres_cnn=nbr_cnn,dim_max_pooling=max_pool,dim_motif=dim_motif,dim_RHN_enc=dim_RHN_enc,nbr_couches_RHN=nbr_couches_RHN_enc,dropout=drop)\n",
        "    decodeur = Decodeur(dim_RHN_dec=dim_RHN_dec,nbr_couches_RHN=nbr_couches_RHN_dec,dropout=drop)\n",
        "    predicteurHRHN = Net_HRHN(encodeur,decodeur,longueur_sequence=longueur_sequence,drop=drop)\n",
        "    predicteurVAR = VAR()\n",
        "    predicteurInitial = Predicteur_Initial(encodeur=encodeur,decodeur=decodeur,predicteurHRHN=predicteurHRHN, predicteurVAR=predicteurVAR, regul=regull2,drop=drop)\n",
        "    correcteurPID = Correcteur_PID(int(batch_size/8),couche='relu')\n",
        "    model = CustomModel(predicteurInitial=predicteurInitial,correcteurPID=correcteurPID,batch_size=int(batch_size/8))\n",
        "    model.build(input_shape=([(int(batch_size/8),longueur_sequence,x_train[0].shape[2]),(int(batch_size/8),longueur_sequence,1)]))\n",
        "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=0.001,decay_steps=50,decay_rate=0.001)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    model.compile(optimizer=optimizer,loss=\"mse\")\n",
        "    CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "  \n",
        "  history = model.fit(x=[x_train[0],x_train[1]],y=y_train,epochs=10000,callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1000)],validation_data=([x_val[0],x_val[1]],y_val),batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
