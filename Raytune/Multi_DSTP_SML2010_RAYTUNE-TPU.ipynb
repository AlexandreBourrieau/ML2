{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_DSTP_SP500_RAYTUNE-TPU.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/Raytune/Multi_DSTP_SML2010_RAYTUNE-TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPK5y4fIYKJt"
      },
      "source": [
        "!pip install -q 'ray[tune]' 'ray[default]'\n",
        "!pip install -q --upgrade aioredis==1.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSRRDMXaK_YA"
      },
      "source": [
        "Les TPU sont des calculateurs qui sont différents du processeur local exécutant le programme Python. Pour travailler avec les TPU, il faut donc commencer par se connecter au cluster distant et les initialiser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Le dataset utilisé est SP500..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des fichiers CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/SPX2010_2021.csv\"\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/VIX2010_2021.csv\"\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/US10Y2010_2021.csv\"\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/TYVIX2010_2021.csv\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données S&P500**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_SP500 = pd.read_csv(\"SPX2010_2021.csv\",sep=\";\",names=[\"Date\",\"SP500_O\",\"SP500_H\",\"SP500_L\",\"SP500_C\"],decimal=\",\")\n",
        "df_SP500['Date'] = pd.to_datetime(df_SP500['Date'])\n",
        "df_SP500 = df_SP500.set_index(\"Date\")\n",
        "df_SP500 = df_SP500.asfreq(freq=\"1D\")\n",
        "df_SP500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_SP500),len(df_SP500)+1),y=df_SP500['SP500_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2v6hCl_2yZB"
      },
      "source": [
        "**2. Analyse et correction des données VIX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1lBSgjy2yZC"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_VIX = pd.read_csv(\"VIX2010_2021.csv\",sep=\";\",names=[\"Date\",\"VIX_O\",\"VIX_H\",\"VIX_L\",\"VIX_C\"],decimal=\",\")\n",
        "df_VIX['Date'] = pd.to_datetime(df_VIX['Date'])\n",
        "df_VIX = df_VIX.set_index(\"Date\")\n",
        "df_VIX = df_VIX.asfreq(freq=\"1D\")\n",
        "df_VIX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjcWJ1fw2yZD"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_VIX),len(df_VIX)+1),y=df_VIX['VIX_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcV_ZlnD57Y5"
      },
      "source": [
        "**3. Analyse et correction des données US10Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL5Ifonj57Y6"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_US10Y= pd.read_csv(\"US10Y2010_2021.csv\",sep=\";\",names=[\"Date\",\"US10Y_O\",\"US10Y_H\",\"US10Y_L\",\"US10Y_C\"],decimal=\",\")\n",
        "df_US10Y['Date'] = pd.to_datetime(df_US10Y['Date'])\n",
        "df_US10Y = df_US10Y.set_index(\"Date\")\n",
        "df_US10Y = df_US10Y.asfreq(freq=\"1D\")\n",
        "df_US10Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaeKBt1V57Y9"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_US10Y),len(df_US10Y)+1),y=df_US10Y['US10Y_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIH2nnno-aXp"
      },
      "source": [
        "**4. Analyse et correction des données TYVIX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JjB4FLL-aXq"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_TYVIX= pd.read_csv(\"TYVIX2010_2021.csv\",sep=\";\",names=[\"Date\",\"TYVIX_C\"],decimal=\",\")\n",
        "df_TYVIX['Date'] = pd.to_datetime(df_TYVIX['Date'])\n",
        "df_TYVIX = df_TYVIX.set_index(\"Date\")\n",
        "df_TYVIX = df_TYVIX.asfreq(freq=\"1D\")\n",
        "df_TYVIX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1hj1rle-aXr"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_TYVIX),len(df_TYVIX)+1),y=df_TYVIX['TYVIX_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVQ772Gu_3eE"
      },
      "source": [
        "**5. Fusion des données et correction des valeurs NaN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgnvBRSvAjq3"
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "df_liste = [df_SP500,df_VIX,df_US10Y,df_TYVIX]\n",
        "\n",
        "df_etude = reduce(lambda  left,right: pd.merge(left,right,on=['Date'],how='outer'), df_liste)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px1DRPuSBfXc"
      },
      "source": [
        "debut = \"2011-01-10\"\n",
        "#debut = \"2015-01-01\"\n",
        "fin = \"2020-04-24\"\n",
        "\n",
        "mask = (df_etude.index >= debut) & (df_etude.index <= fin)\n",
        "df_etude = df_etude.loc[mask]\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n0i_0lhCjPT"
      },
      "source": [
        "df_etude.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGO9fDfwC-FL"
      },
      "source": [
        "df_etude = df_etude.interpolate(method=\"linear\")\n",
        "df_etude.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8I2iqRsDZy7"
      },
      "source": [
        "Déplace la cible à la fin :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnq-XsMRDRUQ"
      },
      "source": [
        "col = df_etude.pop('SP500_C')\n",
        "df_etude.insert(len(df_etude.columns),\"SP500_C\",col)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlTj0KPxEaSO"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['SP500_C'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude.values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 256                 \n",
        "longueur_sequence = 5\n",
        "longueur_sortie = 1\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "def Create_train(dataset):\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "\n",
        "  # Extrait les X,Y du dataset\n",
        "  x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                          # y=43x(BS,1,1)\n",
        "  for i in range(len(x)):\n",
        "    X1.append(x[i][0])          \n",
        "    X2.append(x[i][1])\n",
        "\n",
        "  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "  # Recombine les données\n",
        "  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "  x_train = [X1,X2]\n",
        "  y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "  return x_train,y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVbVOFDwERm-"
      },
      "source": [
        "x_train, y_train = Create_train(dataset)\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "def Create_val(dataset_val):\n",
        "  X1 = []\n",
        "  X2 = []\n",
        "\n",
        "  # Extrait les X,Y du dataset\n",
        "  x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                          # y=43x(BS,1,1)\n",
        "  for i in range(len(x)):\n",
        "    X1.append(x[i][0])          \n",
        "    X2.append(x[i][1])\n",
        "\n",
        "  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "  # Recombine les données\n",
        "  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "  x_val = [X1,X2]\n",
        "  y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "  return x_val,y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTzC0c34Enxr"
      },
      "source": [
        "x_val,y_val = Create_val(dataset_val)\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle DSTP-RNN #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0lDwKjfzzPh"
      },
      "source": [
        "Le modèle DSTP-RNN implanté est le suivant :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFTrTbJ5zuqj"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNN-VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "**1. Création de la couche d'attention spatiale de l'étage n°1 / Phase 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmThmaJnoyEF"
      },
      "source": [
        "On commence par créer la couche permettant de calculer le score. Cette fonction calcule le score de l'encodeur, c'est-à-dire le score à attribuer à chaque série d'entrée.  \n",
        "Cette fonction est appellée par l'encodeur à l'aide de la méthode TimeDistribued de Keras, pour chaque série d'entrée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx3ac1_G00Ga"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScore__.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiCdmR0dnXp4"
      },
      "source": [
        "class CalculScores_Encodeur_Phase1(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM_enc_P1):\n",
        "    self.dim_LSTM_enc_P1 = dim_LSTM_enc_P1\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.Wf = self.add_weight(shape=(input_shape[1],2*self.dim_LSTM_enc_P1),initializer=\"normal\",name=\"Wf\")     # (Tin, 2x#LSTM_encP1)\n",
        "    self.Uf = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"Uf\")             # (Tin, Tin)\n",
        "    self.bf = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"bf\")                          # (Tin, 1)\n",
        "    self.vf = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"vf\")                          # (Tin, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM_encP1)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM_encP1)]\n",
        "\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     score:          Score               : (batch_size,1,1)\n",
        "\n",
        "  def call(self, input):\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM_encP1)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM_encP1) => (batch_size,2#LSTM_encP1,1)\n",
        "        e = tf.matmul(self.Wf,hs)                                               # (Tin,2x#LSTM_encP1)x(batch_size,2x#LSTM_encP1,1) = (batch_size,Tin,1)\n",
        "        e = e + tf.matmul(self.Uf,input)                                        # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bf                                                         # (batch_size,Tin,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Uf,input)                                            # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bf                                                         # (batch_size,Tin,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vf),e)                                  # (1,Tin)x(batch_size,Tin,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                                 # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM8tVDxynXp4"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMlE2tNF1XRB"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase1__.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8n_qKEnXp4"
      },
      "source": [
        "class Encodeur_Phase1(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dim_LSTM_enc_P1, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM_enc_P1 = dim_LSTM_enc_P1          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM_enc_P1,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Encodeur\")\n",
        "    self.CalculScores_Encodeur_Phase1 = CalculScores_Encodeur_Phase1(dim_LSTM_enc_P1=self.dim_LSTM_enc_P1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM_encP1)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM_encP1)]\n",
        "  #     index:          index série         : (1)\n",
        "  # Sorties :\n",
        "  #     out_hid : Sortie vecteur caché      : (batch_size,#LSTM_encP1)\n",
        "  #     out_cell: Sortie cell state         : (btach_size,#LSTM_encP1)\n",
        "  #     x_tilda : Coupe temporelle pondérée : (batch_size,1,#dim)\n",
        "\n",
        "  def call(self, input, hidd_state, cell_state, index):\n",
        "    # Calcul des scores\n",
        "    input_TD = tf.transpose(input,perm=[0,2,1])                               # (batch_size,Tin,#dim) => (batch_size,#dim,Tin)\n",
        "    input_TD = tf.expand_dims(input_TD,axis=-1)                               # (batch_size,#dim,Tin) => (batch_size,#dim,Tin,1)\n",
        "    self.CalculScores_Encodeur_Phase1.SetStates(hidd_state,cell_state)\n",
        "    a = tf.keras.layers.TimeDistributed(\n",
        "        self.CalculScores_Encodeur_Phase1)(input_TD)                          # (batch_size,#dim,Tin,1) : Timestep=#dim\n",
        "                                                                              # (batch_size,Tin,1) envoyé #dim fois en //\n",
        "                                                                              # (batch_size,#dim,1) retourné\n",
        "    # Normalisation des scores alpha\n",
        "    a = tf.keras.activations.softmax(a,axis=1)                                # (batch_size,#dim,1)\n",
        "\n",
        "    # Applique les poids normalisés à la coupe temporelle des séries exogènes\n",
        "    x_tilda = tf.multiply(tf.expand_dims(input[:,index,:],-1),a)              # (batch_size,#dim,1) _x_ (batch_size,#dim,1) = (batch_size,#dim,1)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,1,#dim)\n",
        "\n",
        "    # Applique x_tilda à la cellule LSTM\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,#dim,1)\n",
        "    out_dec, out_hid, out_cell = self.couche_LSTM(x_tilda)                    # out_dec et out_cell : (batch_size,#LSTM_encP1)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,1,#dim)\n",
        "\n",
        "    return out_hid, out_cell, x_tilda\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI5YwkzX84Vi"
      },
      "source": [
        "**2. Création de la couche d'attention spatiale de l'étage n°1 / Phase 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN79pZ349FR6"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase2_CalculScore__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6pu4agmAtqB"
      },
      "source": [
        "On commence par créer le calcul  du score :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pA1z8N-koF"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase2_CalculScore2__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-IiIBuJ-SKi"
      },
      "source": [
        "class CalculScores_Encodeur_Phase2(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dim_LSTM_encP2):\n",
        "    self.dim_LSTM_encP2 = dim_LSTM_encP2\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.Ws = self.add_weight(shape=(input_shape[1],2*self.dim_LSTM_encP2),initializer=\"normal\",name=\"Ws\")    # (Tin, 2x#LSTM_encP2)\n",
        "    self.Us = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"Us\")     # (Tin, Tin)\n",
        "    self.bs = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"bs\")                  # (Tin, 1)\n",
        "    self.vs = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"vs\")                  # (Tin, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM_encP2)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM_encP2)]\n",
        "\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées Z           : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     score:          Score               : (batch_size,1,1)\n",
        "\n",
        "  def call(self, input):\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM_encP2)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM_encP2) => (batch_size,2#LSTM_encP2,1)\n",
        "        e = tf.matmul(self.Ws,hs)                                               # (Tin,2x#LSTM_encP2)x(batch_size,2x#LSTM_encP2,1) = (batch_size,Tin,1)\n",
        "        e = e + tf.matmul(self.Us,input)                                        # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bs                                                         # (batch_size,Tin,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Us,input)                                            # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bs                                                         # (batch_size,Tin,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vs),e)                                  # (1,Tin)x(batch_size,Tin,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                                 # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe10l-HI_Nsb"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WmZzmBV_p4D"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase22__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7elzJ8S_Rrd"
      },
      "source": [
        "class Encodeur_Phase2(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dim_LSTM_encP2, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM_encP2 = dim_LSTM_encP2          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM_encP2,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Encodeur\")\n",
        "    self.CalculScores_Encodeur_Phase2 = CalculScores_Encodeur_Phase2(dim_LSTM_encP2=self.dim_LSTM_encP2)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées Z           : (batch_size,Tin,#dim+1)\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM_encP2)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM_encP2)]\n",
        "  #     index:          index série         : (1)\n",
        "  # Sorties :\n",
        "  #     out_hid : Sortie vecteur caché      : (batch_size,#LSTM_encP2)\n",
        "  #     out_cell: Sortie cell state         : (btach_size,#LSTM_encP2)\n",
        "\n",
        "  def call(self, input, hidd_state, cell_state, index):\n",
        "    # Calcul des scores\n",
        "    input_TD = tf.transpose(input,perm=[0,2,1])                               # (batch_size,Tin,#dim+1) => (batch_size,#dim+1,Tin)\n",
        "    input_TD = tf.expand_dims(input_TD,axis=-1)                               # (batch_size,#dim+1,Tin) => (batch_size,#dim+1,Tin,1)\n",
        "    self.CalculScores_Encodeur_Phase2.SetStates(hidd_state,cell_state)\n",
        "    b = tf.keras.layers.TimeDistributed(\n",
        "        self.CalculScores_Encodeur_Phase2)(input_TD)                          # (batch_size,#dim+1,Tin,1) : Timestep=#dim+1\n",
        "                                                                              # (batch_size,Tin,1) envoyé #dim+1 fois en //\n",
        "                                                                              # (batch_size,#dim+1,1) retourné\n",
        "    # Normalisation des scores beta\n",
        "    b = tf.keras.activations.softmax(b,axis=1)                                # (batch_size,#dim+1,1)\n",
        "\n",
        "    # Applique les poids normalisés à la série\n",
        "    z_tilda = tf.multiply(tf.expand_dims(input[:,index,:],-1),b)              # (batch_size,#dim+1,1) _x_ (batch_size,#dim+1,1) = (batch_size,#dim+1,1)\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,1,#dim+1)\n",
        "\n",
        "    # Applique z_tilda à la cellule LSTM\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,#dim+1,1)\n",
        "    out_dec, out_hid, out_cell = self.couche_LSTM(z_tilda)                    # out_dec et out_cell : (batch_size,#LSTM_encP2)\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,1,#dim+1)\n",
        "\n",
        "    return out_hid, out_cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLuRur14AWxz"
      },
      "source": [
        "**3. Création de la couche d'attention du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-_zhkmaDucJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur3.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyjAVbSbAama"
      },
      "source": [
        "On commence par créer la couche de calcul du score du décodeur.  \n",
        "Cette fonction calcule le score du décodeur, c'est-à-dire le score à attribuer à chaque hidden-state en sortie de l'encodeur.  \n",
        "Cette fonction est appellée par la couche d'attention temporelle du décodeur à l'aide de la méthode TimeDistribued de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPqAY_fBByOC"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur4.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohWFJ93YCc9i"
      },
      "source": [
        "class CalculScores_Decodeur(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,dim_LSTM_encP2,dim_LSTM_dec):\n",
        "    self.dim_LSTM_encP2 = dim_LSTM_encP2        # Dimension des vecteurs cachés provenant de l'encodeur\n",
        "    self.dim_LSTM_dec = dim_LSTM_dec            # Dimension des vecteurs cachés du décodeur\n",
        "\n",
        "    super().__init__()                  # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.Wd = self.add_weight(shape=(self.dim_LSTM_dec,2*self.dim_LSTM_dec),initializer=\"normal\",name=\"Wd\")     # (#dim_LSTM_dec, 2x#dim_LSTM_dec)\n",
        "    self.Ud = self.add_weight(shape=(self.dim_LSTM_dec,self.dim_LSTM_encP2),initializer=\"normal\",name=\"Ud\")     # (#dim_LSTM_dec, #dim_LSTM_encP2)\n",
        "    self.bd = self.add_weight(shape=(self.dim_LSTM_dec,1),initializer=\"normal\",name=\"bd\")                       # (#dim_LSTM_dec, 1)\n",
        "    self.vd = self.add_weight(shape=(self.dim_LSTM_dec,1),initializer=\"normal\",name=\"vd\")                       # (#dim_LSTM_dec, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#dim_LSTM_dec)\n",
        "  #     cell_state:     Cell state          : (batch_size,#dim_LSTM_dec)\n",
        "\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:        Entrée score décodeur : (batch_size,#LSTM_encP1)\n",
        "  # Sorties :\n",
        "  #     score:        score                 : (batch_size,1)\n",
        "\n",
        "  def call(self,input):\n",
        "    input = tf.expand_dims(input,-1)\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#dim_LSTM_dec)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#dim_LSTM_dec) => (batch_size,2#dim_LSTM_dec,1)\n",
        "        e = tf.matmul(self.Wd,hs)                                               # (#dim_LSTM_dec,2x#dim_LSTM_dec)x(batch_size,2x#dim_LSTM_dec,1) = (batch_size,#dim_LSTM_dec,1)\n",
        "        e = e + tf.matmul(self.Ud,input)                                        # (#dim_LSTM_dec,#dim_LSTM_encP2)x(batch_size,#LSTM_encP1,1) = (batch_size,#dim_LSTM_dec,1)\n",
        "        e = e + self.bd                                                         # (batch_size,#dim_LSTM_dec,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Ud,input)                                            # (#dim_LSTM_dec,#dim_LSTM_encP2)x(batch_size,#LSTM_encP1,1) = (batch_size,#dim_LSTM_dec,1)\n",
        "        e = e + self.bd                                                         # (batch_size,#dim_LSTM_dec,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vd),e)                                  # (1,#dim_LSTM_dec)x(batch_size,#dim_LSTM_dec,1) = (batch_size,1,1)\n",
        "    score = tf.squeeze(score,-1)                                                # (batch_size,1)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BeYO8p6DZ6b"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diFuNNIpEf6i"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur5.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v5qRdB9DpJC"
      },
      "source": [
        "class CalculAttention_Decodeur(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dim_LSTM_encP2, dim_LSTM_dec):\n",
        "    self.dim_LSTM_encP2 = dim_LSTM_encP2          # Dimension des vecteurs cachés de l'encodeur\n",
        "    self.dim_LSTM_dec = dim_LSTM_dec              # dimension vecteurs cachés décodeur\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.couche_CalculScores_Decodeur = CalculScores_Decodeur(dim_LSTM_encP2=self.dim_LSTM_encP2,dim_LSTM_dec=self.dim_LSTM_dec)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM_dec)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM_dec)\n",
        "\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#LSTM_encP2)\n",
        "  # Sorties :\n",
        "  #     vect_contexte   Vecteur Contexte    : (batch_size,1,#LSTM_encP2)\n",
        "\n",
        "  def call(self, input):\n",
        "    # Calcul des scores\n",
        "    self.couche_CalculScores_Decodeur.SetStates(self.hidd_state,self.cell_state)\n",
        "    g = tf.keras.layers.TimeDistributed(\n",
        "        self.couche_CalculScores_Decodeur)(input)                             # (batch_size,Tin,#LSTM_encP2) : Timestep=Tin\n",
        "                                                                              # (batch_size,#LSTM_encP2) envoyé Tin fois en //\n",
        "                                                                              # (batch_size,Tin,1) retourné\n",
        "    # Normalisation des scores gama\n",
        "    g = tf.keras.activations.softmax(g,axis=1)                                # (batch_size,Tin,1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    C = tf.multiply(input,g)        # (batch_size,Tin,#LSTM_encP2)_x_(batch_size,Tin,1) = (batch_size,Tin,#LSTM_encP2)\n",
        "    C = K.sum(C,axis=1)             # (batch_size,#LSTM_encP2)\n",
        "    C = tf.expand_dims(C,1)         # (batch_size,1,#LSTM_encP2)\n",
        "    return C\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbk9Nbn8EvEx"
      },
      "source": [
        "**4. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L72CzheDExup"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CoucheDecodeurAll.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tJnICP9FStv"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,dim_LSTM_encP2, dim_LSTM_dec, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM_encP2 = dim_LSTM_encP2            # Dimension des vecteurs cachés\n",
        "    self.dim_LSTM_dec = dim_LSTM_dec\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.couche_Attention_Decodeur = CalculAttention_Decodeur(dim_LSTM_encP2=self.dim_LSTM_encP2,dim_LSTM_dec=self.dim_LSTM_dec)\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM_dec,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Decodeur\")\n",
        "    self.W = self.add_weight(shape=(self.dim_LSTM_encP2+1,1),initializer=\"normal\",name=\"W\")                   # (#dim_LSTM_encP2+1, 1)\n",
        "    self.b = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"b\")                                       # (1, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:        Entrée décodeur       : (batch_size,Tin,#LSTM_encP2)\n",
        "  #     Y:            Yt                    : (batch_size,1,1)\n",
        "  #     hid_state:    hidden state          : (batch_size,#LSTM_dec)\n",
        "  #     cell_state:   cell_state            : (batch_size,#LSTM_dec)\n",
        "  # Sorties :\n",
        "  #     out_hid :     hidden_state          : (batch_size,#LSTM_dec)\n",
        "  #     out_cell :    cell_state            : (batch_size,#LSTM_dec)\n",
        "  #     v_contexte:   vecteur contexte      : (batch_size,#LSTM_encP2)\n",
        "\n",
        "  def call(self,input,Y,hid_state,cell_state):\n",
        "    # Calcul du vecteur contexte\n",
        "    self.couche_Attention_Decodeur.SetStates(hid_state,cell_state)\n",
        "    C = self.couche_Attention_Decodeur(input)           # (batch_size,1,#LSTM_encP2)\n",
        "\n",
        "    # Calcul de Y_tilda\n",
        "    add = tf.keras.layers.concatenate([Y,C],axis=2)     # (batch_size,1,#LSTM_encP2+1)\n",
        "    add = tf.transpose(add,perm=[0,2,1])                # (batch_size,#LSTM_encP2+1,1)\n",
        "    Y_tilda = tf.matmul(tf.transpose(self.W),add)       # (1,#LSTM_encP2+1) x (batch_size,#LSTM_encP2+1,1) = (batch_size,1,1)\n",
        "    Y_tilda = Y_tilda + self.b\n",
        "\n",
        "    # Calcul des hidden state et cell state\n",
        "    if hid_state is not None:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda,initial_state=[hid_state,cell_state])\n",
        "    else:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda)\n",
        "\n",
        "    return out_hid,out_cell, C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJz4ghULFn6a"
      },
      "source": [
        "**5. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8pXmw_bFxoJ"
      },
      "source": [
        "Il ne reste plus qu'à créer l'architecture complète et d'ajouter l'estimation de la sortie :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qpMf0rFtsJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNN-VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUEWJblwv3BI"
      },
      "source": [
        "Prédictions des valeurs multi-step :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7eE1A-8GrvC"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNNPredictions__.png?raw=true' width=600>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD5OoZc4K7SJ"
      },
      "source": [
        "class Net_DSTPRNN(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,encodeur_phase1, encodeur_phase2,decodeur,longueur_sequence, longueur_sortie, dim_LSTM_dec, dim_LSTM_encP2, regul=0.0, drop = 0.0):\n",
        "    self.encodeur_phase1 = encodeur_phase1\n",
        "    self.encodeur_phase2 = encodeur_phase2\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    self.dim_LSTM_dec = dim_LSTM_dec\n",
        "    self.dim_LSTM_encP2 = dim_LSTM_encP2\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.Wy = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM_dec,self.dim_LSTM_dec+self.dim_LSTM_encP2),initializer=\"normal\",name=\"Wy\")        # (longueur_sortie,#LSTM_dec, #LSTM_dec+#LSTM_encP2)\n",
        "    self.by = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM_dec,1),initializer=\"normal\",name=\"by\")                                            # (longueur_sortie,#LSTM_dec, 1)\n",
        "    self.vy = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM_dec,1),initializer=\"normal\",name=\"vy\")                                            # (longueur_sortie,#LSTM_dec,1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     output_seq:     Sortie séquence Y   : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction Y        : (batch_size,longueur_sortie,1)\n",
        "\n",
        "  def call(self,input,output_seq):\n",
        "    # Phase n°1 d'encodage\n",
        "    # Calcul les représentations spatiales pondérées\n",
        "    # des coupes temporelles des séries exogènes en entrée\n",
        "    # x_tilda\n",
        "    x_tilda = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state, x_t = self.encodeur_phase1(input,hid_state,cell_state,i)\n",
        "      x_t = tf.squeeze(x_t,1)                     # (batch_size,1,#dim) => (batch_size,#dim)\n",
        "      x_tilda.append(x_t)                         # (batch_size,#dim)\n",
        "    x_tilda = tf.convert_to_tensor(x_tilda)       # (Tin,batch_size,#dim)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[1,0,2])  # (batch_size,Tin,#dim)\n",
        "\n",
        "    # Concaténation des sorties de la phase 1 avec la série cible\n",
        "    Z = []\n",
        "\n",
        "    for i in range(input.shape[1]):\n",
        "      z = tf.keras.layers.concatenate([x_tilda[:,i,:],                 # (batch_size,#dim+1)\n",
        "                                       output_seq[:,i,:]],axis=1)\n",
        "      Z.append(z)\n",
        "    Z = tf.convert_to_tensor(Z)                   # (Tin,batch_size,#dim+1)\n",
        "    Z = tf.transpose(Z,perm=[1,0,2])              # (batch_size,Tin,#dim+1)\n",
        "\n",
        "    # Phase n°2 d'encodage\n",
        "    # Création des représentations cachées des\n",
        "    # concaténations précédentes\n",
        "    hid = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state = self.encodeur_phase2(Z,hid_state,cell_state,i)\n",
        "      hid.append(hid_state)\n",
        "    hid = tf.convert_to_tensor(hid)               # (Tin,batch_size,#LSTM_encP2)\n",
        "    hid = tf.transpose(hid,perm=[1,0,2])          # (batch_size,Tin,#LSTM_encP2)\n",
        "\n",
        "\n",
        "    # Phase de décodage\n",
        "    # Récupère les états cachés à (T-1)\n",
        "    hid_ = None\n",
        "    cell_ = None\n",
        "    for i in range(0,output_seq.shape[1]-1):\n",
        "      hid_, cell_, vc = self.decodeur(hid,output_seq[:,i:i+1,:],hid_,cell_)\n",
        "    \n",
        "    # hid_  : hT-1    : hidden state à t=T-1\n",
        "    # cell_ : sT-1    : cell state à t=T-1\n",
        "    # vc    : CT-1    : vecteur contexte à t=T-1\n",
        "    \n",
        "    # Estimation des sorties\n",
        "    # hid_ : (batch_size,#LSTM_dec)\n",
        "    # vc   : (batch_size,1,#LSTM_encP2)\n",
        "    Y = []\n",
        "    y = tf.expand_dims(output_seq[:,-1,:],-1)        # y = YT : (batch_size,1,1)\n",
        "    \n",
        "    for i in range(0,self.longueur_sortie):\n",
        "      hid_, cell_, vc = self.decodeur(hid,y,hid_,cell_)\n",
        "      add = tf.keras.layers.concatenate([tf.expand_dims(hid_,1),vc],axis=2)         # (batch_size,1,#LSTM_dec+#LSTM_encP2)\n",
        "      add = tf.transpose(add,perm=[0,2,1])                                          # (batch_size,#LSTM_dec+#LSTM_encP2,1)\n",
        "      sortie = tf.matmul(self.Wy[i,:,:],add)                                      # (#LSTM_dec,#LSTM_dec+#LSTM_encP2) x (batch_size,#LSTM_dec+#LSTM_encP2,1) = (batch_size,#LSTM_dec,1)\n",
        "      sortie = sortie + self.by[i,:,:]                                            # (batch_size,#LSTM_dec,1)\n",
        "      sortie = tf.matmul(tf.transpose(self.vy[i,:,:]),sortie)                     # (1,#LSTM_dec)x(batch_size,#LSTM_dec,1) = (batch_size,1,1)\n",
        "      y = sortie\n",
        "      Y.append(y)\n",
        "\n",
        "    Y = tf.convert_to_tensor(Y)           # Y = (longueur_sortie,batch_size,1,1)\n",
        "    Y = tf.transpose(Y,perm=[1,0,2,3])    # Y = (batch_size,longueur_sortie,1,1)\n",
        "    Y = tf.squeeze(Y,-1)                  # Y = (batch_size,longueur_sortie,1)\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj12PHWgPTjD"
      },
      "source": [
        "# Mise en place du modèle DSTP-RNN #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ9tj9S2PWqM"
      },
      "source": [
        "def DSTP_model(config):\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(config['longueur_sequence'],x_train[0].shape[2]))\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(config['longueur_sequence'],1))\n",
        "\n",
        "  encodeur_P1 = Encodeur_Phase1(dim_LSTM_enc_P1=config['dim_LSTM_encP1'],drop=config['drop'],regul=config['l2reg'])\n",
        "  encodeur_P2 = Encodeur_Phase2(dim_LSTM_encP2=config['dim_LSTM_encP2'],drop=config['drop'],regul=config['l2reg'])\n",
        "  decodeur = Decodeur(dim_LSTM_encP2=config['dim_LSTM_encP2'],dim_LSTM_dec=config['dim_LSTM_dec'],drop=config['drop'],regul=config['l2reg'])\n",
        "\n",
        "  sortie = Net_DSTPRNN(encodeur_P1,encodeur_P2,decodeur,longueur_sequence=config['longueur_sequence'],longueur_sortie=1, dim_LSTM_dec=config['dim_LSTM_dec'],dim_LSTM_encP2=config['dim_LSTM_encP2'], regul=config['l2reg'],drop=config['drop'])(entrees_sequences,sorties_sequence)\n",
        "\n",
        "  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdSoHUDnkpgu"
      },
      "source": [
        "# Configuration de l'optimiseur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQdgXWaTkx0w"
      },
      "source": [
        "**1. Espace des hyperparamètres**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r8pgY-FBm8l"
      },
      "source": [
        "from ray import tune\n",
        "\n",
        "def create_search_space():\n",
        "  config = {\n",
        "      \"longueur_sequence\": tune.choice([5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80]),\n",
        "      \"dim_LSTM_encP1\": tune.choice([16,32,64,128,256]),\n",
        "      \"dim_LSTM_encP2\": tune.choice([16,32,64,128,256]),\n",
        "      \"dim_LSTM_dec\": tune.choice([16,32,64,128,256]),\n",
        "      \"drop\": tune.choice([0.0,0.01,0.1,0.3,0.6]),\n",
        "      \"l2reg\": tune.choice([0.0,0.0001,0.001,0.01]),\n",
        "      \"batch_size\": tune.choice([128,256,512]),\n",
        "      \"lr\": tune.loguniform(1e-4, 1e-1)\n",
        "      }\n",
        "  \n",
        "  initial_best_config = [{\n",
        "      \"longueur_sequence\": 10,\n",
        "      \"dim_LSTM_encP1\": 128,\n",
        "      \"dim_LSTM_encP2\": 128,\n",
        "      \"dim_LSTM_dec\": 128,\n",
        "      \"drop\": 0.0,\n",
        "      \"l2reg\": 0.0,\n",
        "      \"batch_size\": 128,\n",
        "      \"lr\": 0.001\n",
        "      }]\n",
        "\n",
        "  return config,initial_best_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylc12PPtPGIv"
      },
      "source": [
        "class TuneReporter(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, reporter=None, freq=\"batch\", logs=None):\n",
        "        self.iteration = 0\n",
        "        logs = logs or {}\n",
        "        if freq not in [\"batch\", \"epoch\"]:\n",
        "            raise ValueError(\"{} not supported as a frequency.\".format(freq))\n",
        "        self.freq = freq\n",
        "        super(TuneReporter, self).__init__()\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        from ray import tune\n",
        "        logs = logs or {}\n",
        "        if not self.freq == \"batch\":\n",
        "            return\n",
        "        self.iteration += 1\n",
        "        for metric in list(logs):\n",
        "            if \"loss\" in metric and \"neg_\" not in metric:\n",
        "                logs[\"neg_\" + metric] = -logs[metric]\n",
        "        if \"acc\" in logs:\n",
        "            tune.report(keras_info=logs, mean_accuracy=logs[\"acc\"])\n",
        "        else:\n",
        "            tune.report(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"))\n",
        "    \n",
        "    def on_epoch_end(self, batch, logs=None):\n",
        "        from ray import tune\n",
        "        logs = logs or {}\n",
        "        if not self.freq == \"epoch\":\n",
        "            return\n",
        "        self.iteration += 1\n",
        "        for metric in list(logs):\n",
        "            if \"loss\" in metric and \"neg_\" not in metric:\n",
        "                logs[\"neg_\" + metric] = -logs[metric]\n",
        "        if \"acc\" in logs:\n",
        "            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs[\"acc\"])\n",
        "        else:\n",
        "            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs.get(\"accuracy\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1llAfcqwOjpR"
      },
      "source": [
        "def create_callbacks():\n",
        "    callbacks = []\n",
        "    tune_reporter = TuneReporter(freq=\"batch\")\n",
        "    callbacks.append(tune_reporter)\n",
        "    return callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W2wP-rtLIG4"
      },
      "source": [
        "def train_model(config,):\n",
        "  dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], config['longueur_sequence'],longueur_sortie,config['batch_size'],shift)\n",
        "  dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],config['longueur_sequence'],longueur_sortie,config['batch_size'],shift)\n",
        "  x_train, y_train = Create_train(dataset)\n",
        "  x_val, y_val = Create_train(dataset_val)\n",
        "  \n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "  with strategy.scope():\n",
        "    model = DSTP_model(config)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config['lr']),loss='mse')\n",
        "  \n",
        "  callbacks = create_callbacks()\n",
        "  history = model.fit(x=[x_train[0],x_train[1]],y=y_train,epochs=500,callbacks=callbacks,validation_data=([x_val[0],x_val[1]],y_val),batch_size=config['batch_size'])\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbCbgrqkPrd4"
      },
      "source": [
        "from ray.tune import Callback\n",
        "import ftplib\n",
        "\n",
        "class SendFileToFTP(Callback):\n",
        "  def on_trial_complete(self,iteration,trials,trial,**info):\n",
        "    ftp= ftplib.FTP()\n",
        "    HOST = \"62.210.208.36\"\n",
        "    PORT = 2122\n",
        "    ftp.connect(HOST,PORT)\n",
        "    ftp.login('rdpdo','passamoi290876')\n",
        "    os.system(\"zip -r /content/ray_results/RayTuneDSTPI_SP500.zip /content/ray_results\")\n",
        "    localfile = \"/content/ray_results/RayTuneDSTPI_SP500.zip\"\n",
        "    remotefile = \"RayTuneDSTPI_SP500.zip\"\n",
        "    with open(localfile,\"rb\") as file:\n",
        "      ftp.storbinary('STOR %s' %remotefile,file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXnzuTH0uiZU"
      },
      "source": [
        "# Téléchargement des résultats précédents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHhMs-fd_ZKW"
      },
      "source": [
        "train_dir = os.path.abspath(\"ray_results/train_dir\")\n",
        "val_dir = os.path.abspath(\"ray_results/val_dir\")\n",
        "checkpoint_dir = os.path.abspath(\"ray_results/chackpoint_dir\")\n",
        "results_dir = os.path.abspath(\"ray_results\")\n",
        "\n",
        "!rm -r \"/content/ray_results\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ6alY_dul3L"
      },
      "source": [
        "import ftplib\n",
        "\n",
        "ftp= ftplib.FTP()\n",
        "HOST = \"62.210.208.36\"\n",
        "PORT = 2122\n",
        "ftp.connect(HOST,PORT)\n",
        "ftp.login('rdpdo','passamoi290876')\n",
        "remotefile = \"/home/rdpdo/RayTuneDSTPI_SP500.zip\"\n",
        "ftp.retrbinary('RETR %s' %remotefile, open('/content/RayTuneDSTPI_SP500.zip',\"wb\").write)\n",
        "os.system(\"unzip /content/RayTuneDSTPI_SP500.zip -d /\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl-0TUJeloJC"
      },
      "source": [
        "# Lancement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6b5eCeMorVe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/ray_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiRfpaXTB_YM"
      },
      "source": [
        "import ray\n",
        "import os\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.suggest import ConcurrencyLimiter\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "\n",
        "ray.init(configure_logging=False,ignore_reinit_error=True)\n",
        "config, initial_best_config = create_search_space()\n",
        "\n",
        "scheduler = AsyncHyperBandScheduler(time_attr='training_iteration',metric=\"val_loss\",mode=\"min\",grace_period=800,max_t=1000)\n",
        "search_alg = HyperOptSearch(metric=\"val_loss\",mode=\"min\",points_to_evaluate=initial_best_config)\n",
        "search_alg = ConcurrencyLimiter(search_alg, max_concurrent=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRa7aHc7lvV-"
      },
      "source": [
        "analysis = tune.run(train_model, \n",
        "                    local_dir=results_dir,\n",
        "                    name=\"-RayTuneDSTPI_SP500\",\n",
        "                    verbose=0,\n",
        "                    num_samples=1000,\n",
        "                    scheduler=scheduler,\n",
        "                    search_alg=search_alg,\n",
        "                    raise_on_failed_trial=False,\n",
        "                    resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
        "                    config=config,\n",
        "                    checkpoint_freq = 1,\n",
        "                    checkpoint_at_end=True,\n",
        "#                    resume=True,\n",
        "                    callbacks=[SendFileToFTP()])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}