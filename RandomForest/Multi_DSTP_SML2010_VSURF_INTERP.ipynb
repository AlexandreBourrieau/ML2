{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_DSTP_SML2010_VSURF_INTERP.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/RandomForest/Multi_DSTP_SML2010_VSURF_INTERP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Ce dataset est utilisé pour effectuer la prédiction de la température d'une pièce en fonction de plusieurs paramètres mesurés. La fréquence originale des données est d'une minute, puis a été modifiée à 15minutes avec un filtrage. L'ensemble correspond environ à une durée de 40 jours.  \n",
        "Nous allons utiliser ici la température de la chambre comme cible et sélectionner 18 séries exogènes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/SMLselected_VSURF_pred.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_etude = pd.read_csv(\"SMLselected_VSURF_pred.csv\")\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oElnLHPailF"
      },
      "source": [
        "df_etude = df_etude.drop(['Unnamed: 0'],axis=1)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INH5D4lncQRY"
      },
      "source": [
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igyc5qUTcdXo"
      },
      "source": [
        "Modifie les type en float32 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_0svvF8chHQ"
      },
      "source": [
        "df_etude = df_etude.astype(dtype='float32')\n",
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['Temperature_Habitacion_Sensor'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude.values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128\n",
        "longueur_sequence = 20\n",
        "longueur_sortie = 1\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train = [X1,X2]\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_val = [X1,X2]\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2VZ45y73VTh"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(np.linspace(0,longueur_sequence,longueur_sequence),x_train[0][0,:,0:3],label=\"X_train (X)\")\n",
        "ax.plot(np.linspace(0,longueur_sequence,longueur_sequence),x_train[1][0,:,:],label=\"X_train (Y)\")\n",
        "\n",
        "ax.plot(np.linspace(longueur_sequence+1,longueur_sequence+2,1),y_train[0,:,:],label=\"Y_train\",marker=\"*\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle DSTP-RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0lDwKjfzzPh"
      },
      "source": [
        "Le modèle DSTP-RNN implanté est le suivant :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFTrTbJ5zuqj"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNN-VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "**1. Création de la couche d'attention spatiale de l'étage n°1 / Phase 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmThmaJnoyEF"
      },
      "source": [
        "On commence par créer la couche permettant de calculer le score. Cette fonction calcule le score de l'encodeur, c'est-à-dire le score à attribuer à chaque série d'entrée.  \n",
        "Cette fonction est appellée par l'encodeur à l'aide de la méthode TimeDistribued de Keras, pour chaque série d'entrée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx3ac1_G00Ga"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScore__.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiCdmR0dnXp4"
      },
      "source": [
        "class CalculScores_Encodeur_Phase1(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wf = self.add_weight(shape=(input_shape[1],2*self.dim_LSTM),initializer=\"normal\",name=\"Wf\")    # (Tin, 2x#LSTM)\n",
        "    self.Uf = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"Uf\")     # (Tin, Tin)\n",
        "    self.bf = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"bf\")                  # (Tin, 1)\n",
        "    self.vf = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"vf\")                  # (Tin, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     score:          Score               : (batch_size,1,1)\n",
        "  def call(self, input):\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.Wf,hs)                                               # (Tin,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,Tin,1)\n",
        "        e = e + tf.matmul(self.Uf,input)                                        # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bf                                                         # (batch_size,Tin,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Uf,input)                                            # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bf                                                         # (batch_size,Tin,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vf),e)                                  # (1,Tin)x(batch_size,Tin,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                                 # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM8tVDxynXp4"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMlE2tNF1XRB"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase1__.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8n_qKEnXp4"
      },
      "source": [
        "class Encodeur_Phase1(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Encodeur\")\n",
        "    self.CalculScores_Encodeur_Phase1 = CalculScores_Encodeur_Phase1(dim_LSTM=self.dim_LSTM)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  #     index:          index série         : (1)\n",
        "  # Sorties :\n",
        "  #     out_hid : Sortie vecteur caché      : (batch_size,#LSTM)\n",
        "  #     out_cell: Sortie cell state         : (btach_size,#LSTM)\n",
        "  #     x_tilda : Coupe temporelle pondérée : (batch_size,1,#dim)\n",
        "  def call(self, input, hidd_state, cell_state, index):\n",
        "    # Calcul des scores\n",
        "    input_TD = tf.transpose(input,perm=[0,2,1])                               # (batch_size,Tin,#dim) => (batch_size,#dim,Tin)\n",
        "    input_TD = tf.expand_dims(input_TD,axis=-1)                               # (batch_size,#dim,Tin) => (batch_size,#dim,Tin,1)\n",
        "    self.CalculScores_Encodeur_Phase1.SetStates(hidd_state,cell_state)\n",
        "    a = tf.keras.layers.TimeDistributed(\n",
        "        self.CalculScores_Encodeur_Phase1)(input_TD)                          # (batch_size,#dim,Tin,1) : Timestep=#dim\n",
        "                                                                              # (batch_size,Tin,1) envoyé #dim fois en //\n",
        "                                                                              # (batch_size,#dim,1) retourné\n",
        "    # Normalisation des scores alpha\n",
        "    a = tf.keras.activations.softmax(a,axis=1)                                # (batch_size,#dim,1)\n",
        "\n",
        "    # Applique les poids normalisés à la coupe temporelle des séries exogènes\n",
        "    x_tilda = tf.multiply(tf.expand_dims(input[:,index,:],-1),a)              # (batch_size,#dim,1) _x_ (batch_size,#dim,1) = (batch_size,#dim,1)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,1,#dim)\n",
        "\n",
        "    # Applique x_tilda à la cellule LSTM\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,#dim,1)\n",
        "    out_dec, out_hid, out_cell = self.couche_LSTM(x_tilda)                    # out_dec et out_cell : (batch_size,#LSTM)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,1,#dim)\n",
        "\n",
        "    return out_hid, out_cell, x_tilda\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI5YwkzX84Vi"
      },
      "source": [
        "**2. Création de la couche d'attention spatiale de l'étage n°1 / Phase 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN79pZ349FR6"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase2_CalculScore__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6pu4agmAtqB"
      },
      "source": [
        "On commence par créer le calcul  du score :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pA1z8N-koF"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase2_CalculScore2__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-IiIBuJ-SKi"
      },
      "source": [
        "class CalculScores_Encodeur_Phase2(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Ws = self.add_weight(shape=(input_shape[1],2*self.dim_LSTM),initializer=\"normal\",name=\"Ws\")    # (Tin, 2x#LSTM)\n",
        "    self.Us = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"Us\")     # (Tin, Tin)\n",
        "    self.bs = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"bs\")                  # (Tin, 1)\n",
        "    self.vs = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"vs\")                  # (Tin, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées Z           : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     score:          Score               : (batch_size,1,1)\n",
        "  def call(self, input):\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.Ws,hs)                                               # (Tin,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,Tin,1)\n",
        "        e = e + tf.matmul(self.Us,input)                                        # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bs                                                         # (batch_size,Tin,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Us,input)                                            # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bs                                                         # (batch_size,Tin,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vs),e)                                  # (1,Tin)x(batch_size,Tin,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                                 # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe10l-HI_Nsb"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WmZzmBV_p4D"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase22__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7elzJ8S_Rrd"
      },
      "source": [
        "class Encodeur_Phase2(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Encodeur\")\n",
        "    self.CalculScores_Encodeur_Phase2 = CalculScores_Encodeur_Phase2(dim_LSTM=self.dim_LSTM)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées Z           : (batch_size,Tin,#dim+1)\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  #     index:          index série         : (1)\n",
        "  # Sorties :\n",
        "  #     out_hid : Sortie vecteur caché      : (batch_size,#LSTM)\n",
        "  #     out_cell: Sortie cell state         : (btach_size,#LSTM)\n",
        "  def call(self, input, hidd_state, cell_state, index):\n",
        "    # Calcul des scores\n",
        "    input_TD = tf.transpose(input,perm=[0,2,1])                               # (batch_size,Tin,#dim+1) => (batch_size,#dim+1,Tin)\n",
        "    input_TD = tf.expand_dims(input_TD,axis=-1)                               # (batch_size,#dim+1,Tin) => (batch_size,#dim+1,Tin,1)\n",
        "    self.CalculScores_Encodeur_Phase2.SetStates(hidd_state,cell_state)\n",
        "    b = tf.keras.layers.TimeDistributed(\n",
        "        self.CalculScores_Encodeur_Phase2)(input_TD)                          # (batch_size,#dim+1,Tin,1) : Timestep=#dim+1\n",
        "                                                                              # (batch_size,Tin,1) envoyé #dim+1 fois en //\n",
        "                                                                              # (batch_size,#dim+1,1) retourné\n",
        "    # Normalisation des scores beta\n",
        "    b = tf.keras.activations.softmax(b,axis=1)                                # (batch_size,#dim+1,1)\n",
        "\n",
        "    # Applique les poids normalisés à la série\n",
        "    z_tilda = tf.multiply(tf.expand_dims(input[:,index,:],-1),b)              # (batch_size,#dim+1,1) _x_ (batch_size,#dim+1,1) = (batch_size,#dim+1,1)\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,1,#dim+1)\n",
        "\n",
        "    # Applique z_tilda à la cellule LSTM\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,#dim+1,1)\n",
        "    out_dec, out_hid, out_cell = self.couche_LSTM(z_tilda)                    # out_dec et out_cell : (batch_size,#LSTM)\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,1,#dim+1)\n",
        "\n",
        "    return out_hid, out_cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLuRur14AWxz"
      },
      "source": [
        "**3. Création de la couche d'attention du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-_zhkmaDucJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur3.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyjAVbSbAama"
      },
      "source": [
        "On commence par créer la couche de calcul du score du décodeur.  \n",
        "Cette fonction calcule le score du décodeur, c'est-à-dire le score à attribuer à chaque hidden-state en sortie de l'encodeur.  \n",
        "Cette fonction est appellée par la couche d'attention temporelle du décodeur à l'aide de la méthode TimeDistribued de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPqAY_fBByOC"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur4.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohWFJ93YCc9i"
      },
      "source": [
        "class CalculScores_Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    super().__init__()                  # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wd = self.add_weight(shape=(self.dim_LSTM,2*self.dim_LSTM),initializer=\"normal\",name=\"Wd\")     # (#LSTM, 2x#LSTM)\n",
        "    self.Ud = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ud\")       # (#LSTM, #LSTM)\n",
        "    self.bd = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bd\")                   # (#LSTM, 1)\n",
        "    self.vd = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"vd\")                   # (#LSTM, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:        Entrée score décodeur : (batch_size,#LSTM)\n",
        "  # Sorties :\n",
        "  #     score:        score                 : (batch_size,1)\n",
        "  def call(self,input):\n",
        "    input = tf.expand_dims(input,-1)\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.Wd,hs)                                               # (#LSTM,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + tf.matmul(self.Ud,input)                                        # (#LSTM,#LSTM)x(batch_size,#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + self.bd                                                         # (batch_size,#LSTM,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Ud,input)                                            # (#LSTM,#LSTM)x(batch_size,#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + self.bd                                                         # (batch_size,#LSTM,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vd),e)                                  # (1,#LSTM)x(batch_size,#LSTM,1) = (batch_size,1,1)\n",
        "    score = tf.squeeze(score,-1)                                                # (batch_size,1)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BeYO8p6DZ6b"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diFuNNIpEf6i"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur5.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v5qRdB9DpJC"
      },
      "source": [
        "class CalculAttention_Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_CalculScores_Decodeur = CalculScores_Decodeur(dim_LSTM=self.dim_LSTM)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#LSTM)\n",
        "  # Sorties :\n",
        "  #     vect_contexte   Vecteur Contexte    : (batch_size,1,#LSTM)\n",
        "  def call(self, input):\n",
        "    # Calcul des scores\n",
        "    self.couche_CalculScores_Decodeur.SetStates(self.hidd_state,self.cell_state)\n",
        "    g = tf.keras.layers.TimeDistributed(\n",
        "        self.couche_CalculScores_Decodeur)(input)                             # (batch_size,Tin,#LSTM) : Timestep=Tin\n",
        "                                                                              # (batch_size,#LSTM) envoyé Tin fois en //\n",
        "                                                                              # (batch_size,Tin,1) retourné\n",
        "    # Normalisation des scores gama\n",
        "    g = tf.keras.activations.softmax(g,axis=1)                                # (batch_size,Tin,1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    C = tf.multiply(input,g)        # (batch_size,Tin,#LSTM)_x_(batch_size,Tin,1) = (batch_size,Tin,#LSTM)\n",
        "    C = K.sum(C,axis=1)             # (batch_size,#LSTM)\n",
        "    C = tf.expand_dims(C,1)         # (batch_size,1,#LSTM)\n",
        "    return C\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbk9Nbn8EvEx"
      },
      "source": [
        "**4. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L72CzheDExup"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CoucheDecodeurAll.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tJnICP9FStv"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_Attention_Decodeur = CalculAttention_Decodeur(dim_LSTM=self.dim_LSTM)\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Decodeur\")\n",
        "    self.W = self.add_weight(shape=(self.dim_LSTM+1,1),initializer=\"normal\",name=\"W\")                   # (#LSTM+1, 1)\n",
        "    self.b = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"b\")                                 # (1, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:        Entrée décodeur       : (batch_size,Tin,#LSTM)\n",
        "  #     Y:            Yt                    : (batch_size,1,1)\n",
        "  #     hid_state:    hidden state          : (batch_size,#LSTM)\n",
        "  #     cell_state:   cell_state            : (batch_size,#LSTM)\n",
        "  # Sorties :\n",
        "  #     out_hid :     hidden_state          : (batch_size,#LSTM)\n",
        "  #     out_cell :    cell_state            : (batch_size,#LSTM)\n",
        "  #     v_contexte:   vecteur contexte      : (batch_size,#LSTM)\n",
        "  def call(self,input,Y,hid_state,cell_state):\n",
        "    # Calcul du vecteur contexte\n",
        "    self.couche_Attention_Decodeur.SetStates(hid_state,cell_state)\n",
        "    C = self.couche_Attention_Decodeur(input)           # (batch_size,1,#LSTM)\n",
        "\n",
        "    # Calcul de Y_tilda\n",
        "    add = tf.keras.layers.concatenate([Y,C],axis=2)     # (batch_size,1,#LSTM+1)\n",
        "    add = tf.transpose(add,perm=[0,2,1])                # (batch_size,#LSTM+1,1)\n",
        "    Y_tilda = tf.matmul(tf.transpose(self.W),add)       # (1,#LSTM+1) x (batch_size,#LSTM+1,1) = (batch_size,1,1)\n",
        "    Y_tilda = Y_tilda + self.b\n",
        "\n",
        "    # Calcul des hidden state et cell state\n",
        "    if hid_state is not None:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda,initial_state=[hid_state,cell_state])\n",
        "    else:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda)\n",
        "\n",
        "    return out_hid,out_cell, C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJz4ghULFn6a"
      },
      "source": [
        "**5. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8pXmw_bFxoJ"
      },
      "source": [
        "Il ne reste plus qu'à créer l'architecture complète et d'ajouter l'estimation de la sortie :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qpMf0rFtsJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNN-VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUEWJblwv3BI"
      },
      "source": [
        "Prédictions des valeurs multi-step :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7eE1A-8GrvC"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNNPredictions__.png?raw=true' width=600>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD5OoZc4K7SJ"
      },
      "source": [
        "class Net_DSTPRNN(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur_phase1, encodeur_phase2,decodeur,longueur_sequence, longueur_sortie, dim_LSTM, regul=0.0, drop = 0.0):\n",
        "    self.encodeur_phase1 = encodeur_phase1\n",
        "    self.encodeur_phase2 = encodeur_phase2\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wy = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM,2*self.dim_LSTM),initializer=\"normal\",name=\"Wy\")        # (longueur_sortie,#LSTM, 2x#LSTM)\n",
        "    self.by = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM,1),initializer=\"normal\",name=\"by\")                      # (longueur_sortie,#LSTM, 1)\n",
        "    self.vy = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM,1),initializer=\"normal\",name=\"vy\")                      # (longueur_sortie,#LSTM,1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     output_seq:     Sortie séquence Y   : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction Y        : (batch_size,longueur_sortie,1)\n",
        "  def call(self,input,output_seq):\n",
        "    # Phase n°1 d'encodage\n",
        "    # Calcul les représentations spatiales pondérées\n",
        "    # des coupes temporelles des séries exogènes en entrée\n",
        "    # x_tilda\n",
        "    x_tilda = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state, x_t = self.encodeur_phase1(input,hid_state,cell_state,i)\n",
        "      x_t = tf.squeeze(x_t,1)                     # (batch_size,1,#dim) => (batch_size,#dim)\n",
        "      x_tilda.append(x_t)                         # (batch_size,#dim)\n",
        "    x_tilda = tf.convert_to_tensor(x_tilda)       # (Tin,batch_size,#dim)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[1,0,2])  # (batch_size,Tin,#dim)\n",
        "\n",
        "    # Concaténation des sorties de la phase 1 avec la série cible\n",
        "    Z = []\n",
        "\n",
        "    for i in range(input.shape[1]):\n",
        "      z = tf.keras.layers.concatenate([x_tilda[:,i,:],                 # (batch_size,#dim+1)\n",
        "                                       output_seq[:,i,:]],axis=1)\n",
        "      Z.append(z)\n",
        "    Z = tf.convert_to_tensor(Z)                   # (Tin,batch_size,#dim+1)\n",
        "    Z = tf.transpose(Z,perm=[1,0,2])              # (batch_size,Tin,#dim+1)\n",
        "\n",
        "    # Phase n°2 d'encodage\n",
        "    # Création des représentations cachées des\n",
        "    # concaténations précédentes\n",
        "    hid = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state = self.encodeur_phase2(Z,hid_state,cell_state,i)\n",
        "      hid.append(hid_state)\n",
        "    hid = tf.convert_to_tensor(hid)               # (Tin,batch_size,#LSTM)\n",
        "    hid = tf.transpose(hid,perm=[1,0,2])          # (batch_size,Tin,#LSTM)\n",
        "\n",
        "\n",
        "    # Phase de décodage\n",
        "    # Récupère les états cachés à (T-1)\n",
        "    hid_ = None\n",
        "    cell_ = None\n",
        "    for i in range(0,output_seq.shape[1]-1):\n",
        "      hid_, cell_, vc = self.decodeur(hid,output_seq[:,i:i+1,:],hid_,cell_)\n",
        "    \n",
        "    # hid_  : hT-1    : hidden state à t=T-1\n",
        "    # cell_ : sT-1    : cell state à t=T-1\n",
        "    # vc    : CT-1    : vecteur contexte à t=T-1\n",
        "    \n",
        "    # Estimation des sorties\n",
        "    # hid_ : (batch_size,#LSTM)\n",
        "    # vc   : (batch_size,1,#LSTM)\n",
        "    Y = []\n",
        "    y = tf.expand_dims(output_seq[:,-1,:],-1)        # y = YT : (batch_size,1,1)\n",
        "    \n",
        "    for i in range(0,self.longueur_sortie):\n",
        "      hid_, cell_, vc = self.decodeur(hid,y,hid_,cell_)\n",
        "      add = tf.keras.layers.concatenate([tf.expand_dims(hid_,1),vc],axis=2)         # (batch_size,1,2x#LSTM)\n",
        "      add = tf.transpose(add,perm=[0,2,1])                                          # (batch_size,2x#LSTM,1)\n",
        "      sortie = tf.matmul(self.Wy[i,:,:],add)                                      # (#LSTM,2x#LSTM) x (batch_size,2x#LSTM+1,1) = (batch_size,#LSTM,1)\n",
        "      sortie = sortie + self.by[i,:,:]                                            # (batch_size,#LSTM,1)\n",
        "      sortie = tf.matmul(tf.transpose(self.vy[i,:,:]),sortie)                     # (1,#LSTM)x(batch_size,#LSTM,1) = (batch_size,1,1)\n",
        "      y = sortie\n",
        "      Y.append(y)\n",
        "\n",
        "    Y = tf.convert_to_tensor(Y)           # Y = (longueur_sortie,batch_size,1,1)\n",
        "    Y = tf.transpose(Y,perm=[1,0,2,3])    # Y = (batch_size,longueur_sortie,1,1)\n",
        "    Y = tf.squeeze(Y,-1)                  # Y = (batch_size,longueur_sortie,1)\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj12PHWgPTjD"
      },
      "source": [
        "**6. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ9tj9S2PWqM"
      },
      "source": [
        "dim_LSTM = 128\n",
        "drop=0.0\n",
        "l2reg=0.0\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,x_train[0].shape[2]))\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "\n",
        "  encodeur_P1 = Encodeur_Phase1(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  encodeur_P2 = Encodeur_Phase2(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "\n",
        "  sortie = Net_DSTPRNN(encodeur_P1,encodeur_P2,decodeur,longueur_sequence=longueur_sequence,longueur_sortie=longueur_sortie, dim_LSTM=dim_LSTM,regul=l2reg,drop=drop)(entrees_sequences,sorties_sequence)\n",
        "\n",
        "  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 500\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#  optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train[0],x_train[1]],y=y_train,validation_data=([x_val[0],x_val[1]],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=200)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jheNXKz5O3aO"
      },
      "source": [
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vve7jENFlN3"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxjwVyvJFmtx"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plI7hKwCPWnm"
      },
      "source": [
        "start = 400\n",
        "\n",
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[start:])),erreur_entrainement[start:], label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement[start:])),erreur_validation[start:], label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRamuIcLFoCJ"
      },
      "source": [
        "model.evaluate(x=[x_train[0],x_train[1]],y=y_train)\n",
        "model.evaluate(x=[x_val[0],x_val[1]],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05bqnRr-pyf"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Models/Multi_DSTP_SML2010_VSURF_PRED.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XKdrbK-022"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"Multi_DSTP_SML2010_VSURF_PRED.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train[0],x_train[1]],verbose=1)\n",
        "pred_val = model.predict([x_val[0],x_val[1]],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,-1],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wCeoEnMEYv"
      },
      "source": [
        "**Erreurs en single step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2_ceD69Mju6"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_ent = tf.keras.losses.mse(serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV5JTpnnNApH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_test = tf.keras.losses.mse(serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FShIaJINHBX"
      },
      "source": [
        "print(mse_ent)\n",
        "print(mse_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
