{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_DSTPII_VAR_PID_SML2010_MultiStep.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrOOJabrUaZ2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **DSTPII + VAR + Correcteur PID**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Ce dataset est utilisé pour effectuer la prédiction de la température d'une pièce en fonction de plusieurs paramètres mesurés. La fréquence originale des données est d'une minute, puis a été modifiée à 15minutes avec un filtrage. L'ensemble correspond environ à une durée de 40 jours.  \n",
        "Nous allons utiliser ici la température de la chambre comme cible et sélectionner 18 séries exogènes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.txt\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/NEW-DATA-1.T15.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_etude = pd.read_csv(\"NEW-DATA-1.T15.txt\",sep=\" \")\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtpJcijfabsQ"
      },
      "source": [
        "Supprime les colonnes non utiles :\n",
        " - Date et heure\n",
        " - Exterior Entalpic 1, 2 et turbo  \n",
        "   \n",
        "Déplace la cible (4:Temperature_Habitacion_Sensor) en dernière colonne :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oElnLHPailF"
      },
      "source": [
        "df_etude = df_etude.drop(['Date','Time','19:Exterior_Entalpic_1', '20:Exterior_Entalpic_2', '21:Exterior_Entalpic_turbo'],axis=1)\n",
        "cible = df_etude.pop(\"4:Temperature_Habitacion_Sensor\")\n",
        "df_etude.insert(len(df_etude.columns),\"Temperature_Habitacion_Sensor\",cible)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INH5D4lncQRY"
      },
      "source": [
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igyc5qUTcdXo"
      },
      "source": [
        "Modifie les type en float32 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_0svvF8chHQ"
      },
      "source": [
        "df_etude = df_etude.astype(dtype='float32')\n",
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['Temperature_Habitacion_Sensor'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude.values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i817Q5rwIL_x"
      },
      "source": [
        "Les datasets sont créés de la manière suivante :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwssikbyIEzc"
      },
      "source": [
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/Datasets_DSTP.png?raw=true' width=700/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoiHptoQyTxL"
      },
      "source": [
        "**1. Exemple de dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQQTRH_5wjrc"
      },
      "source": [
        "X1 = np.linspace(1,100,100)\n",
        "X2 = np.linspace(101,200,100)\n",
        "X3 = np.linspace(201,300,100)\n",
        "Y = np.linspace(301,400,100)\n",
        "\n",
        "X1 = tf.expand_dims(X1,-1)\n",
        "X2 = tf.expand_dims(X2,-1)\n",
        "X3 = tf.expand_dims(X3,-1)\n",
        "Y = tf.expand_dims(Y,-1)\n",
        "\n",
        "Serie_X = tf.concat([X1,X2,X3],axis=1)\n",
        "Serie_Y = Y\n",
        "print(Serie_X.shape)\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),((X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1, YT+2, YT+3, ...\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-longueur_sortie:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "test_dataset = prepare_dataset_XY(Serie_X,Serie_Y,10,4,1,1)\n",
        "\n",
        "print(len(list(test_dataset.as_numpy_iterator())))\n",
        "for element in test_dataset.take(2):\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLEjFxNnyWse"
      },
      "source": [
        "**2. Préparation des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),((X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT)}\n",
        "# Y = YT+1, YT+2, YT+3, ...\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-longueur_sortie:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128\n",
        "longueur_sequence = 20\n",
        "longueur_sortie = 5\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n",
        "  print(element[1].shape)               # YT+1, YT+2, YT+3, ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT\n",
        "  print(element[1].shape)               # YT+1,YT+2, YT+3..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train = [X1,X2]\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_val = [X1,X2]\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9aPVTcDn6TQ"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence),x_train[0][0,:,0:20],label=\"X\")\n",
        "ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence),x_train[1][0,:,:],label=\"Y\")\n",
        "ax.plot(np.linspace(longueur_sequence,longueur_sequence+longueur_sortie-1,longueur_sortie),y_train[0,:,:],label=\"Y\")\n",
        "\n",
        "ax.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle DSTPII-RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0lDwKjfzzPh"
      },
      "source": [
        "Le modèle DSTP-RNN implanté est le suivant :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFTrTbJ5zuqj"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNN-VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "**1. Création de la couche d'attention spatiale de l'étage n°1 / Phase 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmThmaJnoyEF"
      },
      "source": [
        "On commence par créer la couche permettant de calculer le score. Cette fonction calcule le score de l'encodeur, c'est-à-dire le score à attribuer à chaque série d'entrée.  \n",
        "Cette fonction est appellée par l'encodeur à l'aide de la méthode TimeDistribued de Keras, pour chaque série d'entrée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx3ac1_G00Ga"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScore__.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiCdmR0dnXp4"
      },
      "source": [
        "class CalculScores_Encodeur_Phase1(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wf = self.add_weight(shape=(input_shape[1],2*self.dim_LSTM),initializer=\"normal\",name=\"Wf\")    # (Tin, 2x#LSTM)\n",
        "    self.Uf = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"Uf\")     # (Tin, Tin)\n",
        "    self.bf = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"bf\")                  # (Tin, 1)\n",
        "    self.vf = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"vf\")                  # (Tin, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     score:          Score               : (batch_size,1,1)\n",
        "  def call(self, input):\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.Wf,hs)                                               # (Tin,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,Tin,1)\n",
        "        e = e + tf.matmul(self.Uf,input)                                        # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bf                                                         # (batch_size,Tin,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Uf,input)                                            # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bf                                                         # (batch_size,Tin,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vf),e)                                  # (1,Tin)x(batch_size,Tin,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                                 # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM8tVDxynXp4"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMlE2tNF1XRB"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase1__.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX8n_qKEnXp4"
      },
      "source": [
        "class Encodeur_Phase1(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Encodeur\")\n",
        "    self.CalculScores_Encodeur_Phase1 = CalculScores_Encodeur_Phase1(dim_LSTM=self.dim_LSTM)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  #     index:          index série         : (1)\n",
        "  # Sorties :\n",
        "  #     out_hid : Sortie vecteur caché      : (batch_size,#LSTM)\n",
        "  #     out_cell: Sortie cell state         : (btach_size,#LSTM)\n",
        "  #     x_tilda : Coupe temporelle pondérée : (batch_size,1,#dim)\n",
        "  def call(self, input, hidd_state, cell_state, index):\n",
        "    # Calcul des scores\n",
        "    input_TD = tf.transpose(input,perm=[0,2,1])                               # (batch_size,Tin,#dim) => (batch_size,#dim,Tin)\n",
        "    input_TD = tf.expand_dims(input_TD,axis=-1)                               # (batch_size,#dim,Tin) => (batch_size,#dim,Tin,1)\n",
        "    self.CalculScores_Encodeur_Phase1.SetStates(hidd_state,cell_state)\n",
        "    a = tf.keras.layers.TimeDistributed(\n",
        "        self.CalculScores_Encodeur_Phase1)(input_TD)                          # (batch_size,#dim,Tin,1) : Timestep=#dim\n",
        "                                                                              # (batch_size,Tin,1) envoyé #dim fois en //\n",
        "                                                                              # (batch_size,#dim,1) retourné\n",
        "    # Normalisation des scores alpha\n",
        "    a = tf.keras.activations.softmax(a,axis=1)                                # (batch_size,#dim,1)\n",
        "\n",
        "    # Applique les poids normalisés à la coupe temporelle des séries exogènes\n",
        "    x_tilda = tf.multiply(tf.expand_dims(input[:,index,:],-1),a)              # (batch_size,#dim,1) _x_ (batch_size,#dim,1) = (batch_size,#dim,1)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,1,#dim)\n",
        "\n",
        "    # Applique x_tilda à la cellule LSTM\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,#dim,1)\n",
        "    out_dec, out_hid, out_cell = self.couche_LSTM(x_tilda)                    # out_dec et out_cell : (batch_size,#LSTM)\n",
        "    x_tilda = tf.transpose(x_tilda,perm=[0,2,1])                              # (batch_size,1,#dim)\n",
        "\n",
        "    return out_hid, out_cell, x_tilda\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI5YwkzX84Vi"
      },
      "source": [
        "**2. Création de la couche d'attention spatiale de l'étage n°1 / Phase 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN79pZ349FR6"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase2_CalculScore__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6pu4agmAtqB"
      },
      "source": [
        "On commence par créer le calcul  du score :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pA1z8N-koF"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase2_CalculScore2__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-IiIBuJ-SKi"
      },
      "source": [
        "class CalculScores_Encodeur_Phase2(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Ws = self.add_weight(shape=(input_shape[1],2*self.dim_LSTM),initializer=\"normal\",name=\"Ws\")    # (Tin, 2x#LSTM)\n",
        "    self.Us = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"Us\")     # (Tin, Tin)\n",
        "    self.bs = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"bs\")                  # (Tin, 1)\n",
        "    self.vs = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"vs\")                  # (Tin, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées Z           : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     score:          Score               : (batch_size,1,1)\n",
        "  def call(self, input):\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.Ws,hs)                                               # (Tin,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,Tin,1)\n",
        "        e = e + tf.matmul(self.Us,input)                                        # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bs                                                         # (batch_size,Tin,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Us,input)                                            # (Tin,Tin)x(batch_size,Tin,1) = (batch_size,Tin,1)\n",
        "        e = e + self.bs                                                         # (batch_size,Tin,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vs),e)                                  # (1,Tin)x(batch_size,Tin,1) = (batch_size,1,1)\n",
        "    return tf.squeeze(score,-1)                                                 # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe10l-HI_Nsb"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WmZzmBV_p4D"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/EncodeurPhase22__.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7elzJ8S_Rrd"
      },
      "source": [
        "class Encodeur_Phase2(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Encodeur\")\n",
        "    self.CalculScores_Encodeur_Phase2 = CalculScores_Encodeur_Phase2(dim_LSTM=self.dim_LSTM)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées Z           : (batch_size,Tin,2*#dim+2)\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)]\n",
        "  #     index:          index série         : (1)\n",
        "  # Sorties :\n",
        "  #     out_hid : Sortie vecteur caché      : (batch_size,#LSTM)\n",
        "  #     out_cell: Sortie cell state         : (btach_size,#LSTM)\n",
        "  def call(self, input, hidd_state, cell_state, index):\n",
        "    # Calcul des scores\n",
        "    input_TD = tf.transpose(input,perm=[0,2,1])                               # (batch_size,Tin,2*#dim+2) => (batch_size,2*#dim+2,Tin)\n",
        "    input_TD = tf.expand_dims(input_TD,axis=-1)                               # (batch_size,2*#dim+2,Tin) => (batch_size,2*#dim+2,Tin,1)\n",
        "    self.CalculScores_Encodeur_Phase2.SetStates(hidd_state,cell_state)\n",
        "    b = tf.keras.layers.TimeDistributed(\n",
        "        self.CalculScores_Encodeur_Phase2)(input_TD)                          # (batch_size,2*#dim+2,Tin,1) : Timestep=2*#dim+2\n",
        "                                                                              # (batch_size,Tin,1) envoyé 2*#dim+2 fois en //\n",
        "                                                                              # (batch_size,2*#dim+2,1) retourné\n",
        "    # Normalisation des scores beta\n",
        "    b = tf.keras.activations.softmax(b,axis=1)                                # (batch_size,2*#dim+2,1)\n",
        "\n",
        "    # Applique les poids normalisés à la série\n",
        "    z_tilda = tf.multiply(tf.expand_dims(input[:,index,:],-1),b)              # (batch_size,2*#dim+2,1) _x_ (batch_size,2*#dim+2,1) = (batch_size,2*#dim+2,1)\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,1,2*#dim+2)\n",
        "\n",
        "    # Applique z_tilda à la cellule LSTM\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,2*#dim+2,1)\n",
        "    out_dec, out_hid, out_cell = self.couche_LSTM(z_tilda)                    # out_dec et out_cell : (batch_size,#LSTM)\n",
        "    z_tilda = tf.transpose(z_tilda,perm=[0,2,1])                              # (batch_size,1,2*#dim+2)\n",
        "\n",
        "    return out_hid, out_cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLuRur14AWxz"
      },
      "source": [
        "**3. Création de la couche d'attention du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-_zhkmaDucJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur3.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyjAVbSbAama"
      },
      "source": [
        "On commence par créer la couche de calcul du score du décodeur.  \n",
        "Cette fonction calcule le score du décodeur, c'est-à-dire le score à attribuer à chaque hidden-state en sortie de l'encodeur.  \n",
        "Cette fonction est appellée par la couche d'attention temporelle du décodeur à l'aide de la méthode TimeDistribued de Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPqAY_fBByOC"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur4.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohWFJ93YCc9i"
      },
      "source": [
        "class CalculScores_Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    super().__init__()                  # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wd = self.add_weight(shape=(self.dim_LSTM,2*self.dim_LSTM),initializer=\"normal\",name=\"Wd\")     # (#LSTM, 2x#LSTM)\n",
        "    self.Ud = self.add_weight(shape=(self.dim_LSTM,self.dim_LSTM),initializer=\"normal\",name=\"Ud\")       # (#LSTM, #LSTM)\n",
        "    self.bd = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"bd\")                   # (#LSTM, 1)\n",
        "    self.vd = self.add_weight(shape=(self.dim_LSTM,1),initializer=\"normal\",name=\"vd\")                   # (#LSTM, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], 1)\n",
        "\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:        Entrée score décodeur : (batch_size,#LSTM)\n",
        "  # Sorties :\n",
        "  #     score:        score                 : (batch_size,1)\n",
        "  def call(self,input):\n",
        "    input = tf.expand_dims(input,-1)\n",
        "    if self.hidd_state is not None:\n",
        "        hs = tf.keras.layers.concatenate([self.hidd_state,self.cell_state],axis=1)        # (batch_size,2x#LSTM)\n",
        "        hs = tf.expand_dims(hs,-1)                                              # (batch_size,2x#LSTM) => (batch_size,2#LSTM,1)\n",
        "        e = tf.matmul(self.Wd,hs)                                               # (#LSTM,2x#LSTM)x(batch_size,2x#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + tf.matmul(self.Ud,input)                                        # (#LSTM,#LSTM)x(batch_size,#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + self.bd                                                         # (batch_size,#LSTM,1)\n",
        "    else:\n",
        "        e = tf.matmul(self.Ud,input)                                            # (#LSTM,#LSTM)x(batch_size,#LSTM,1) = (batch_size,#LSTM,1)\n",
        "        e = e + self.bd                                                         # (batch_size,#LSTM,1)\n",
        "    e = K.tanh(e)\n",
        "    score = tf.matmul(tf.transpose(self.vd),e)                                  # (1,#LSTM)x(batch_size,#LSTM,1) = (batch_size,1,1)\n",
        "    score = tf.squeeze(score,-1)                                                # (batch_size,1)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BeYO8p6DZ6b"
      },
      "source": [
        "Puis maintenant la couche d'attention :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diFuNNIpEf6i"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CalculScoreDecodeur5.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v5qRdB9DpJC"
      },
      "source": [
        "class CalculAttention_Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM):\n",
        "    self.dim_LSTM = dim_LSTM          # Dimension des vecteurs cachés\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_CalculScores_Decodeur = CalculScores_Decodeur(dim_LSTM=self.dim_LSTM)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  #     hidd_state:     hidden_state        : (batch_size,#LSTM)\n",
        "  #     cell_state:     Cell state          : (batch_size,#LSTM)\n",
        "  def SetStates(self,hidd_state, cell_state):\n",
        "    self.hidd_state = hidd_state\n",
        "    self.cell_state = cell_state\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#LSTM)\n",
        "  # Sorties :\n",
        "  #     vect_contexte   Vecteur Contexte    : (batch_size,1,#LSTM)\n",
        "  def call(self, input):\n",
        "    # Calcul des scores\n",
        "    self.couche_CalculScores_Decodeur.SetStates(self.hidd_state,self.cell_state)\n",
        "    g = tf.keras.layers.TimeDistributed(\n",
        "        self.couche_CalculScores_Decodeur)(input)                             # (batch_size,Tin,#LSTM) : Timestep=Tin\n",
        "                                                                              # (batch_size,#LSTM) envoyé Tin fois en //\n",
        "                                                                              # (batch_size,Tin,1) retourné\n",
        "    # Normalisation des scores gama\n",
        "    g = tf.keras.activations.softmax(g,axis=1)                                # (batch_size,Tin,1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    C = tf.multiply(input,g)        # (batch_size,Tin,#LSTM)_x_(batch_size,Tin,1) = (batch_size,Tin,#LSTM)\n",
        "    C = K.sum(C,axis=1)             # (batch_size,#LSTM)\n",
        "    C = tf.expand_dims(C,1)         # (batch_size,1,#LSTM)\n",
        "    return C\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbk9Nbn8EvEx"
      },
      "source": [
        "**4. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L72CzheDExup"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/CoucheDecodeurAll.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tJnICP9FStv"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_Attention_Decodeur = CalculAttention_Decodeur(dim_LSTM=self.dim_LSTM)\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,recurrent_dropout=self.drop, name=\"LSTM_Decodeur\")\n",
        "    self.W = self.add_weight(shape=(self.dim_LSTM+1,1),initializer=\"normal\",name=\"W\")                   # (#LSTM+1, 1)\n",
        "    self.b = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"b\")                                 # (1, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:        Entrée décodeur       : (batch_size,Tin,#LSTM)\n",
        "  #     Y:            Yt                    : (batch_size,1,1)\n",
        "  #     hid_state:    hidden state          : (batch_size,#LSTM)\n",
        "  #     cell_state:   cell_state            : (batch_size,#LSTM)\n",
        "  # Sorties :\n",
        "  #     out_hid :     hidden_state          : (batch_size,#LSTM)\n",
        "  #     out_cell :    cell_state            : (batch_size,#LSTM)\n",
        "  #     v_contexte:   vecteur contexte      : (batch_size,#LSTM)\n",
        "  def call(self,input,Y,hid_state,cell_state):\n",
        "    # Calcul du vecteur contexte\n",
        "    self.couche_Attention_Decodeur.SetStates(hid_state,cell_state)\n",
        "    C = self.couche_Attention_Decodeur(input)           # (batch_size,1,#LSTM)\n",
        "\n",
        "    # Calcul de Y_tilda\n",
        "    add = tf.keras.layers.concatenate([Y,C],axis=2)     # (batch_size,1,#LSTM+1)\n",
        "    add = tf.transpose(add,perm=[0,2,1])                # (batch_size,#LSTM+1,1)\n",
        "    Y_tilda = tf.matmul(tf.transpose(self.W),add)       # (1,#LSTM+1) x (batch_size,#LSTM+1,1) = (batch_size,1,1)\n",
        "    Y_tilda = Y_tilda + self.b\n",
        "\n",
        "    # Calcul des hidden state et cell state\n",
        "    if hid_state is not None:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda,initial_state=[hid_state,cell_state])\n",
        "    else:\n",
        "      out_, out_hid, out_cell = self.couche_LSTM(Y_tilda)\n",
        "\n",
        "    return out_hid,out_cell, C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJz4ghULFn6a"
      },
      "source": [
        "**5. Création de la structure complète**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8pXmw_bFxoJ"
      },
      "source": [
        "Il ne reste plus qu'à créer l'architecture complète et d'ajouter l'estimation de la sortie :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qpMf0rFtsJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2SeqMulti/images/DSTPRNN-VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfqj7m9il6Em"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/DSTPII.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Zmh_AfNbgC"
      },
      "source": [
        "# Création du modèle DSTPII Multistep + VAR + PID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAur8DfiZrD6"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/VARPIDMULTI2.png?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nwZjtlOoUUc"
      },
      "source": [
        "**1. Création de la couche VAR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjzwaw8JZ7Bh"
      },
      "source": [
        "On met en place l'équation suivante :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOSbv3jhbGMN"
      },
      "source": [
        "${\\phi _{T + t}} = \\sum\\limits_{i = 1}^T {{A_i} \\cdot {x_i}}  + \\sum\\limits_{i = 1}^{t - 1} {{A_{T + i}} \\cdot {{\\hat Y}_{T + i}} + \\xi }$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_DWfLohe8jn"
      },
      "source": [
        "class VAR(tf.keras.layers.Layer):\n",
        "  def __init__(self,longueur_sortie):\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Avar = self.add_weight(shape=(input_shape[1],input_shape[2]+1,input_shape[2]+1),initializer=\"normal\",name=\"Avar\")          # (Tin,#dim+1,#dim+1)\n",
        "    self.Apred = self.add_weight(shape=(self.longueur_sortie,1),initializer=\"normal\",name=\"Apred\")                                  # (longueur_sortie,1)\n",
        "    self.zeta = self.add_weight(shape=(1,1),initializer=\"normal\",name=\"zeta\")                                                       # (1,1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "  \n",
        "  # Entrées\n",
        "  #     series_X  : Séries exogènes X           : (batch_size,Tin,#dim)\n",
        "  #     cible_Y   : Cible                       : (batch_size,Tin,1)\n",
        "  #     y_pred_1  : Prédiction globale à (t-1)  : (batch_size,1)\n",
        "  #     index :     Pas de temps\n",
        "  # Sorties\n",
        "  #     phi       : Prédiction VAR              : (batch_size,1)\n",
        "  def call(self,series_X,cible_Y,y_pred_1,index):\n",
        "    entree = tf.keras.layers.concatenate([series_X,cible_Y],axis=2)                 # (batch_size,Tin,#dim+1)\n",
        "    entree = tf.transpose(entree,perm=[0,2,1])                                      # (batch_size,#dim+1,Tin)\n",
        "\n",
        "    # Algorithme VAR\n",
        "    sortie = []\n",
        "    for i in range(series_X.shape[1]):\n",
        "      sortie.append(tf.matmul(self.Avar[i,:,:],entree[:,:,i:i+1]))                  # (#dim+1,#dim+1)x(batch_size,#dim+1,1)=(batch_size,#dim+1,1)\n",
        "    sortie = tf.convert_to_tensor(sortie)                                           # (Tin,batch_size,#dim+1,1)\n",
        "    sortie = tf.transpose(sortie,perm=[1,0,2,3])                                    # (batch_size,Tin,#dim+1,1)\n",
        "    sortie = K.sum(sortie,axis=1)                                                   # (batch_size,#dim+1,1)\n",
        "\n",
        "    # Récupère la prédiction de la cible\n",
        "    sortie = sortie[:,-1,:]                                                         # (batch_size,1)\n",
        "\n",
        "    # Ajoute la combinaison linéaire de la prédiction globale à (t-1)\n",
        "    sortie = tf.expand_dims(sortie,-1) + tf.matmul(self.Apred[index:index+1,:],tf.expand_dims(y_pred_1,-1))             # (batch_size,1,1) + (1,1)x(batch_size,1,1) = (batch_size,1,1)\n",
        "    sortie = tf.squeeze(sortie,-1) + self.zeta                                                         # (batch_size,1)\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWxDydrYgzi"
      },
      "source": [
        "**2. Ajout du correcteur PID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qofproTuWmlW"
      },
      "source": [
        "class Correcteur_PID(tf.keras.layers.Layer):\n",
        "  def __init__(self,batch_size,couche):\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "    self.erreur_1 = tf.Variable(shape=(batch_size,1),trainable=False,initial_value=tf.zeros(shape=(batch_size,1)),name=\"erreur_1\")               # (BS,1) = 0\n",
        "    self.num_iteration = tf.Variable(shape=(batch_size,1),trainable=False,initial_value=tf.ones(shape=(batch_size,1)),name=\"iteration\")          # (BS,1) = 1\n",
        "    self.couche = couche\n",
        "    self.init = False\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Couche_Dense_P = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Ones(),trainable=True,name=\"CoucheP\")\n",
        "    self.Couche_Dense_I = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Zeros(),trainable=True,name=\"CoucheI\")\n",
        "    self.Couche_Dense_D = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Zeros(),trainable=True,name=\"CoucheD\")\n",
        "    self.Couche_DenseII = tf.keras.layers.Dense(units=1,use_bias=True,kernel_initializer=tf.keras.initializers.Ones(),trainable=True,activation=self.couche,name=\"CoucheFNNII\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "  \n",
        "\n",
        "  # Entrées\n",
        "  #     y_tilda_1 :   Prédiction initiale à (t-1) :   (batch_size,1)\n",
        "  #     y_pred_1  :   Prédiction globale à (t-1)  :   (batch_size,1)\n",
        "  # Sorties\n",
        "  #     PID(erreur) : Erreur traitée par PID      :   (batch_size,1)\n",
        "  def call(self, y_tilda_1,y_pred_1):\n",
        "    # Calcul de l'erreur courante\n",
        "    erreur = y_pred_1-y_tilda_1                                                   # (batch_size,1)\n",
        "\n",
        "    # Calcul de la partie proportionnelle\n",
        "    P = self.Couche_Dense_P(erreur)                                               # (batch_size,1)\n",
        "\n",
        "    # Calcul de la partie intégrale\n",
        "    I = (1/self.num_iteration)*self.Couche_Dense_I(self.erreur_1+erreur)          # (batch_size,1)\n",
        "\n",
        "    # Calcul de la partie dérivée\n",
        "    D = self.Couche_Dense_D(erreur-self.erreur_1)                                 # (batch_size,1)\n",
        "    \n",
        "    # Sauvegarde des erreurs et #itérations\n",
        "    self.num_iteration.assign(self.num_iteration+1)                               # (batch_size,1)\n",
        "    self.erreur_1.assign(erreur + self.erreur_1)                                  # (batch_size,1)\n",
        "\n",
        "    return self.Couche_DenseII(tf.keras.layers.Concatenate(axis=1)([P,I,D]))        # (batch_size,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYOTdM7fZT65"
      },
      "source": [
        "**3. Création de la couche DSTPII+VAR+PID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9OvfclMEsuM"
      },
      "source": [
        "class Net_DSTPRNNII_VARPID(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur_phase1_I, encodeur_phase1_II, encodeur_phase2,decodeur,pred_VAR, correcteur_PID,longueur_sequence, longueur_sortie, dim_LSTM, regul=0.0, drop = 0.0):\n",
        "    self.encodeur_phase1_I = encodeur_phase1_I\n",
        "    self.encodeur_phase1_II = encodeur_phase1_II\n",
        "    self.encodeur_phase2 = encodeur_phase2\n",
        "    self.decodeur = decodeur\n",
        "    self.pred_VAR = pred_VAR\n",
        "    self.correcteur_PID = correcteur_PID\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.drop = drop\n",
        "    self.dim_LSTM = dim_LSTM\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wy = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM,2*self.dim_LSTM),initializer=\"normal\",name=\"Wy\")        # (longueur_sortie,#LSTM, 2x#LSTM)\n",
        "    self.by = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM,1),initializer=\"normal\",name=\"by\")                      # (longueur_sortie,#LSTM, 1)\n",
        "    self.vy = self.add_weight(shape=(self.longueur_sortie,self.dim_LSTM,1),initializer=\"normal\",name=\"vy\")                      # (longueur_sortie,#LSTM,1)\n",
        "    self.Couche_DenseI = tf.keras.layers.Dense(units=1,use_bias=True,name=\"CoucheFNNI\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X           : (batch_size,Tin,#dim)\n",
        "  #     output_seq:     Sortie séquence Y   : (batch_size,Tin,1)\n",
        "  # Sorties :\n",
        "  #     sortie:         Prédiction Y        : (batch_size,longueur_sortie,1)\n",
        "  def call(self,input,output_seq):\n",
        "    # Phase n°1-I d'encodage\n",
        "    # Calcul les représentations spatiales pondérées\n",
        "    # des coupes temporelles des séries exogènes en entrée\n",
        "    # x_tilda_1\n",
        "    x_tilda_1 = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state, x_t = self.encodeur_phase1_I(input,hid_state,cell_state,i)\n",
        "      x_t = tf.squeeze(x_t,1)                         # (batch_size,1,#dim) => (batch_size,#dim)\n",
        "      x_tilda_1.append(x_t)                           # (batch_size,#dim)\n",
        "    x_tilda_1 = tf.convert_to_tensor(x_tilda_1)       # (Tin,batch_size,#dim)\n",
        "    x_tilda_1 = tf.transpose(x_tilda_1,perm=[1,0,2])  # (batch_size,Tin,#dim)\n",
        "\n",
        "    # Phase n°1-II d'encodage\n",
        "    # Calcul les représentations spatiales pondérées\n",
        "    # des coupes temporelles des séries exogènes en entrée\n",
        "    # x_tilda_2\n",
        "    x_tilda_2 = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state, x_t = self.encodeur_phase1_II(tf.keras.layers.concatenate([input,output_seq],axis=2),hid_state,cell_state,i)\n",
        "      x_t = tf.squeeze(x_t,1)                           # (batch_size,1,#dim+1) => (batch_size,#dim+1)\n",
        "      x_tilda_2.append(x_t)                             # (batch_size,#dim+1)\n",
        "    x_tilda_2 = tf.convert_to_tensor(x_tilda_2)         # (Tin,batch_size,#dim+1)\n",
        "    x_tilda_2 = tf.transpose(x_tilda_2,perm=[1,0,2])    # (batch_size,Tin,#dim+1)\n",
        "\n",
        "    # Concaténation des sorties des phases 1-II et de la série cible\n",
        "    Z2 = []\n",
        "    for i in range(input.shape[1]):\n",
        "      z = tf.keras.layers.concatenate([x_tilda_2[:,i,:],                  # (batch_size,#dim+1)\n",
        "                                       output_seq[:,i,:]]                 # (batch_size,1)\n",
        "                                      ,axis=1)                            # = (batch_size,#dim+2)\n",
        "      Z2.append(z)\n",
        "    Z2 = tf.convert_to_tensor(Z2)                   # (Tin,batch_size,#dim+2)\n",
        "    Z2 = tf.transpose(Z2,perm=[1,0,2])              # (batch_size,Tin,#dim+2)\n",
        "\n",
        "    # Concaténation des sorties des phases 1-I avec Z2\n",
        "    Z = tf.keras.layers.concatenate([x_tilda_1,Z2],axis=2)  # (batch_size,Tin,2*#dim+2)\n",
        "\n",
        "    # Phase n°2 d'encodage\n",
        "    # Création des représentations cachées des\n",
        "    # concaténations précédentes\n",
        "    hid = []\n",
        "    hid_state = None\n",
        "    cell_state = None\n",
        "    for i in range(input.shape[1]):\n",
        "      hid_state, cell_state = self.encodeur_phase2(Z,hid_state,cell_state,i)\n",
        "      hid.append(hid_state)\n",
        "    hid = tf.convert_to_tensor(hid)               # (Tin,batch_size,#LSTM)\n",
        "    hid = tf.transpose(hid,perm=[1,0,2])          # (batch_size,Tin,#LSTM)\n",
        "\n",
        "\n",
        "    # Phase de décodage\n",
        "    # Récupère les états cachés à (T-1)\n",
        "    hid_ = None\n",
        "    cell_ = None\n",
        "    for i in range(0,output_seq.shape[1]-1):\n",
        "      hid_, cell_, vc = self.decodeur(hid,output_seq[:,i:i+1,:],hid_,cell_)\n",
        "    \n",
        "    # hid_  : hT-1    : hidden state à t=T-1\n",
        "    # cell_ : sT-1    : cell state à t=T-1\n",
        "    # vc    : CT-1    : vecteur contexte à t=T-1\n",
        "    \n",
        "    # Estimation des sorties\n",
        "    # hid_ : (batch_size,#LSTM)\n",
        "    # vc   : (batch_size,1,#LSTM)\n",
        "    Y = []\n",
        "    y = tf.expand_dims(output_seq[:,-1,:],-1)        # y = YT : (batch_size,1,1)\n",
        "\n",
        "    y_pred_1 = tf.matmul(input[:,0:1,0:1],tf.zeros(shape=(1,1)))             # (batch_size,1,1)x(1,1) = (batch_size,1,1)\n",
        "    y_pred_initial_1 = tf.matmul(input[:,0:1,0:1],tf.zeros(shape=(1,1)))     # (batch_size,1,1)x(1,1) = (batch_size,1,1)\n",
        "\n",
        "    y_pred_1 = tf.squeeze(y_pred_1,-1)                            # (batch_size,1)\n",
        "    y_pred_initial_1 = tf.squeeze(y_pred_initial_1,-1)            # (batch_size,1)\n",
        "\n",
        "\n",
        "    for i in range(0,self.longueur_sortie):\n",
        "      # Prédiction DSTPII\n",
        "      hid_, cell_, vc = self.decodeur(hid,y,hid_,cell_)\n",
        "      add = tf.keras.layers.concatenate([tf.expand_dims(hid_,1),vc],axis=2)         # (batch_size,1,2x#LSTM)\n",
        "      add = tf.transpose(add,perm=[0,2,1])                                          # (batch_size,2x#LSTM,1)\n",
        "      y_dstpii = tf.matmul(self.Wy[i,:,:],add)                                      # (#LSTM,2x#LSTM) x (batch_size,2x#LSTM+1,1) = (batch_size,#LSTM,1)\n",
        "      y_dstpii = y_dstpii + self.by[i,:,:]                                            # (batch_size,#LSTM,1)\n",
        "      y_dstpii = tf.matmul(tf.transpose(self.vy[i,:,:]),y_dstpii)                     # (1,#LSTM)x(batch_size,#LSTM,1) = (batch_size,1,1)\n",
        "      y_dstpii = tf.squeeze(y_dstpii,-1)                                              # (batch_size,1)\n",
        "\n",
        "      # Prédiction VAR\n",
        "      y_var = self.pred_VAR(input,output_seq,y_pred_1,i)                                                       # (batch_size,1)\n",
        "\n",
        "      # Prédiction initiale\n",
        "      pred_initial = self.Couche_DenseI(tf.keras.layers.Concatenate(axis=1)([y_dstpii,y_var]))        # (batch_size,1)\n",
        "\n",
        "      # Calcul de l'erreur avec le PID à partir des prédictions initiales et globales à (t-1)\n",
        "      erreur = self.correcteur_PID(y_pred_initial_1,y_pred_1)                                        # (batch_size,1)\n",
        "\n",
        "      # Applique l'erreur à la prédiction initiale\n",
        "      y_pred = pred_initial + erreur\n",
        "\n",
        "      # Sauvegarde la prédiction\n",
        "      Y.append(y_pred)\n",
        "\n",
        "      # Met à jour les prédictions à (t-1)\n",
        "      y_pred_initial_1 = pred_initial\n",
        "      y_pred_1 = y_pred\n",
        "\n",
        "      y = tf.expand_dims(y_pred,-1)\n",
        "\n",
        "    Y = tf.convert_to_tensor(Y)               # (longueur_sortie,batch_size,1)\n",
        "    Y = tf.transpose(Y,perm=[1,0,2])          # (batch_size,longueur_sortie,1)\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh5MO6QCZbfv"
      },
      "source": [
        "#Création du modèle global"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKu1vn97Zbfv"
      },
      "source": [
        "dim_LSTM = 128\n",
        "drop=0.0\n",
        "l2reg=0.0\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,x_train[0].shape[2]),batch_size=batch_size)\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(longueur_sequence,1),batch_size=batch_size)\n",
        "\n",
        "  encodeur_P1_I = Encodeur_Phase1(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  encodeur_P1_II = Encodeur_Phase1(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  encodeur_P2 = Encodeur_Phase2(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  pred_VAR=VAR(longueur_sortie=longueur_sortie)\n",
        "  correcteur = Correcteur_PID(batch_size=batch_size,couche=\"tanh\")\n",
        "\n",
        "  sortie = Net_DSTPRNNII_VARPID(encodeur_P1_I,encodeur_P1_II,encodeur_P2,decodeur,pred_VAR=pred_VAR,correcteur_PID=correcteur, longueur_sequence=longueur_sequence,longueur_sortie=longueur_sortie, dim_LSTM=dim_LSTM,regul=l2reg,drop=drop)(entrees_sequences,sorties_sequence)\n",
        "\n",
        "  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtbX0QyB8Rvr"
      },
      "source": [
        "# Entrainement TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cNt57Q8RIT"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 500\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.01)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur)\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train[0],x_train[1]],y=y_train,validation_data=([x_val[0],x_val[1]],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCzeyy-0BLrR"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjE55tXaBUhA"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05bqnRr-pyf"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Models/Multi_HRHN_VAR_PID_SML2010_MultiStep.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG3JaSuZ09Lz"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  model = get_model()\n",
        "  model.compile(loss=\"mse\",metrics=\"mse\")\n",
        "  model.fit(x=[x_train[0],x_train[1]],y=y_train, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"Multi_HRHN_VAR_PID_SML2010_MultiStep.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pfsHSXY6nDo"
      },
      "source": [
        "# Prédictions multi-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkKpCbAT6nD-"
      },
      "source": [
        "pred_ent = model.predict(dataset,verbose=1)\n",
        "pred_val = model.predict(dataset_val,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChVFPdSq79Bf"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Affiche les courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=tf.squeeze(serie_entrainement_X_norm[:,-1:],-1),line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=tf.squeeze(serie_test_X_norm[:,-1:],-1),line=dict(color='red', width=1)))\n",
        "\n",
        "#Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "#max = 10\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[longueur_sequence+i*longueur_sortie:longueur_sequence+(i+1)*longueur_sortie])\n",
        "  step_val.append(pred_ent[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[longueur_sequence+i*longueur_sortie])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pred_index,y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "#max = 10\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[temps_separation+i*decalage+longueur_sequence:temps_separation+i*decalage+longueur_sequence+longueur_sortie])\n",
        "  step_val.append(pred_val[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[temps_separation+i*decalage+longueur_sequence])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pred_index,y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "# Affiche les prédictions\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wCeoEnMEYv"
      },
      "source": [
        "**Erreurs en multi step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2_ceD69Mju6"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "#Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_ent = tf.keras.losses.mse(serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV5JTpnnNApH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_test = tf.keras.losses.mse(serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}