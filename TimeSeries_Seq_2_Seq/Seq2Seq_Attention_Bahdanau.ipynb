{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq_Attention_Bahdanau.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/TimeSeries_Seq_2_Seq/Seq2Seq_Attention_Bahdanau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-pPkMQp1JI"
      },
      "source": [
        "Dans ce carnet nous allons mettre en place une attention de type Bahdanau dans notre modèle de prédiction de séries temporelles Séquence vers Séquence.  \n",
        "Le papier de recherche sur lequel s'appuie ce modèle est disponible ici : [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Data/Power_PV.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_data = pd.read_csv(\"Power_PV.csv\")\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU5e7LEN5o23"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdlO2gPJ9B0t"
      },
      "source": [
        "Converison des types `object` en `float32` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbdm1hcH53tO"
      },
      "source": [
        "df_data.iloc[:,1:] = pd.DataFrame.replace(df_data.iloc[:,1:],\"?\",\"NaN\")\n",
        "df_data.iloc[:,1:] = df_data.iloc[:,1:].astype(np.float32)\n",
        "df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgPxFdsd5Ybv"
      },
      "source": [
        "df_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ShrX00CM5z"
      },
      "source": [
        "# Définition des dates de début et de fin\n",
        "\n",
        "date_debut = \"2018-07-01 00:00:00\"\n",
        "date_fin = \"2019-04-14 00:00:00\"\n",
        "\n",
        "# Place l'index du dataframe sur la colonne Date\n",
        "df_data = df_data.rename(columns={'Unnamed: 0': \"Date\"})\n",
        "df_data = df_data.set_index(df_data['Date'])\n",
        "\n",
        "# Copie des données dans le dataframe d'étude sur l'intervalle d'étude\n",
        "df_etude = df_data.loc[date_debut:date_fin].copy()\n",
        "\n",
        "# Conversion de la colonne Date au format datetime\n",
        "df_etude.index = pd.to_datetime(df_etude.index)\n",
        "\n",
        "# Suppression de la colonne Unnamed:0\n",
        "df_etude = df_etude.drop(\"Date\", axis=1)\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oOlwpLd9NdW"
      },
      "source": [
        "Vérification des données et correction des anomalies :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZGgjlBr8qMH"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXje2Zaill9l"
      },
      "source": [
        "df_etude = df_etude.fillna(method=\"backfill\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWNilOlvmChW"
      },
      "source": [
        "data_manquantes = sum(np.isnan(df_etude['watts']))\n",
        "print (\"Données manquantes : %s\" %data_manquantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=df_etude['watts'], line=dict(color='blue', width=1),name=\"Puissance (W)\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDYEK-cxE0C"
      },
      "source": [
        "# Analyse de la série"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCbjPSNxWXy"
      },
      "source": [
        "**1. ACF & PACF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CleN4htOxYqZ"
      },
      "source": [
        "# ACF & PACF du bruit blanc\n",
        "\n",
        "serie = df_etude['watts'].values\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "f1.subplots_adjust(hspace=0.3,wspace=0.2)\n",
        "\n",
        "plot_acf(serie, ax=ax1, lags = range(0,500))\n",
        "ax1.set_title(\"Autocorrélation\")\n",
        "\n",
        "plot_pacf(serie, ax=ax2, lags = range(0, 500))\n",
        "ax2.set_title(\"Autocorrélation partielle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLPlWzLJlNhA"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02JYhFxflROo"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude['watts'].values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude['watts'].values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude['watts'].values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6HRdVZmRM6"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JhXRi2DnAY"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsOFDQV_mVkf"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "serie_entrainement_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_entrainement_X,shape=(len(serie_entrainement_X),1)))\n",
        "serie_test_X_norm = min_max_scaler.fit_transform(tf.reshape(serie_test_X,shape=(len(serie_test_X),1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44PYyswAlXFI"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm, label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm, label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "\n",
        "def prepare_dataset_XY(series, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  dataset = dataset.map(lambda x: (x[0:longueur_sequence][:,:],tf.expand_dims(x[-longueur_sortie:][:,0],1)))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128\n",
        "longueur_sequence = 5*24*4      # 5 jours (288*15min)\n",
        "longueur_sortie = 12*4          # 12h\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm,longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0].shape)\n",
        "  print(element[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hppDt9HnNQU"
      },
      "source": [
        "On extrait maintenant les deux tenseurs (X,Y) pour l'entrainement :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWR9V0WnOyR"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # #60x((32,5,779),(32,5,1)) => x = 60x(32,5,779) ; y = 60x(32,5,1)\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)      # 60x(32,5,779) => (60,32,5,779)\n",
        "y = np.asarray(y,dtype=np.float32)      # 60x(32,5,1) => (60,32,5,1)\n",
        "\n",
        "x_train = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))     # (60,32,5,779) => (60*32,5,779)\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))     # (60,32,5,1) => (60*32,5,1)\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUAqFIpmnY8Y"
      },
      "source": [
        "Puis la même chose pour les données de validation :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))\n",
        "\n",
        "# Recombine les données\n",
        "x = np.asarray(x,dtype=np.float32)\n",
        "y = np.asarray(y,dtype=np.float32)\n",
        "\n",
        "x_val = np.asarray(tf.reshape(x,shape=(x.shape[0]*x.shape[1],longueur_sequence,x.shape[3])))\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9aPVTcDn6TQ"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "for i in range(0,3):\n",
        "  ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence),x_train[i],label=\"X_train\")\n",
        "  ax.plot(np.linspace(longueur_sequence,longueur_sequence+longueur_sortie-1,longueur_sortie),y_train[i],label=\"Y_train\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7dkj0dK2S7P"
      },
      "source": [
        "# Codage des couches du modèle Seq2Seq avec attention de Bahadanau (avec encodeur simple GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqeBGpRB2S7r"
      },
      "source": [
        "Notre modèle Séquence vers Séquence est composé :\n",
        "- D'un encodeur, qui comprend une couche GRU uni-directionnelle\n",
        "- D'un décodeur, qui comprend une couche GRU uni-directionnelle.\n",
        "- D'un système d'attention de type Bahdanau dont le but est de décider quelle est la partie de la source la plus importante à chaque étape du décodage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqrQ2THM2S7s"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_GRU_2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK9B2ezPasjh"
      },
      "source": [
        "**1. Création de la couche d'attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzt2zFcVawlg"
      },
      "source": [
        "L'attention est calculée à chaque étape du décodage. Le but est de décider quelles sont les parties les plus importante dans la source.  \n",
        "Pour cela, à chaque étape du décodage le calcul d'attention est effectué de la manière suivante :\n",
        "- L'attention reçoit l'ensemble des états cachés de l'encodeur Enc_out (batch_size, Tin, GRU) ainsi que l'état caché du décodeur à l'étape précédente Hid_state (batch_size,GRU)\n",
        "- A partir de ces deux informations, elle calcule un score d'attention alpha. Ce score représente l'importance de l'état caché issu du décodeur à l'étape de décodage courante. Dans la méthode de Bahdanau, ce score est calculé en appliquant une couche dense sur la concaténation de ces deux informations, avec une fonction d'activation de type tanh.\n",
        "- A partir des scores d'attention, elle calcule les poids d'attention à attribuer à chaque état caché de l'encodeur (Enc_out#1, Enc_out#2, ...). Ces poids représentent des probabilités et sont obtenus en appliquant une fonction de type Softmax sur les scores.\n",
        "- Enfin, elle applique les poids d'attention sur les états cachés de l'encodeur en effectuant la somme pondérée de ces états cachés avec les poids.\n",
        "- Le résultat obtenu est le vecteur contexte C qui sera ensuite concaténé avec l'entrée et injecté dans la cellule GRU de décodage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfhrXQJ6Fo-"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/CalculAttention_Bahdanau2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOiyFNN8sq56"
      },
      "source": [
        "# Classe d'attention de Bahdanau\n",
        "\n",
        "# Importe le Backend de Keras\n",
        "from keras import backend as K\n",
        "\n",
        "# Définit une nouvelle classe Couche_Attention\n",
        "# Héritée de la classe Layer de Keras\n",
        "\n",
        "class Couche_Attention(tf.keras.layers.Layer):\n",
        "  # Fonction d'initialisation de la classe d'attention\n",
        "  def __init__(self,dim_att):\n",
        "    self.dim_att = dim_att          # Dimension du vecteur d'attention\n",
        "    super().__init__()              # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.W1 = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W1\")   # (#Att, #GRU)\n",
        "    self.W2 = self.add_weight(shape=(self.dim_att,input_shape[2]),initializer=\"normal\",name=\"W2\")   # (#Att, #GRU)\n",
        "    self.va = self.add_weight(shape=(self.dim_att,1),initializer=\"normal\",name=\"va\")                # (#Att, 1)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Définit la logique de la couche d'attention\n",
        "  # Entrées : enc_out : Etats cachés de l'encodeur (batch_size, Tin, #GRU)\n",
        "  #           hid_dec : Etat caché du décodeur (batch_size, #GRU)\n",
        "  # Sortie :  vc :      Vecteur contexte (batch_size,#GRU) \n",
        "  def call(self,enc_out,hid_dec):\n",
        "    # Calcul de Xh contenant les\n",
        "    # représentations cachées des vecteurs\n",
        "    # issus de l'encodeur\n",
        "    xt = tf.transpose(enc_out,perm=[0,2,1])     # (batch_size,Tin,#GRU) => (batch_size,#GRU,Tin)\n",
        "    Xh = tf.matmul(self.W1,xt)                  # (#Att,#GRU)x(batch_size,#GRU,Tin) = (batch_size,#Att,Tin)\n",
        "\n",
        "    # Calcul de la représentation cachée S\n",
        "    # du vecteur caché issu du décodeur\n",
        "    hid_dec = tf.expand_dims(hid_dec,-1)      # (batch_size,#GRU) => (batch_size,#GRU,1)\n",
        "    S = tf.matmul(self.W2,hid_dec)            # (#Att,#GRU)x(batch_size,#GRU,1) = (batch_size,#Att,1)\n",
        "\n",
        "    # Addition terme à terme des représentations\n",
        "    # cachées contenues dans les matarices Xh et\n",
        "    # de la représentation cachées contenue dans\n",
        "    # le vecteur S\n",
        "    Yh = tf.add(Xh,S)                         # (batch_size,#Att,Tin) _+_ (batch_size,#Att,1) = (batch_size,#Att,Tin)\n",
        "    Yh = K.tanh(Yh)                           # Yh = (batch_size,#Att,Tin)\n",
        "\n",
        "    # Calcul des poids d'attention normalisés\n",
        "    Yh = tf.transpose(Yh,perm=[0,2,1])        # Yh = (batch_size,#Att,Tin) => (batch_size,Tin,#Att)\n",
        "    a = tf.matmul(Yh,self.va)                 # (batch_size,Tin,#Att)x(#Att,1) = (batch_size,Tin,1)\n",
        "    a = tf.keras.activations.softmax(a,axis=1)\n",
        "\n",
        "    # Calcul du vecteur contexte\n",
        "    xa = tf.multiply(enc_out,a)               # (batch_size,Tin,#GRU)_x_(batch_size,Tin,1) = (batch_size,Tin,#GRU)\n",
        "    vc = K.sum(xa,axis=1)                     # vc = (batch_size,#GRU)\n",
        "    return vc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqqmIcUg2S7s"
      },
      "source": [
        "**2. Création de la couche d'encodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ8pgIJ4YE1d"
      },
      "source": [
        "Dans le document de recherche de Bahdanau, l'encodeur utilisé est de type RNN bi-directionnel. Ici on utilise un encodeur GRU unidurectionnel.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFpxmR0mxSq"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Encodeur_Bahdanau.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCPfeJq22S7s"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=True,return_state=True,dropout=self.drop,name=\"GRU_Encodeur\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée encodeur       : (batch_size,Tin,1) \n",
        "  # Sorties :\n",
        "  #     out_enc : Sortie encodeur       : (batch_size,Tin,#GRU)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,#GRU)\n",
        "  def call(self, input):\n",
        "    out_enc, out_hid = self.couche_GRU(input)\n",
        "    return out_enc, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__rii3mc2S7t"
      },
      "source": [
        "**3. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-jGzrstYSd0"
      },
      "source": [
        "Dans le document de recherche de Bahdanau, le décodeur utilise un réseau RNN unidirectionnel agrémenté de deux matrices de poids pour prendre en compte la combinaison linéaire du vecteur contexte et de l'état caché. Ici, on utilise un réseau de type GRU unidirectionnel et on réalise cette combinaison linéaire à l'extérieur du réseau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Pey2RHm5FE"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Decodeur_Bahdanau_2.png?raw=true' width=900/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnMiDjg12S7t"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_GRU, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_GRU = tf.keras.layers.GRU(self.dim_GRU,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"GRU_Decodeur\")\n",
        "    self.couche_Dense = tf.keras.layers.Dense(units=1,kernel_regularizer=tf.keras.regularizers.l2(self.regul),input_dim=self.dim_GRU)\n",
        "    self.Wc = tf.keras.layers.Dense(units=self.dim_GRU,activation=\"tanh\",use_bias=False)\n",
        "    self.Attention = Couche_Attention(self.dim_GRU)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     dec_input:    Entrée décodeur       : (batch_size,1,1) \n",
        "  #     enc_out:      Sortie encodeur       : (batch_size,Tin,#GRU)\n",
        "  #     hidden:       Vecteur caché         : (batch_size,#GRU)\n",
        "  # Sorties :\n",
        "  #     out_dec : Sortie décodeur       : (batch_size,1)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,#GRU)\n",
        "  def call(self,dec_input, enc_out,hidden):\n",
        "    # Calcul du vecteur contexte\n",
        "    vect_contexte = self.Attention(enc_out,hidden)          # (batch_size,#GRU)\n",
        "\n",
        "    # Concaténation du vecteur contexte\n",
        "    # et de l'entrée du décodeur\n",
        "\n",
        "    vect_contexte = tf.expand_dims(vect_contexte,1)          # (batch_size,#GRU) => (batch_size,1,#GRU)\n",
        "    add = tf.concat([vect_contexte,dec_input],axis=2)        # Concat {(batch_size,1,#GRU),(batch_size,1,1)} = ((batch_size,1,#GRU+1))\n",
        "\n",
        "    # Calcul de la sortie\n",
        "    out_dec, out_hid = self.couche_GRU(add,initial_state=hidden)\n",
        "    out_dec = self.couche_Dense(out_dec)\n",
        "    return out_dec, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSspq38H2S7u"
      },
      "source": [
        "**4. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R20vrAwPY7Wc"
      },
      "source": [
        "Dans le document de recherche, l'état caché initial du décodeur est calculé à partir du premier état caché (sens retour) de l'encodeur. Ici, on calcule cet état initial en utlilisant le dernier état caché de l'encodeur. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Jd51Jhnc97"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Attention_Bahdanau_GRU_2.png?raw=true' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ9tC3nJ2S7u"
      },
      "source": [
        "class Net_GRU(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    self.dim_GRU = dim_GRU\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Ws = tf.keras.layers.Dense(units=self.dim_GRU,activation=\"tanh\",use_bias=False)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size, Tin, 1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    # Encodage de la séquence d'entrée\n",
        "    enc_out, enc_hid = self.encodeur(input)       # (batch_size,Tin,#GRU), (batch_size,#GRU)\n",
        "\n",
        "    # Calcul de l'état caché initial du décodeur\n",
        "    dec_hid = self.Ws(enc_hid)                    # s0 : (batch_size,#GRU)\n",
        "\n",
        "    # La première entrée du décodeur est la\n",
        "    # dernière entrée de l'encodeur\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)   # Y0 : (batch_size,1,1)\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, enc_out, dec_hid)\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, enc_out, dec_hid)\n",
        "        dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_2pjIwEc9TH"
      },
      "source": [
        "# Codage des couches du modèle Seq2Seq avec attention de Bahadanau (avec encodeur LSTM-Bidirectionnel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6y_St_0RHmq"
      },
      "source": [
        "Notre modèle Séquence vers Séquence est composé :\n",
        "- D'un encodeur, qui comprend une couche LSTM bi-directionnelle\n",
        "- D'un décodeur, qui comprend une couche LSTM uni-directionnelle. Une couche denses est utilisée pour recréer la sortie univariée à partir des sorties des cellules LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uheOjyuOQ-rg"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_basique_LSTM.png?raw=true' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8J4fmfWc9TV"
      },
      "source": [
        "**1. Création de la couche d'encodeur à base de LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AOxn0upx3-H"
      },
      "source": [
        "La couche Bidirectionnelle appliquée à une couche LSTM retourne 5 tenseurs :\n",
        "- La concaténation (ou la somme, en fonction de la méthode `merge` choisie) des états cachés avance / recul\n",
        "- Le dernier état caché du LSM d'avance : (batch_size, dim_LSTM*2) si concaténation ou (batch_size,dim_LSTM) si merge\n",
        "- Le derner cell-state du LSTM d'avance : (batch_size, dim_LSTM)\n",
        "- Le dernier état caché du LSTM de recul : (batch_size, dim_LSTM)\n",
        "- Le dernier cell-state du LSTM de recul : (batch_size, dim_LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqSeRbS7c9TW"
      },
      "source": [
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"LSTM_Encodeur\"),merge_mode=\"sum\")\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée encodeur       : (batch_size,1,1) \n",
        "  #     hidden:   Vecteur caché         : (batch_size,128)\n",
        "  # Sorties :\n",
        "  #     out_enc :   Sortie encodeur       : (batch_size,128)\n",
        "  #     out_hid_x : Sortie vecteur caché  : (batch_size,128)\n",
        "  #     state_c_x : Sortie cell state     : (btach_size,128)\n",
        "  def call(self, input, hidden=None):\n",
        "    out_enc, out_hid_1, state_c_1, out_hid_2, state_c_2 = self.couche_LSTM(input,initial_state=hidden)\n",
        "    return out_enc, out_hid_1, state_c_1, out_hid_2, state_c_2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBcCalqUc9TZ"
      },
      "source": [
        "**2. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb6xzfpjc9Ta"
      },
      "source": [
        "class Decodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self,dim_LSTM, regul=0.0, drop=0.0):\n",
        "    self.regul = regul\n",
        "    self.dim_LSTM = dim_LSTM            # Dimension des vecteurs cachés\n",
        "    self.drop = drop\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.couche_LSTM = tf.keras.layers.LSTM(self.dim_LSTM,kernel_regularizer=tf.keras.regularizers.l2(self.regul),return_sequences=False,return_state=True,dropout=self.drop,name=\"LSTM_Decodeur\")\n",
        "    self.couche_Dense = tf.keras.layers.Dense(units=1,kernel_regularizer=tf.keras.regularizers.l2(self.regul))\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:    Entrée décodeur       : (batch_size,1,1) \n",
        "  #     hidden:   Vecteur caché         : (batch_size,128)\n",
        "  # Sorties :\n",
        "  #     out_dec : Sortie décodeur       : (batch_size,1,1)\n",
        "  #     out_hid : Sortie vecteur caché  : (batch_size,128)\n",
        "  def call(self,input,hidden):\n",
        "    out_dec, out_hid, state_c = self.couche_LSTM(input,initial_state=hidden)\n",
        "    out_dec = self.couche_Dense(out_dec)\n",
        "    return out_dec, out_hid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7AGHa66c9Tc"
      },
      "source": [
        "**3. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-xaHB1vc9Tc"
      },
      "source": [
        "class Net_LSTM(tf.keras.layers.Layer):\n",
        "  def __init__(self,encodeur,decodeur,longueur_sequence, longueur_sortie, regul=0.0):\n",
        "    self.encodeur = encodeur\n",
        "    self.decodeur = decodeur\n",
        "    self.longueur_sequence = longueur_sequence\n",
        "    self.longueur_sortie = longueur_sortie\n",
        "    self.regul = regul\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:      Entrée réseau         : (batch_size,longueur_sequence,1) \n",
        "  #     output_seq  Sorties réelles       : (batch_size, longueur_sortie,1)\n",
        "  # Sorties :\n",
        "  def call(self,input,output_seq,training=False):\n",
        "    longueur_entree = input.shape[1]\n",
        "    sortie = []\n",
        "\n",
        "    enc_out, enc_hid_1, enc_state_1, enc_hid_2, enc_state_2 = self.encodeur(input[:,0:1,:])\n",
        "\n",
        "    for i in range(1,longueur_entree):\n",
        "      enc_out, enc_hid_1, enc_state_1, enc_hid_2, enc_state_2 = self.encodeur(input[:,i:i+1,:],[enc_hid_1, enc_state_1, enc_hid_2, enc_state_2])\n",
        "\n",
        "    dec_input = tf.expand_dims(input[:,-1,:],1)\n",
        "    dec_hid = tf.keras.layers.Add()((enc_hid_1,enc_hid_2))\n",
        "    dec_state = tf.keras.layers.Add()((enc_state_1,enc_state_2))\n",
        "\n",
        "    if (training == False):\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, [dec_hid,dec_state])\n",
        "        dec_input = tf.expand_dims(dec_out,-1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "    else:\n",
        "      for i in range(0,self.longueur_sortie):\n",
        "        dec_out, dec_hid = self.decodeur(dec_input, [dec_hid,dec_state])\n",
        "        dec_input = tf.expand_dims(output_seq[:,i,:],1)\n",
        "        sortie.append(dec_out)\n",
        "      sortie = tf.convert_to_tensor(tf.transpose(sortie, perm=[1,0,2]))\n",
        "\n",
        "    return sortie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eo-lt6tc9Te"
      },
      "source": [
        "# Création du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdTK4wAQ1O4v"
      },
      "source": [
        "Notre modèle doit avoir deux comportements différents suivant qu'il soit en entrainement ou en prédiction. Pour différencier ce que réalisent ces deux modes, nous devons écrire nos propres fonctions d'entrainement et de test.  \n",
        "\n",
        "Pour faire cela, nous allons dériver la classe `CustomModel` qui permet d'implanter ses propres fonctions :\n",
        "- train_step : Définit la fonction d'entrainement.\n",
        "- test_step : Définit la fonction de test.  \n",
        "  \n",
        "Lors de la phase d'entrainement, l'optimiseur doit appliquer l'algorithme de rétropropagation du gradient sur la fonction d'erreur. Il doit donc calculer les dérivées partielles de l'erreur par rapport aux différents poids du modèle. Ce calcul est effectué par la ligne `self.optimizer.apply_gradients(zip(gradients, trainable_vars))`  \n",
        "\n",
        "L'optimisaeur a donc besoin des gradients de l'erreur. C'est pendant l'application de la fonction d'erreur aux prédictions que ces gradients sont calculés et sauvegardés par différentiation automatique. La ligne qui réalise cela est la ligne : `loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)`\n",
        "\n",
        "Ce calcul est réalisé sous la surveillance de la fonction `GradientTape` qui permet de sauvegarder les résultats du gradients, afin qu'ils soient utilisés par l'optimiseur.  \n",
        "\n",
        "Dans la phase de test, l'optimisation n'est pas utilisée. Les gradients n'ont donc pas besoin d'être sauvegardés et donc le calcul de l'erreur n'est pas appelé sous la surveillance de GradientTape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoMrc-_pZ2WE"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Calcul du gradient\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Optimisation des poids\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Calcul des predictions\n",
        "        y_pred = self(x, training=False)\n",
        "\n",
        "        # Mise à jour des erreurs\n",
        "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Mise à jour des métriques\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "\n",
        "        # Retourne un dictionnaire avec les résultats\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozgilOOUbJmb"
      },
      "source": [
        "dim_GRU = 128\n",
        "drop=0.2\n",
        "l2reg=0.01\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,1))\n",
        "  sorties_sequences = tf.keras.layers.Input(shape=(longueur_sortie,1))\n",
        "\n",
        "\n",
        "  encodeur = Encodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_GRU=dim_GRU,drop=drop,regul=l2reg)\n",
        "\n",
        "  sortie = Net_GRU(encodeur,decodeur,longueur_sequence=longueur_sequence, longueur_sortie=longueur_sortie)(entrees_sequences,sorties_sequences)\n",
        "\n",
        "  model = CustomModel([entrees_sequences,sorties_sequences],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vN_WrGytPT8"
      },
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  model = get_model()\n",
        "\n",
        "  # Définition de la fonction de régulation du taux d'apprentissage\n",
        "  def RegulationTauxApprentissage(periode, taux):\n",
        "    return 1e-8*10**(periode/10)\n",
        "\n",
        "  # Définition de l'optimiseur à utiliser\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=1e-8)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "  historique = model.fit(x=[x_train,y_train],y=y_train,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint],batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w7sm7aytdWz"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 0.2])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "  max_periodes = 1000\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.001)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train,y_train],y=y_train,validation_data=([x_val,y_val],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfFTzSYfyIf"
      },
      "source": [
        "model.evaluate(x=[x_train,y_train],y=y_train)\n",
        "model.evaluate(x=[x_val,y_val],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIBUbI1TICU"
      },
      "source": [
        "**1. Seq2Seq Bahdanau - Encodeur unidirectionnel GRU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOahfnJ-yX1"
      },
      "source": [
        "**Seq2Seq_Bahdanau_GRU128_DR02_L2_001_BS128_EP1000_5J_12H :**\n",
        "- Encodeur unidirectionnel GRU #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 24h\n",
        "- Drop : 0.2\n",
        "- L2 : 0.01\n",
        "- Batch Size : 128\n",
        "- Périodes : 1000  \n",
        "=> mse : 0.0171 / 0.0239  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Train_TPU_Seq2SeqBahdanau1.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XKdrbK-022"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flEANw1UxQfg"
      },
      "source": [
        "!rm *.hdf5\n",
        "!wget --no-check-certificate --content-disposition https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Bahadanau_GRU128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Bahadanau_GRU128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci9qh0TdTPD2"
      },
      "source": [
        "**2. Seq2Seq Basique - Encodeur LSTM Bidirectionnel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckazHYvhTfeF"
      },
      "source": [
        "**Seq2Seq_Basique_LSTMBiDir128_DR02_L2_001_BS128_EP1000_5J_12H :**\n",
        "- Encodeur Bi-directionnel LSTM #128\n",
        "- Longueur entrée : 5 jours\n",
        "- Longueur sortie : 24h\n",
        "- Drop : 0.2\n",
        "- L2 : 0.01\n",
        "- Batch Size : 128\n",
        "- Périodes : 1000  \n",
        "=> mse : 0.0072 / 0.0131\n",
        "  \n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Seq2Seq/images/Seq2Seq_Basique_LSTMBiDir.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tLGoCvbbUlc"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--9c2OEwbZhP"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Seq2Seq/Modeles/Seq2Seq_Basique_LSTMBiDir128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46rSxuZbfEO"
      },
      "source": [
        "model.load_weights(\"Seq2Seq_Basique_LSTMBiDir128_DR02_L2_001_BS128_EP1000_5J_12H.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions multi-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train,y_train],verbose=1)\n",
        "pred_val = model.predict([x_val,y_val],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6JOfhb5sukN"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = longueur_sortie\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Affiche les courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "# Calcul les prédictions sur l'entrainement\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "\n",
        "max = int(len(pred_ent)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[longueur_sequence+i*longueur_sortie:longueur_sequence+(i+1)*longueur_sortie])\n",
        "  step_val.append(pred_ent[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[longueur_sequence+i*longueur_sortie])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "\n",
        "#Calcul les prédictions sur les validations\n",
        "pred = []\n",
        "pred_index = []\n",
        "step_time = []\n",
        "step_val = []\n",
        "max = int(len(pred_val)/longueur_sortie)\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i*longueur_sortie,0:decalage,:],1))\n",
        "  pred_index.append(df_etude.index[temps_separation+i*decalage+longueur_sequence:temps_separation+i*decalage+longueur_sequence+longueur_sortie])\n",
        "  step_val.append(pred_val[i*longueur_sortie,0,0])\n",
        "  step_time.append(df_etude.index[temps_separation+i*decalage+longueur_sequence])\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "pred_index = np.asarray(pred_index)\n",
        "pred_index = np.reshape(pred_index,(pred_index.shape[0]*pred_index.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=pd.to_datetime(pred_index),y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "fig.add_trace(go.Scatter(x=step_time,y=step_val, mode='markers', line=dict(color='black', width=1)))\n",
        "\n",
        "# Affiche les prédictions\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az6iW5lAFZ34"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,0],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,0],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}