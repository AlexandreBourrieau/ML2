{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RHN_Test.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/TimeSeries_Multi/RHN_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeah3y_6kif"
      },
      "source": [
        "# Création de la série temporelle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJfSLtub6sdc"
      },
      "source": [
        "# Fonction permettant d'afficher une série temporelle\n",
        "def affiche_serie(temps, serie, format=\"-\", debut=0, fin=None, label=None):\n",
        "    plt.plot(temps[debut:fin], serie[debut:fin], format, label=label)\n",
        "    plt.xlabel(\"Temps\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14)\n",
        "    plt.grid(True)\n",
        "\n",
        "# Fonction permettant de créer une tendance\n",
        "def tendance(temps, pente=0):\n",
        "    return pente * temps\n",
        "\n",
        "# Fonction permettant de créer un motif\n",
        "def motif_periodique(instants):\n",
        "    return (np.where(instants < 0.4,                            # Si les instants sont < 0.4\n",
        "                    np.cos(instants * 2 * np.pi),               # Alors on retourne la fonction cos(2*pi*t)\n",
        "                    1 / np.exp(3 * instants)))                  # Sinon, on retourne la fonction exp(-3t)\n",
        "\n",
        "# Fonction permettant de créer une saisonnalité avec un motif\n",
        "def saisonnalite(temps, periode, amplitude=1, phase=0):\n",
        "    \"\"\"Répétition du motif sur la même période\"\"\"\n",
        "    instants = ((temps + phase) % periode) / periode            # Mapping du temps =[0 1 2 ... 1460] => instants = [0.0 ... 1.0]\n",
        "    return amplitude * motif_periodique(instants)\n",
        "\n",
        "# Fonction permettant de générer du bruit gaussien N(0,1)\n",
        "def bruit_blanc(temps, niveau_bruit=1, graine=None):\n",
        "    rnd = np.random.RandomState(graine)\n",
        "    return rnd.randn(len(temps)) * niveau_bruit\n",
        "\n",
        "# Création de la série temporelle\n",
        "temps = np.arange(4 * 365)                # temps = [0 1 2 .... 4*365] = [0 1 2 .... 1460]\n",
        "amplitude = 40                            # Amplitude de la la saisonnalité\n",
        "niveau_bruit = 5                          # Niveau du bruit\n",
        "offset = 10                               # Offset de la série\n",
        "\n",
        "serie = offset + tendance(temps, 0.1) + saisonnalite(temps, periode=365, amplitude=amplitude) + bruit_blanc(temps,niveau_bruit,graine=40)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie)\n",
        "plt.title('Série temporelle expérimentale')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOtbHUk7AYt"
      },
      "source": [
        "# Préparation des données X et Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78vEUlpIFMp6"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# au format X(X1,X2,...Xn) / Y(Y1,Y2,...,Yn)\n",
        "# X sont les données d'entrées du réseau\n",
        "# Y sont les labels\n",
        "\n",
        "def prepare_dataset_XY(serie, taille_fenetre, batch_size, buffer_melange):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(serie)\n",
        "  dataset = dataset.window(taille_fenetre+1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda x: x.batch(taille_fenetre + 1))\n",
        "  dataset = dataset.shuffle(buffer_melange).map(lambda x: (x[:-1], x[-1:]))\n",
        "  dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHgcL7ZPJlws"
      },
      "source": [
        "**1. Séparation des données en données pour l'entrainement et la validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ggmBUlAKWH2"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/Images/Series/illustration1.png?raw=true\" width=\"600\">  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_14USRg5JvSu"
      },
      "source": [
        "temps_separation = 1000\n",
        "\n",
        "# Extraction des temps et des données d'entrainement\n",
        "temps_entrainement = temps[:temps_separation]\n",
        "x_entrainement = serie[:temps_separation]\n",
        "\n",
        "# Exctraction des temps et des données de valiadation\n",
        "temps_validation = temps[temps_separation:]\n",
        "x_validation = serie[temps_separation:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjh4mjMrKhA5"
      },
      "source": [
        "**2. Préparation des données X et des labels Y**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdqfYabAKoE2"
      },
      "source": [
        "On commence par créer notre dataset à partir de la série (remarque : les valeurs ci-dessous sont en réalité mélangées) :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2CDLaYDoDms"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/split_XY_2.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziajfOefKvsu"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "taille_fenetre = 20\n",
        "batch_size = 32\n",
        "buffer_melange = 1000\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset = prepare_dataset_XY(x_entrainement,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val = prepare_dataset_XY(x_validation,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWJQ0ajCbXTY"
      },
      "source": [
        "**3. Normalisation des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0hJrSCFbRUw"
      },
      "source": [
        "# Calcul de la moyenne et de l'écart type de la série\n",
        "mean = tf.math.reduce_mean(serie)\n",
        "std = tf.math.reduce_std(serie)\n",
        "\n",
        "# Normalise les données\n",
        "Serie_Normalisee = (serie-mean)/std\n",
        "min = tf.math.reduce_min(serie)\n",
        "max = tf.math.reduce_max(serie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSdWLIqDbUYw"
      },
      "source": [
        "# Création des données pour l'entrainement et le test\n",
        "x_entrainement_norm = Serie_Normalisee[:temps_separation]\n",
        "x_validation_norm = Serie_Normalisee[temps_separation:]\n",
        "\n",
        "# Création du dataset X,Y\n",
        "dataset_norm = prepare_dataset_XY(x_entrainement_norm,taille_fenetre,batch_size,buffer_melange)\n",
        "\n",
        "# Création du dataset X,Y de validation\n",
        "dataset_Val_norm = prepare_dataset_XY(x_validation_norm,taille_fenetre,batch_size,buffer_melange)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlvrJJknAMew"
      },
      "source": [
        "# Création de la couche RHN (Recurrent Highway Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3_mIcrAS2I"
      },
      "source": [
        "Le modèle RHN (Recurrent Highway Network) implanté est le suivant. Le document de recherche associé est consultable ici : [Recurrent Highway Networks\n",
        "](https://arxiv.org/pdf/1607.03474)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CygD9DXbBTDJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Structure_RHN3.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RL27t0wLOat"
      },
      "source": [
        "**1. Création d'une cellule RHN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Y23K35LR9d"
      },
      "source": [
        "On commence par créer une cellule RHN (de taile L couches) qui sera appelée T fois (T est le nombre d'instants) par la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-ZlJIvlBdkk"
      },
      "source": [
        "class Cellule_RHN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_RHN, nbr_couches):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches = nbr_couches\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wh = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wh\")       # (#dim, #RHN)\n",
        "    self.Wt = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wt\")       # (#dim, #RHN)\n",
        "    self.Wc = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wc\")       # (#dim, #RHN)\n",
        "\n",
        "    self.Rh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rh\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rt\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rc\")      # (n_couches,#RHN, #RHN)\n",
        "\n",
        "    self.bh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bh\")        # (n_couches,#RHN, 1)\n",
        "    self.bt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bt\")        # (n_couches,#RHN, 1)\n",
        "    self.bc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bc\")        # (n_couches,#RHN, 1)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:          Entrées X[t]        : (batch_size,1,#dim)\n",
        "  #     init_hidden:    Etat caché Init.    : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     sL:             Etat caché SL[t]    : (batch_size,#RHN)\n",
        "  def call(self, input,init_hidden=None):\n",
        "    # Construction d'un vecteur d'état nul si besoin\n",
        "    if init_hidden == None:\n",
        "      init_hidden = tf.matmul(tf.zeros(shape=(self.dim_RHN,input.shape[2])), # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "                              tf.transpose(input,perm=[0,2,1]))\n",
        "      init_hidden = tf.squeeze(init_hidden,-1)                               # (batch_size,#RHN,1) => (batch_size,#RHN)\n",
        "    \n",
        "    # Calcul de hl, tl et cl\n",
        "    for i in range(self.nbr_couches):\n",
        "      # Calcul de hl\n",
        "      if i == 0:\n",
        "        hl = tf.matmul(self.Rh[i,:,:],tf.expand_dims(init_hidden,-1))        # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "      else:\n",
        "        hl = tf.matmul(self.Rh[i,:,:],tf.expand_dims(init_hidden,-1))        # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + tf.matmul(tf.transpose(self.Wh),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "      # Calcul de tl\n",
        "      if i == 0:\n",
        "        tl = tf.matmul(self.Rt[i,:,:],tf.expand_dims(init_hidden,-1))        # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = K.tanh(tl)\n",
        "      else:\n",
        "        tl = tf.matmul(self.Rt[i,:,:],tf.expand_dims(init_hidden,-1))        # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + tf.matmul(tf.transpose(self.Wt),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "      # Calcul de cl\n",
        "      if i == 0:\n",
        "        cl = tf.matmul(self.Rc[i,:,:],tf.expand_dims(init_hidden,-1))        # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = K.tanh(cl)\n",
        "      else:\n",
        "        cl = tf.matmul(self.Rc[i,:,:],tf.expand_dims(init_hidden,-1))        # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + tf.matmul(tf.transpose(self.Wc),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "      \n",
        "      # Calcul de sl\n",
        "      sl = tf.keras.layers.multiply([hl,tl])\n",
        "      sl = sl + tf.keras.layers.multiply([init_hidden,cl])\n",
        "      init_hidden = sl\n",
        "\n",
        "    return sl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpde_pa2ZnXc"
      },
      "source": [
        "**2. Création de la couche RHN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1hApKMCZ3pV"
      },
      "source": [
        "Cette couche prend en entrée un tenseur au format (batch_size,Tin,#RHN) puis retourne une sortie au format (batch_size,#RHN) si `return_sequence=False` ou au format (batch_size,Tin,#RHN) si `return_sequence=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmgTejiNZv-S"
      },
      "source": [
        "class Couche_RHN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_RHN, nbr_couches, return_sequence=False):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches = nbr_couches\n",
        "    self.return_sequence = return_sequence\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.RHN = Cellule_RHN(dim_RHN=self.dim_RHN,nbr_couches=self.nbr_couches)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:          Entrées X         : (batch_size,Tin,#dim)\n",
        "  #     init_hidden:    Etat caché Init.  : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     y:              Dernier Sl        : (batch_size,#RHN)\n",
        "  #            (ou)     Séquence entière  : (batch_size,Tin,#RHN)\n",
        "  def call(self, input, init_hidden=None):\n",
        "    sequence = []\n",
        "    hidden = init_hidden\n",
        "    for i in range(input.shape[1]):\n",
        "      hidden = self.RHN(input[:,i:i+1,:],hidden)      # Envoie (batch_size,1,#dim)\n",
        "      sequence.append(hidden)                         # Sauve (batch_size,#RHN)\n",
        "    \n",
        "    if self.return_sequence == False:\n",
        "      return hidden\n",
        "    else:\n",
        "      sequence = tf.convert_to_tensor(sequence)       # (Tin,batch_size,#RHN)\n",
        "      sequence = tf.transpose(sequence,perm=[1,0,2])  # (batch_size,Tin,#RHN)\n",
        "      return sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yt-EgZ3sgPY"
      },
      "source": [
        "# Création du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyrcfKcgCsZ7"
      },
      "source": [
        "**1. Création du réseau et adaptation des formats d'entrée et de sortie**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at-Z_8-L-aaL"
      },
      "source": [
        "Le réseau que nous allons mettre en place est un réseau de neurones récurrent, composé de :\n",
        "  - Une couche récurrente LSTM à 40 neurones, de type séquence vers vecteur (sequence to vector) (Encodeur)\n",
        "  - Une couche dense avec 40 neurones et fonction d'activation tanh (Décodeur)\n",
        "  - Une couche dense avec 1 neurone (Générateur)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7uyzTcI-4fc"
      },
      "source": [
        "Pour adapter les données en entrée et en sortie du réseau au format attendu, nous allons utiliser une [couche de type lambda](https://keras.io/api/layers/core_layers/lambda/) avec Keras. Ce type de couche nous permet d'intégrer une fonction spécifique en tant que couche dans notre réseau de neurone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7iwhon8AD_9"
      },
      "source": [
        "- En entrée de la première couche récurrente, le format attendu est de type : [batch_size, #instants, #dims].   \n",
        "Le **format d'entrée attendu est donc [None,None,1]** car notre série temporelle est de type univariée, qu'on souhaite pouvoir traiter une séquence infinie, et que le batch size est géré automatiquement par tensorflow.  \n",
        "Il faut donc en entrée transformer le format [taille_fenetre, 1] vers le format [None, None, 1].  \n",
        "\n",
        "- En sortie de la deuxième couche récurrente, le format est de type [batch_size, #instants, #neurones]. Il sera de type [None, None, 40].  \n",
        "- La couche dense possède un seul neurone. En **sortie de la couche dense, le format est de type [None, None, 1]**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQifKMDBvcy"
      },
      "source": [
        "Sous Keras, nous allons utiliser une couche récurrente de type LSTM avec la classe [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/). En sortie, la fonction d'activation par défaut est de type `tanh` qui sort une sortie comprise entre [-1,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeJyix8HK7Kt"
      },
      "source": [
        "Sous forme de shéma, notre réseau est donc le suivant :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZkfsmnBNHY"
      },
      "source": [
        "<img src=\"https://github.com/AlexandreBourrieau/ML/blob/main/Carnets%20Jupyter/S%C3%A9ries%20temporelles/images/LSTM_1111.png?raw=true\" width=\"1200\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86TUv-QxEFy9"
      },
      "source": [
        "Pour insérer une dimension de type `None` au format de l'entrée, on utilise la méthode [expand_dims](https://www.tensorflow.org/api_docs/python/tf/expand_dims) de tensorflow. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9KYLYtT7Qj2"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDTE4-i0YHGG"
      },
      "source": [
        "# Fonction de la couche lambda d'entrée\n",
        "def Traitement_Entrees(x):\n",
        "  return tf.expand_dims(x,axis=-1)\n",
        "\n",
        "# Définition de l'entrée du modèle\n",
        "entrees = tf.keras.layers.Input(shape=(taille_fenetre))\n",
        "\n",
        "# Encodeur\n",
        "e_adapt = tf.keras.layers.Lambda(Traitement_Entrees)(entrees)\n",
        "s_encodeur = Couche_RHN(dim_RHN=40,nbr_couches=1)(e_adapt)\n",
        "\n",
        "# Décodeur\n",
        "s_decodeur = tf.keras.layers.Dense(40,activation=\"tanh\")(s_encodeur)\n",
        "s_decodeur = tf.keras.layers.Concatenate()([s_decodeur,s_encodeur])\n",
        "\n",
        "# Générateur\n",
        "sortie = tf.keras.layers.Dense(1)(s_decodeur)\n",
        "\n",
        "# Construction du modèle\n",
        "model = tf.keras.Model(entrees,sortie)\n",
        "\n",
        "model.save_weights(\"model_initial.hdf5\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kjgIQKYeN0q"
      },
      "source": [
        "**2. Optimisation du taux d'apprentissage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aixSutOy_X-T"
      },
      "source": [
        "# Remise à zéro de tous les états générés par Keras\n",
        "tf.keras.backend.clear_session()\n",
        "model.load_weights(\"model_initial.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeECx41tT3bT"
      },
      "source": [
        "# Définition de la fonction de régulation du taux d'apprentissage\n",
        "def RegulationTauxApprentissage(periode, taux):\n",
        "  return 1e-8*10**(periode/10)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur, metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle en utilisant notre fonction personnelle de régulation du taux d'apprentissage\n",
        "historique = model.fit(dataset_norm,epochs=100,verbose=1, callbacks=[tf.keras.callbacks.LearningRateScheduler(RegulationTauxApprentissage), CheckPoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQsIpEOfUVrB"
      },
      "source": [
        "# Construit un vecteur avec les valeurs du taux d'apprentissage à chaque période \n",
        "taux = 1e-8*(10**(np.arange(100)/10))\n",
        "\n",
        "# Affiche l'erreur en fonction du taux d'apprentissage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(taux,historique.history[\"loss\"])\n",
        "plt.axis([ taux[0], taux[99], 0, 0.5])\n",
        "plt.title(\"Evolution de l'erreur en fonction du taux d'apprentissage\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXof-yChVHBZ"
      },
      "source": [
        "**3. Entrainement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9xIawiy4Se8"
      },
      "source": [
        "# Remise à zéro des états du modèle\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6TYJxqA4T4s"
      },
      "source": [
        "# Chargement des poids sauvegardés\n",
        "model.load_weights(\"poids.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNVIYw744shF"
      },
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.n_steps = 0\n",
        "        self.t_step = 0\n",
        "        self.n_batch = 0\n",
        "        self.total_batch = 0\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.t_step = self.t_step  + timer()-self.starttime\n",
        "        self.n_steps = self.n_steps + 1\n",
        "        if (self.total_batch == 0):\n",
        "          self.total_batch=self.n_batch - 1\n",
        "    def on_train_batch_begin(self,batch,logs=None):\n",
        "      self.n_batch= self.n_batch + 1\n",
        "    def GetInfos(self):\n",
        "      return([self.t_step/(self.n_steps*self.total_batch), self.t_step, self.total_batch])\n",
        "\n",
        "cb = TimingCallback()\n",
        "\n",
        "# Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=10,\n",
        "    decay_rate=0.01)\n",
        "\n",
        "# Définition de l'optimiseur à utiliser\n",
        "optimiseur=tf.keras.optimizers.SGD(learning_rate=lr_schedule,momentum=0.9)\n",
        "\n",
        "# Utilisation de la méthode ModelCheckPoint\n",
        "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_entrainement.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "# Compile le modèle\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=optimiseur,metrics=\"mae\")\n",
        "\n",
        "# Entraine le modèle\n",
        "historique = model.fit(dataset_norm,validation_data=dataset_Val_norm, epochs=500,verbose=1, callbacks=[CheckPoint,cb])\n",
        "\n",
        "# Affiche quelques informations sur les timings\n",
        "infos = cb.GetInfos()\n",
        "print(\"Step time : %.3f\" %infos[0])\n",
        "print(\"Total time : %.3f\" %infos[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YurVEewK76YB"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE0ARCgco5HJ"
      },
      "source": [
        "**5. Prédictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouStOYe4F_jF"
      },
      "source": [
        "Puisque le format d'entrée permet de prendre des séquences infinies, nous pouvons entrer des séquences de n'importe quelle taille :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV4QkHep-ZCf"
      },
      "source": [
        "X = np.reshape(serie[0:20],(1,20))\n",
        "model.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amfVL_Tcbaak"
      },
      "source": [
        "taille_fenetre = 20\n",
        "\n",
        "# Création d'une liste vide pour recevoir les prédictions\n",
        "predictions = []\n",
        "\n",
        "# Calcul des prédiction pour chaque groupe de 20 valeurs consécutives de la série\n",
        "# dans l'intervalle de validation\n",
        "for t in temps[temps_separation:-taille_fenetre]:\n",
        "    X = np.reshape(Serie_Normalisee[t:t+taille_fenetre],(1,taille_fenetre))\n",
        "    predictions.append(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ3ibdZmlAyF"
      },
      "source": [
        "# Affiche la série et les prédictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps,serie,label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title('Prédictions avec le modèle LSTM simple couche')\n",
        "plt.show()\n",
        "\n",
        "# Zoom sur l'intervalle de validation\n",
        "plt.figure(figsize=(10, 6))\n",
        "affiche_serie(temps[temps_separation:],serie[temps_separation:],label=\"Série temporelle\")\n",
        "affiche_serie(temps[temps_separation+taille_fenetre:],np.asarray(predictions*std+mean)[:,0,0],label=\"Prédictions\")\n",
        "plt.title(\"Prédictions avec le modèle LSTM simple couche (zoom sur l'intervalle de validation)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}