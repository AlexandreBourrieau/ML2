{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_HRHN_NASDAQ.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreBourrieau/ML2/blob/main/TimeSeries_Multi/Multi_HRHN_NASDAQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Luvr5mg72jn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngD1T5rE09j"
      },
      "source": [
        "# Initialisation TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azq2F27MXmeS"
      },
      "source": [
        "import os\n",
        "\n",
        "use_tpu = True\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "  TPU_ADDRESS = ''\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXLu7v7ZiZP"
      },
      "source": [
        "# Chargement et correction des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg8UqC4JTMqD"
      },
      "source": [
        "Le dataset utilisé contient les prix des actions des 81 principales compagnies du NASDAQ100. La valeur de l'index du NASDAQ est utilisé comme cible.  \n",
        "La fréquence des information est d'une minute, depuis le 26 juillet 2016 jusqu'au 22 décembre 2016, soit 105 jours au total (les samedi et dimanche ne sont pas comptés, ainsi que le 25 novembre qui ne possède que 210 données et le 22 décembre qui n'en possède que 180)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNPjm5bA9_u8"
      },
      "source": [
        "**1. Chargement des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WwTu0bDquT2"
      },
      "source": [
        "!rm *.csv\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/nasdaq100_padding.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66721h8-CY1"
      },
      "source": [
        "**2. Analyse et correction des données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffclRRHzqxYO"
      },
      "source": [
        "# Création de la série sous Pandas\n",
        "df_etude = pd.read_csv(\"nasdaq100_padding.csv\",dtype=\"float32\")\n",
        "df_etude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2st85d9AKo"
      },
      "source": [
        "Affiche les types :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INH5D4lncQRY"
      },
      "source": [
        "df_etude.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXCWWy_kBmpZ"
      },
      "source": [
        "**5. Affiche les données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0rshQNtq2P-"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['NDX'], line=dict(color='blue', width=1),name=\"Index\"))\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CVmrVUCMh5"
      },
      "source": [
        "# Séparation des données de test et d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKbWLsWRCMh6"
      },
      "source": [
        "# Sépare les données en entrainement et tests\n",
        "pourcentage = 0.8\n",
        "temps_separation = int(len(df_etude.values) * pourcentage)\n",
        "date_separation = df_etude.index[temps_separation]\n",
        "\n",
        "serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n",
        "serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n",
        "\n",
        "print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n",
        "print(\"Taille de la validation : %d\" %len(serie_test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsZWwM0-CMh7"
      },
      "source": [
        "**Normalisation des données :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yniWB2X8CMh8"
      },
      "source": [
        "On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emf7MqosCMh8"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Constrution des séries\n",
        "serie_entrainement_X_norm = []\n",
        "serie_test_X_norm = []\n",
        "\n",
        "for i in range(0,len(df_etude.columns)):\n",
        "  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n",
        "  serie_test_X_norm.append(serie_test_X[:,i])\n",
        "\n",
        "serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n",
        "serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n",
        "serie_test_X_norm = tf.transpose(serie_test_X_norm)\n",
        "\n",
        "# Initialisaton du MinMaxScaler\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "min_max_scaler.fit(serie_entrainement_X_norm)\n",
        "\n",
        "# Normalisation des séries\n",
        "serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n",
        "serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBbMQIICMh9"
      },
      "source": [
        "print(serie_entrainement_X_norm.shape)\n",
        "print(serie_test_X_norm.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6THNLf2CMh-"
      },
      "source": [
        "# Affiche quelques séries\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:5], label=\"X_Ent\")\n",
        "ax.plot(df_etude.index[temps_separation:].values,serie_test_X_norm[:,0:5], label=\"X_Val\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNJjTisMlfgQ"
      },
      "source": [
        "# Création des datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i817Q5rwIL_x"
      },
      "source": [
        "Les datasets sont créés de la manière suivante :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwssikbyIEzc"
      },
      "source": [
        "  <img src='https://raw.githubusercontent.com/AlexandreBourrieau/FICHIERS/main/Series_Temporelles/Multi/images/DatasetDARNN.bmp' width=500/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoiHptoQyTxL"
      },
      "source": [
        "**1. Exemple de dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQQTRH_5wjrc"
      },
      "source": [
        "X1 = np.linspace(1,100,100)\n",
        "X2 = np.linspace(101,200,100)\n",
        "X3 = np.linspace(201,300,100)\n",
        "Y = np.linspace(301,400,100)\n",
        "\n",
        "X1 = tf.expand_dims(X1,-1)\n",
        "X2 = tf.expand_dims(X2,-1)\n",
        "X3 = tf.expand_dims(X3,-1)\n",
        "Y = tf.expand_dims(Y,-1)\n",
        "\n",
        "Serie_X = tf.concat([X1,X2,X3],axis=1)\n",
        "Serie_Y = Y\n",
        "print(Serie_X.shape)\n",
        "\n",
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),((X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT-1)}\n",
        "# Y = YT\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence-1][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "test_dataset = prepare_dataset_XY(Serie_X,Serie_Y,10,1,1,1)\n",
        "\n",
        "print(len(list(test_dataset.as_numpy_iterator())))\n",
        "for element in test_dataset.take(1):\n",
        "  print(element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLEjFxNnyWse"
      },
      "source": [
        "**2. Préparation des datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y67w_LmnpiP"
      },
      "source": [
        "# Fonction permettant de créer un dataset à partir des données de la série temporelle\n",
        "# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n",
        "#       (Y1,Y2,...,YT-1)}\n",
        "# Y = YT\n",
        "\n",
        "def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n",
        "  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n",
        "  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n",
        "  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence-1][:,:]))\n",
        "  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n",
        "  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n",
        "  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n",
        "  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n",
        "  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n",
        "  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhUzmxdlj0g"
      },
      "source": [
        "# Définition des caractéristiques du dataset que l'on souhaite créer\n",
        "batch_size = 128\n",
        "longueur_sequence = 10\n",
        "longueur_sortie = 1\n",
        "shift=1\n",
        "\n",
        "# Création du dataset\n",
        "dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n",
        "dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJX_otLmJ7w"
      },
      "source": [
        "print(len(list(dataset.as_numpy_iterator())))\n",
        "for element in dataset.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # (Y1,Y2,...,YT-1)\n",
        "  print(element[1].shape)               # YT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKsAMMRn2JI"
      },
      "source": [
        "print(len(list(dataset_val.as_numpy_iterator())))\n",
        "for element in dataset_val.take(1):\n",
        "  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n",
        "  print(element[0][1].shape)            # Y1,Y2,...,YT-1\n",
        "  print(element[1].shape)               # YT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuecK3H6GUeX"
      },
      "source": [
        "**3. Préparation des X/Y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpCqWrvonaB3"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_train = [X1,X2]\n",
        "y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_train[0].shape)\n",
        "print(x_train[1].shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDnemp8NujA"
      },
      "source": [
        "X1 = []\n",
        "X2 = []\n",
        "\n",
        "# Extrait les X,Y du dataset\n",
        "x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n",
        "                                        # y=43x(BS,1,1)\n",
        "for i in range(len(x)):\n",
        "  X1.append(x[i][0])          \n",
        "  X2.append(x[i][1])\n",
        "\n",
        "X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n",
        "X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n",
        "\n",
        "X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n",
        "X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n",
        "\n",
        "# Recombine les données\n",
        "y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n",
        "X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n",
        "X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n",
        "\n",
        "x_val = [X1,X2]\n",
        "y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n",
        "\n",
        "# Affiche les formats\n",
        "print(x_val[0].shape)\n",
        "print(x_val[1].shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNV_5uXS4eaS"
      },
      "source": [
        "# Affichage des séries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9aPVTcDn6TQ"
      },
      "source": [
        "# Affiche la série\n",
        "fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n",
        "\n",
        "ax.plot(np.linspace(0,longueur_sequence,longueur_sequence),x_train[0][0,:,0:3],label=\"X_train (X)\")\n",
        "ax.plot(np.linspace(0,longueur_sequence-1,longueur_sequence-1),x_train[1][0,:,:],label=\"X_train (Y)\")\n",
        "\n",
        "ax.plot(np.linspace(longueur_sequence,longueur_sequence+1,1),y_train[0,:,:],label=\"Y_train\",marker=\"*\")\n",
        "\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KvuPUL1nXpX"
      },
      "source": [
        "# Création du modèle HRHN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBvAzS8KtFlp"
      },
      "source": [
        "Le modèle HRHN est décrit dans ce document de recherche : [Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction](https://arxiv.org/pdf/1806.00685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKq0mqg2ts2w"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Mod%C3%A8leHRHN1.png?raw=true' width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrCCnwy8nXp4"
      },
      "source": [
        "# **1. Création de l'encodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tn5xnfnuehY"
      },
      "source": [
        "L'encodeur a pour but de créer des représentations cachées des séries exogènes qui prennent en compte les relations spatiales entre ces séries ainsi que les relations temporelles.  \n",
        "Les relations spatiales sont extraitent à l'aide d'un ensemble de réseaux de convolution qui produisent des représentations w1, w2... w(T-1).  \n",
        "Ces représentations sont ensuites codées par un réseau RHN à 3 couches afin d'en extraire les relations temporelles. En sortie de ce réseau RHN, on extrait 3 tenseurs dont chacun contient les (T-1) états cachés de chaque couche du réseau RHN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0mq5BSauQUq"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-boowSGDp3"
      },
      "source": [
        "***a. Création des CNN parallèlisés***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6dF5Cp9vzWB"
      },
      "source": [
        "La structure d'un réseau de convolution est composée de trois couches CNN-1D + Max-pooling :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jga5_ZzKv6CI"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN1.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CMXl2tgwJ76"
      },
      "source": [
        "L'intégration de caque réseau dans Keras est parallélisée :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtR-u7ZqwjTL"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN2.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ETqqvAIGKLi"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "\n",
        "class Encodeur_CNN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling,dim_motif):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  # Création de Tin réseaux de convolution + max_pooling en //\n",
        "  ############################################################\n",
        "  def build(self,input_shape):\n",
        "    convs = []\n",
        "    input_cnns = []\n",
        "\n",
        "    # Création des Tin entrées des réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "        input_cnns.append(tf.keras.Input(shape=(input_shape[2],1)))       # input = Tin*(batch_size,#dim,1)\n",
        "\n",
        "    # Création des Tin réseaux CNN\n",
        "    for i in range(input_shape[1]):\n",
        "      conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[0],      # conv : (batch_size,#dim,16)\n",
        "                                    kernel_size=self.dim_filtres_cnn[0],\n",
        "                                    activation='relu',\n",
        "                                    padding='same',\n",
        "                                    strides=1)(input_cnns[i])\n",
        "      conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[0],      # conv : (batch_size,#pooling1,16)\n",
        "                                       padding='same')(conv)\n",
        "      for n in range(1,len(self.dim_filtres_cnn)):\n",
        "        conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                      kernel_size=self.dim_filtres_cnn[n],\n",
        "                                      activation='relu',\n",
        "                                      padding='same',\n",
        "                                      strides=1)(conv)\n",
        "        conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n",
        "                                         padding='same')(conv)\n",
        "      convs.append(conv)\n",
        "    \n",
        "    # Création de la sortie concaténée des Tin réseaux CNN\n",
        "    out = tf.convert_to_tensor(convs)                                     # out : (Tin,batch_size,#pooling,64)\n",
        "    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,Tin,#pooling,64)\n",
        "    out = tf.keras.layers.Reshape(                                        # out : (batch_size,Tin,#pooling*64)\n",
        "        target_shape=(out.shape[1],out.shape[2]*out.shape[3]))(out)\n",
        "\n",
        "    out = tf.keras.layers.Dense(units=self.dim_motif)(out)                # out : (batch_size,Tin,dim_motif) \n",
        "\n",
        "    # Création du modèle global\n",
        "    self.conv_model = tf.keras.Model(inputs=input_cnns,outputs=out)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:  Entrée séries exogènes  : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     w:      Sorties des motifs CNN  : (batch_size,Tin,#pooling,64)\n",
        "  #                                       (taille dernier filtre=64)\n",
        "  def call(self, input):\n",
        "    # Coupes temporelles sur les séries exogènes\n",
        "    # au format : Tin*(batch_size,#dim,1)\n",
        "    input_list = []\n",
        "    for i in range(input.shape[1]):\n",
        "      input_list.append(tf.transpose(input[:,i:i+1,:],perm=[0,2,1]))      # (batch_size,#dim,1)\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.conv_model(input_list)                                       # (batch_size,Tin,dim_motif)\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_y68mmiRpR1"
      },
      "source": [
        "***b. Création des cellules RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FQ47zOsxHpx"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_RHN.png?raw=true'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvNrS-wQ1B5C"
      },
      "source": [
        "On crée une cellule RHN en reprenant le code précédent auquel :  \n",
        "- On ajoute la possibilité de retourner tous les états cachés de chaque couche\n",
        "- On ajoute la prise en compte de la dimension d'entrée correspondant à la dimension des motifs en sortie des réseaux CNN (dim_motif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CygD9DXbBTDJ"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Structure_RHN4.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HldCwl9z3fY"
      },
      "source": [
        "class Cellule_RHN(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_RHN, nbr_couches, return_all_states = False, dim_input=1):\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches = nbr_couches\n",
        "    self.dim_input = dim_input\n",
        "    self.return_all_states = return_all_states\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.Wh = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wh\")       # (#dim, #RHN)\n",
        "    self.Wt = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wt\")       # (#dim, #RHN)\n",
        "    self.Wc = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wc\")       # (#dim, #RHN)\n",
        "\n",
        "    self.Rh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rh\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rt\")      # (n_couches,#RHN, #RHN)\n",
        "    self.Rc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rc\")      # (n_couches,#RHN, #RHN)\n",
        "\n",
        "    self.bh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bh\")        # (n_couches,#RHN, 1)\n",
        "    self.bt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bt\")        # (n_couches,#RHN, 1)\n",
        "    self.bc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bc\")        # (n_couches,#RHN, 1)\n",
        "\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "\n",
        "    # Initialisation des masques de dropout\n",
        "  def InitMasquesDropout(self,drop=0.0):\n",
        "    self.Wh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Wc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n",
        "    self.Rh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "    self.Rc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n",
        "\n",
        "  # Entrées :\n",
        "  #     input:          Entrées X[t]        : (batch_size,1,#dim)\n",
        "  #     init_hidden:    Etat caché Init.    : (batch_size,#RHN)\n",
        "  # Sorties :\n",
        "  #     sL:             Etat caché de la dernière couche       : (batch_size,#RHN) \n",
        "  #           ou        Etats cachés de chaque couche SL[t]    : (batch_size,nbr_couches,#RHN)\n",
        "  def call(self, input, init_hidden=None):\n",
        "    # Construction d'un vecteur d'état nul si besoin\n",
        "    if init_hidden == None:\n",
        "      init_hidden = tf.matmul(tf.zeros(shape=(self.dim_RHN,input.shape[2])), # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "                              tf.transpose(input,perm=[0,2,1]))\n",
        "      init_hidden = tf.squeeze(init_hidden,-1)                               # (batch_size,#RHN,1) => (batch_size,#RHN)\n",
        "  \n",
        "    liste_sl = []                                                            # Liste pour  enregistrer les états cachés de chaque couche\n",
        "    # Calcul de hl, tl et cl\n",
        "    for i in range(self.nbr_couches):\n",
        "      if i==0:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[0,:,:],self.Rh[0,:,:])                      # (#RHN,1)_x_(#RHN,#RHN) = (#RHN,#RHN)\n",
        "        Rt = tf.multiply(self.Rt_[0,:,:],self.Rt[0,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[0,:,:],self.Rc[0,:,:])\n",
        "\n",
        "        Wh = tf.multiply(self.Wh_,self.Wh)                                    # (#dim,1)_x_(#dim,#RHN) = (#dim,#RHN)\n",
        "        Wt = tf.multiply(self.Wt_,self.Wt)\n",
        "        Wc = tf.multiply(self.Wc_,self.Wc)\n",
        "   \n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + tf.matmul(tf.transpose(Wh),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + tf.matmul(tf.transpose(Wt),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + tf.matmul(tf.transpose(Wc),\n",
        "                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "\n",
        "      else:\n",
        "        # Applique le masque aux poids\n",
        "        Rh = tf.multiply(self.Rh_[i,:,:],self.Rh[i,:,:])\n",
        "        Rt = tf.multiply(self.Rt_[i,:,:],self.Rt[i,:,:])\n",
        "        Rc = tf.multiply(self.Rc_[i,:,:],self.Rc[i,:,:])\n",
        "\n",
        "        # Calcul de hl\n",
        "        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = hl + self.bh[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n",
        "        hl = K.tanh(hl)\n",
        "\n",
        "        # Calcul de tl\n",
        "        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tl + self.bt[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n",
        "        tl = tf.keras.activations.sigmoid(tl)\n",
        "\n",
        "        # Calcul de cl\n",
        "        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = cl + self.bc[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n",
        "        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n",
        "        cl = tf.keras.activations.sigmoid(cl)\n",
        "      \n",
        "      # Calcul de sl\n",
        "      sl = tf.keras.layers.multiply([hl,tl])                                # (batch_size,#RHN)\n",
        "      sl = sl + tf.keras.layers.multiply([init_hidden,cl])                  # (batch_size,#RHN)\n",
        "      liste_sl.append(sl)       # Sauvegarde l'état caché de la couche courante\n",
        "      init_hidden = sl\n",
        "    if self.return_all_states == False:\n",
        "      return sl\n",
        "    else:\n",
        "      liste_sl = tf.convert_to_tensor(liste_sl)                             # (nbr_couches,batch_size,#RHN)\n",
        "      liste_sl = tf.transpose(liste_sl,perm=[1,0,2])                        # (batch_size,nbr_couches,#RHN)\n",
        "      return liste_sl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJY-TY7W55ZF"
      },
      "source": [
        "***c. Création de l'encodeur : Convolutions + RHN***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMT8C8-UVe2O"
      },
      "source": [
        "<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYQu67IfBdel"
      },
      "source": [
        "# Arguments de la méthode __init__\n",
        "#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n",
        "#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n",
        "#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n",
        "#   dim_motif         :   dimension du motif en sortie du CNN\n",
        "#   dim_RHN           :   dimension du vecteur caché RHN\n",
        "#   nbr_couches_RHN   :   nombre de couches du RHN\n",
        "#   dropout           :   dropout variationnel pour le RHN ex: [0.1]\n",
        "\n",
        "class Encodeur(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling, dim_motif,dim_RHN,nbr_couches_RHN, dropout=0.0):\n",
        "    self.dim_filtres_cnn = dim_filtres_cnn\n",
        "    self.nbr_filtres_cnn = nbr_filtres_cnn\n",
        "    self.dim_max_pooling = dim_max_pooling\n",
        "    self.dim_motif = dim_motif\n",
        "    self.dim_RHN = dim_RHN\n",
        "    self.nbr_couches_RHN = nbr_couches_RHN\n",
        "    self.dropout = dropout\n",
        "    super().__init__()                # Appel du __init__() de la classe Layer\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    self.encodeur_cnn = Encodeur_CNN(dim_filtres_cnn=self.dim_filtres_cnn,nbr_filtres_cnn=self.nbr_filtres_cnn,dim_max_pooling=self.dim_max_pooling,dim_motif=self.dim_motif)\n",
        "    self.RHN = Cellule_RHN(dim_RHN=self.dim_RHN,nbr_couches=self.nbr_couches_RHN,return_all_states=True,dim_input=self.dim_motif)\n",
        "    super().build(input_shape)        # Appel de la méthode build()\n",
        "    \n",
        "  # Entrées :\n",
        "  #     input:          Entrées X         : (batch_size,Tin,#dim)\n",
        "  # Sorties :\n",
        "  #     hidden_states   Vecteurs cachés   : (batch_size,nbr_couches,Tin,#RHN)\n",
        "  def call(self, input):\n",
        "    # Convolutions spatiales des séries exogènes\n",
        "    w = self.encodeur_cnn(input)      #  (batch_size,Tin,dim_motif)\n",
        "\n",
        "    # Encodage des motifs CNN avec les cellules RHN\n",
        "    sequence = []\n",
        "    hidden = None\n",
        "\n",
        "    # Initialisation des masques de dropout pour tous les pas de temps\n",
        "    self.RHN.InitMasquesDropout(self.dropout)\n",
        "\n",
        "    # Applique la cellule RHN à chaque pas de temps\n",
        "    for i in range(input.shape[1]):\n",
        "      hidden = self.RHN(w[:,i:i+1,:],hidden)          # Envoie (batch_size,1,dim_motif)\n",
        "      sequence.append(hidden)                         # Sauve (batch_size,nbr_couches,#RHN)\n",
        "\n",
        "      # Le premier état caché du prochain instant\n",
        "      # est l'état caché de la dernière couche précédente\n",
        "      hidden = hidden[:,self.nbr_couches_RHN-1,:]       # (batch_size,#RHN)\n",
        "\n",
        "    # Traite le format des vecteurs cachés de l'encodeur\n",
        "    sequence = tf.convert_to_tensor(sequence)               # (Tin,batch_size,nbr_couches,#RHN)\n",
        "    hidden_states = tf.transpose(sequence,perm=[1,2,0,3])   # (batch_size,nbr_couches,Tin,#RHN)  \n",
        "\n",
        "    return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOHqRH7oASLh"
      },
      "source": [
        "**Test de l'encodeur**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbasFfpdAUkA"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvygu1kuD9lz"
      },
      "source": [
        "x_train[0][0:5,:,:].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v427DL7AW5p"
      },
      "source": [
        "s = Encodeur(dim_filtres_cnn=[16,32,64],nbr_filtres_cnn=[3,3,3],dim_max_pooling=[3,3,3],dim_motif=3,dim_RHN=128,nbr_couches_RHN=3,dropout=0.0)(x_train[0][0:5,:,:])\n",
        "s.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM8tVDxynXp4"
      },
      "source": [
        "**2. Création de la couche d'encodeur à base de LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXeI4FoknXp8"
      },
      "source": [
        "**2. Création de la couche d'attention du décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9vEoEscnXp-"
      },
      "source": [
        "**3. Création de la couche de décodeur**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUjhbMK8nXp-"
      },
      "source": [
        "**4. Création de la couche réseau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHECl3KPnXp_"
      },
      "source": [
        "**4. Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9KMMJOqnXp_"
      },
      "source": [
        "dim_LSTM = 128\n",
        "drop=0.0\n",
        "l2reg=0.0\n",
        "\n",
        "def get_model():\n",
        "  entrees_sequences = tf.keras.layers.Input(shape=(longueur_sequence,x_train[0].shape[2]))\n",
        "  sorties_sequence = tf.keras.layers.Input(shape=(longueur_sequence-1,1))\n",
        "\n",
        "  encodeur = Encodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "  decodeur = Decodeur(dim_LSTM=dim_LSTM,drop=drop,regul=l2reg)\n",
        "\n",
        "  sortie = Net_DARNN(encodeur,decodeur,longueur_sequence=longueur_sequence,dim_LSTM=dim_LSTM,regul=l2reg,drop=drop)(entrees_sequences,sorties_sequence)\n",
        "\n",
        "  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWHVAO0auHZ"
      },
      "source": [
        "# Entrainement avec TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAWNR0jq-90"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "max_periodes = 500\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  # Création du modèle\n",
        "  model = get_model()\n",
        "\n",
        "  # Définition des paramètres liés à l'évolution du taux d'apprentissage\n",
        "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      initial_learning_rate=0.001,\n",
        "      decay_steps=50,\n",
        "      decay_rate=0.001)\n",
        "\n",
        "  optimiseur=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  # Utilisation de la méthode ModelCheckPoint\n",
        "  CheckPoint = tf.keras.callbacks.ModelCheckpoint(\"poids_train.hdf5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only = True, mode='auto', save_freq='epoch')\n",
        "\n",
        "  # Compile le modèle\n",
        "  model.compile(loss=\"mse\", optimizer=optimiseur, metrics=\"mse\")\n",
        "\n",
        "  # Entraine le modèle\n",
        "  historique = model.fit(x=[x_train[0],x_train[1]],y=y_train,validation_data=([x_val[0],x_val[1]],y_val), epochs=max_periodes,verbose=1, callbacks=[CheckPoint,tf.keras.callbacks.EarlyStopping(monitor='loss', patience=100)],batch_size=batch_size)\n",
        "\n",
        "files.download('poids_train.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfv3rZfryIO"
      },
      "source": [
        "model.load_weights(\"poids_train.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkK74TexF-Ou"
      },
      "source": [
        "erreur_entrainement = historique.history[\"loss\"]\n",
        "erreur_validation = historique.history[\"val_loss\"]\n",
        "\n",
        "# Affiche l'erreur en fonction de la période\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_entrainement, label=\"Erreurs sur les entrainements\")\n",
        "plt.plot(np.arange(0,len(erreur_entrainement)),erreur_validation, label =\"Erreurs sur les validations\")\n",
        "plt.legend()\n",
        "\n",
        "plt.title(\"Evolution de l'erreur en fonction de la période\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxfFTzSYfyIf"
      },
      "source": [
        "model.evaluate(x=[x_train[0],x_train[1]],y=y_train)\n",
        "model.evaluate(x=[x_val[0],x_val[1]],y=y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Xeyk8B-dlm"
      },
      "source": [
        "# Chargement du modèle pré-entrainé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOahfnJ-yX1"
      },
      "source": [
        "**DA-RNN128_DR0_L2_0_BS128_EP500_10**  \n",
        "\n",
        "  - Vecteur LSTM : 128  \n",
        "\t- Longueur entrée : 10  \n",
        "\t- Longueur sortie : 1\n",
        "\t- Drop : 0.0\n",
        "\t- L2 : 0.00  \n",
        "\t- Batch Size : 128  \n",
        "\t- Périodes : 500   \n",
        "\t=> mse :  2.2493e-05 / 2.8063e-04\n",
        "  \n",
        "  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Train_DARNN2.png?raw=true' width=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-XKdrbK-022"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05bqnRr-pyf"
      },
      "source": [
        "!rm *.hdf5\n",
        "!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Models/DA-RNN128_DR0_L2_0_BS128_EP500_10.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmmlRjiBImP"
      },
      "source": [
        "model.load_weights(\"DA-RNN128_DR0_L2_0_BS128_EP500_10.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfIkATObr8cV"
      },
      "source": [
        "# Prédictions single-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA2H5J4S35o"
      },
      "source": [
        "pred_ent = model.predict([x_train[0],x_train[1]],verbose=1)\n",
        "pred_val = model.predict([x_val[0],x_val[1]],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q1NA36BFZ4N"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "decalage = 1\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Courbes originales\n",
        "fig.add_trace(go.Scatter(x=df_etude.index,y=serie_entrainement_X_norm[:,-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation:],y=serie_test_X_norm[:,-1],line=dict(color='red', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur l'entrainement\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "#Affiche les prédictions sur les validations\n",
        "pred = []\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence:],y=pred, mode='lines', line=dict(color='green', width=1)))\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9wCeoEnMEYv"
      },
      "source": [
        "**Erreurs en single step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2_ceD69Mju6"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_ent)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_ent[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[longueur_sequence:],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_ent = tf.keras.losses.mse(serie_entrainement_X_norm[longueur_sequence:-(serie_entrainement_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV5JTpnnNApH"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "decalage = 1\n",
        "pred = []\n",
        "\n",
        "max = len(pred_val)\n",
        "max = max\n",
        "for i in range(0,max):\n",
        "  pred.append(tf.squeeze(pred_val[i,0:decalage,:],1))\n",
        "pred = tf.convert_to_tensor(pred).numpy()\n",
        "pred = np.reshape(pred,(pred.shape[0]*pred.shape[1]))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],line=dict(color='blue', width=1)))\n",
        "fig.add_trace(go.Scatter(x=df_etude.index[temps_separation+longueur_sequence::],y=pred,line=dict(color='green', width=1)))\n",
        "\n",
        "\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "yaxis=dict(autorange = True,fixedrange= False)\n",
        "fig.update_yaxes(yaxis)\n",
        "fig.show()\n",
        "\n",
        "mse_test = tf.keras.losses.mse(serie_test_X_norm[longueur_sequence:-(serie_test_X_norm[longueur_sequence:,:].shape[0]-pred.shape[0]),-1],pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FShIaJINHBX"
      },
      "source": [
        "print(mse_ent)\n",
        "print(mse_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}