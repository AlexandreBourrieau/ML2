{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copie de Multi_HRHN-PID_SP500-RAYTUNE.ipynb","provenance":[{"file_id":"https://github.com/AlexandreBourrieau/essais_ML/blob/main/Multi_HRHN_PID_SP500_RAYTUNE.ipynb","timestamp":1627922240846}],"collapsed_sections":["j8CVmrVUCMh5","YNJjTisMlfgQ","_KvuPUL1nXpX","I8_PgjEpdC8z","uEWxDydrYgzi","rh5MO6QCZbfv","GdSoHUDnkpgu","eXnzuTH0uiZU","Yl-0TUJeloJC"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IrOOJabrUaZ2"},"source":["\n","\n","---\n","\n","\n","### **RHN + PID incorporé**\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"9Luvr5mg72jn"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras import backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkvcvZ3AjqGw"},"source":["!pip install -q 'ray[tune]' 'ray[default]'\n","!pip install -q --upgrade aioredis==1.3.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SngD1T5rE09j"},"source":["# Initialisation TPU"]},{"cell_type":"code","metadata":{"id":"azq2F27MXmeS"},"source":["import os\n","\n","use_tpu = True\n","\n","if use_tpu:\n","    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n","\n","if 'COLAB_TPU_ADDR' in os.environ:\n","  TPU_ADDRESS = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n","else:\n","  TPU_ADDRESS = ''\n","\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices('TPU'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ArXLu7v7ZiZP"},"source":["# Chargement et correction des données"]},{"cell_type":"markdown","metadata":{"id":"mg8UqC4JTMqD"},"source":["Le dataset utilisé est SP500..."]},{"cell_type":"markdown","metadata":{"id":"eNPjm5bA9_u8"},"source":["**1. Chargement des fichiers CSV**"]},{"cell_type":"code","metadata":{"id":"2WwTu0bDquT2"},"source":["!rm *.csv\n","!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/SPX2010_2021.csv\"\n","!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/VIX2010_2021.csv\"\n","!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/US10Y2010_2021.csv\"\n","!curl --location --remote-header-name --remote-name \"https://github.com/AlexandreBourrieau/FICHIERS/raw/main/Series_Temporelles/Multi/Data/TYVIX2010_2021.csv\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z66721h8-CY1"},"source":["**2. Analyse et correction des données S&P500**"]},{"cell_type":"code","metadata":{"id":"ffclRRHzqxYO"},"source":["# Création de la série sous Pandas\n","df_SP500 = pd.read_csv(\"SPX2010_2021.csv\",sep=\";\",names=[\"Date\",\"SP500_O\",\"SP500_H\",\"SP500_L\",\"SP500_C\"],decimal=\",\")\n","df_SP500['Date'] = pd.to_datetime(df_SP500['Date'])\n","df_SP500 = df_SP500.set_index(\"Date\")\n","df_SP500 = df_SP500.asfreq(freq=\"1D\")\n","df_SP500"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0rshQNtq2P-"},"source":["import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","\n","fig.add_trace(go.Scatter(x=np.linspace(0,len(df_SP500),len(df_SP500)+1),y=df_SP500['SP500_C'], line=dict(color='blue', width=1),name=\"Index\"))\n","fig.update_xaxes(rangeslider_visible=True)\n","yaxis=dict(autorange = True,fixedrange= False)\n","fig.update_yaxes(yaxis)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2v6hCl_2yZB"},"source":["**2. Analyse et correction des données VIX**"]},{"cell_type":"code","metadata":{"id":"D1lBSgjy2yZC"},"source":["# Création de la série sous Pandas\n","df_VIX = pd.read_csv(\"VIX2010_2021.csv\",sep=\";\",names=[\"Date\",\"VIX_O\",\"VIX_H\",\"VIX_L\",\"VIX_C\"],decimal=\",\")\n","df_VIX['Date'] = pd.to_datetime(df_VIX['Date'])\n","df_VIX = df_VIX.set_index(\"Date\")\n","df_VIX = df_VIX.asfreq(freq=\"1D\")\n","df_VIX"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjcWJ1fw2yZD"},"source":["import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","\n","fig.add_trace(go.Scatter(x=np.linspace(0,len(df_VIX),len(df_VIX)+1),y=df_VIX['VIX_C'], line=dict(color='blue', width=1),name=\"Index\"))\n","fig.update_xaxes(rangeslider_visible=True)\n","yaxis=dict(autorange = True,fixedrange= False)\n","fig.update_yaxes(yaxis)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PcV_ZlnD57Y5"},"source":["**3. Analyse et correction des données US10Y**"]},{"cell_type":"code","metadata":{"id":"NL5Ifonj57Y6"},"source":["# Création de la série sous Pandas\n","df_US10Y= pd.read_csv(\"US10Y2010_2021.csv\",sep=\";\",names=[\"Date\",\"US10Y_O\",\"US10Y_H\",\"US10Y_L\",\"US10Y_C\"],decimal=\",\")\n","df_US10Y['Date'] = pd.to_datetime(df_US10Y['Date'])\n","df_US10Y = df_US10Y.set_index(\"Date\")\n","df_US10Y = df_US10Y.asfreq(freq=\"1D\")\n","df_US10Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaeKBt1V57Y9"},"source":["import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","\n","fig.add_trace(go.Scatter(x=np.linspace(0,len(df_US10Y),len(df_US10Y)+1),y=df_US10Y['US10Y_C'], line=dict(color='blue', width=1),name=\"Index\"))\n","fig.update_xaxes(rangeslider_visible=True)\n","yaxis=dict(autorange = True,fixedrange= False)\n","fig.update_yaxes(yaxis)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIH2nnno-aXp"},"source":["**4. Analyse et correction des données TYVIX**"]},{"cell_type":"code","metadata":{"id":"2JjB4FLL-aXq"},"source":["# Création de la série sous Pandas\n","df_TYVIX= pd.read_csv(\"TYVIX2010_2021.csv\",sep=\";\",names=[\"Date\",\"TYVIX_C\"],decimal=\",\")\n","df_TYVIX['Date'] = pd.to_datetime(df_TYVIX['Date'])\n","df_TYVIX = df_TYVIX.set_index(\"Date\")\n","df_TYVIX = df_TYVIX.asfreq(freq=\"1D\")\n","df_TYVIX"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1hj1rle-aXr"},"source":["import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","\n","fig.add_trace(go.Scatter(x=np.linspace(0,len(df_TYVIX),len(df_TYVIX)+1),y=df_TYVIX['TYVIX_C'], line=dict(color='blue', width=1),name=\"Index\"))\n","fig.update_xaxes(rangeslider_visible=True)\n","yaxis=dict(autorange = True,fixedrange= False)\n","fig.update_yaxes(yaxis)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JVQ772Gu_3eE"},"source":["**5. Fusion des données et correction des valeurs NaN**"]},{"cell_type":"code","metadata":{"id":"UgnvBRSvAjq3"},"source":["from functools import reduce\n","\n","df_liste = [df_SP500,df_VIX,df_US10Y,df_TYVIX]\n","\n","df_etude = reduce(lambda  left,right: pd.merge(left,right,on=['Date'],how='outer'), df_liste)\n","df_etude"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"px1DRPuSBfXc"},"source":["debut = \"2011-01-10\"\n","fin = \"2020-04-24\"\n","\n","mask = (df_etude.index >= debut) & (df_etude.index <= fin)\n","df_etude = df_etude.loc[mask]\n","df_etude"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_n0i_0lhCjPT"},"source":["df_etude.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGO9fDfwC-FL"},"source":["df_etude = df_etude.interpolate(method=\"linear\")\n","df_etude.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8I2iqRsDZy7"},"source":["Déplace la cible à la fin :"]},{"cell_type":"code","metadata":{"id":"bnq-XsMRDRUQ"},"source":["col = df_etude.pop('SP500_C')\n","df_etude.insert(len(df_etude.columns),\"SP500_C\",col)\n","df_etude"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlTj0KPxEaSO"},"source":["import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","\n","fig.add_trace(go.Scatter(x=np.linspace(0,len(df_etude),len(df_etude)+1),y=df_etude['SP500_C'], line=dict(color='blue', width=1),name=\"Index\"))\n","fig.update_xaxes(rangeslider_visible=True)\n","yaxis=dict(autorange = True,fixedrange= False)\n","fig.update_yaxes(yaxis)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j8CVmrVUCMh5"},"source":["# Séparation des données"]},{"cell_type":"code","metadata":{"id":"hKbWLsWRCMh6"},"source":["# Sépare les données en entrainement et tests\n","# 80% / 10% / 10%\n","\n","temps_separation = int(len(df_etude.values) * 0.8)\n","temps_separation_PID = int(len(df_etude.values) * 0.9)\n","date_separation = df_etude.index[temps_separation]\n","date_separation_PID = df_etude.index[temps_separation_PID]\n","\n","serie_entrainement_X = np.array(df_etude.values[:temps_separation],dtype=np.float32)\n","serie_test_X = np.array(df_etude.values[temps_separation:],dtype=np.float32)\n","\n","serie_entrainement_PID_X = np.array(df_etude.values[temps_separation:temps_separation_PID],dtype=np.float32)\n","serie_test_PID_X = np.array(df_etude.values[temps_separation_PID:],dtype=np.float32)\n","\n","\n","print(\"Taille de l'entrainement : %d\" %len(serie_entrainement_X))\n","print(\"Taille de la validation : %d\" %len(serie_test_X))\n","print(\"Taille de l'entrainement PID : %d\" %len(serie_entrainement_PID_X))\n","print(\"Taille de la validation PID: %d\" %len(serie_test_PID_X))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GsZWwM0-CMh7"},"source":["**Normalisation des données :**"]},{"cell_type":"markdown","metadata":{"id":"yniWB2X8CMh8"},"source":["On normalise les données à l'aide de la fonction [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"]},{"cell_type":"code","metadata":{"id":"emf7MqosCMh8"},"source":["from sklearn import preprocessing\n","\n","# Constrution des séries\n","serie_entrainement_X_norm = []\n","serie_test_X_norm = []\n","\n","serie_entrainement_PID_X_norm = []\n","serie_test_PID_X_norm = []\n","\n","\n","for i in range(0,len(df_etude.columns)):\n","  serie_entrainement_X_norm.append(serie_entrainement_X[:,i])\n","  serie_test_X_norm.append(serie_test_X[:,i])\n","\n","  serie_entrainement_PID_X_norm.append(serie_entrainement_PID_X[:,i])\n","  serie_test_PID_X_norm.append(serie_test_PID_X[:,i])\n","\n","serie_entrainement_X_norm = tf.convert_to_tensor(serie_entrainement_X_norm)\n","serie_entrainement_X_norm = tf.transpose(serie_entrainement_X_norm)\n","serie_test_X_norm = tf.convert_to_tensor(serie_test_X_norm)\n","serie_test_X_norm = tf.transpose(serie_test_X_norm)\n","serie_entrainement_PID_X_norm = tf.convert_to_tensor(serie_entrainement_PID_X_norm)\n","serie_entrainement_PID_X_norm = tf.transpose(serie_entrainement_PID_X_norm)\n","serie_test_PID_X_norm = tf.convert_to_tensor(serie_test_PID_X_norm)\n","serie_test_PID_X_norm = tf.transpose(serie_test_PID_X_norm)\n","\n","\n","# Initialisaton du MinMaxScaler\n","min_max_scaler = preprocessing.MinMaxScaler()\n","min_max_scaler.fit(serie_entrainement_X_norm)\n","\n","# Normalisation des séries\n","serie_entrainement_X_norm = min_max_scaler.transform(serie_entrainement_X_norm)\n","serie_test_X_norm = min_max_scaler.transform(serie_test_X_norm)\n","serie_entrainement_PID_X_norm = min_max_scaler.transform(serie_entrainement_PID_X_norm)\n","serie_test_PID_X_norm = min_max_scaler.transform(serie_test_PID_X_norm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoBbMQIICMh9"},"source":["print(serie_entrainement_X_norm.shape)\n","print(serie_test_X_norm.shape)\n","print(serie_entrainement_PID_X_norm.shape)\n","print(serie_test_PID_X_norm.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6THNLf2CMh-"},"source":["# Affiche quelques séries\n","fig, ax = plt.subplots(constrained_layout=True, figsize=(15,5))\n","\n","ax.plot(df_etude.index[:temps_separation].values,serie_entrainement_X_norm[:,0:1], label=\"X_Ent\")\n","ax.plot(df_etude.index[temps_separation:temps_separation_PID].values,serie_entrainement_PID_X_norm[:,0:1], label=\"X_PID_Ent\")\n","ax.plot(df_etude.index[temps_separation_PID:].values,serie_test_PID_X_norm[:,0:1], label=\"X_PID_Val\")\n","\n","ax.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YNJjTisMlfgQ"},"source":["# Création des datasets"]},{"cell_type":"markdown","metadata":{"id":"i817Q5rwIL_x"},"source":["Les datasets sont créés de la manière suivante :"]},{"cell_type":"markdown","metadata":{"id":"EwssikbyIEzc"},"source":["  <img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ConstructionDataset.png?raw=true' width=500/>"]},{"cell_type":"markdown","metadata":{"id":"uLEjFxNnyWse"},"source":["**1. Préparation des datasets**"]},{"cell_type":"code","metadata":{"id":"0Y67w_LmnpiP"},"source":["# Fonction permettant de créer un dataset à partir des données de la série temporelle\n","# X = {((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),(X3_1,X3_2,...,X3_T)),\n","#       (Y1,Y2,...,YT)}\n","# Y = YT+1\n","\n","def prepare_dataset_XY(seriesX, serieY, longueur_sequence, longueur_sortie, batch_size,shift):\n","  datasetX = tf.data.Dataset.from_tensor_slices(seriesX)\n","  datasetX = datasetX.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n","  datasetX = datasetX.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n","  datasetX = datasetX.map(lambda x: (x[0:longueur_sequence][:,:]))\n","  datasetX = datasetX.batch(batch_size,drop_remainder=True).prefetch(1)\n","\n","  datasetY = tf.data.Dataset.from_tensor_slices(serieY)\n","  datasetY = datasetY.window(longueur_sequence+longueur_sortie, shift=shift, drop_remainder=True)\n","  datasetY = datasetY.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie))\n","  datasetY = datasetY.map(lambda x: (x[0:longueur_sequence][:,:]))\n","  datasetY = datasetY.batch(batch_size,drop_remainder=True).prefetch(1)\n","\n","  datasetYPred = tf.data.Dataset.from_tensor_slices(serieY)\n","  datasetYPred = datasetYPred.window(longueur_sequence+longueur_sortie+1, shift=shift, drop_remainder=True)\n","  datasetYPred = datasetYPred.flat_map(lambda x: x.batch(longueur_sequence + longueur_sortie+1))\n","  datasetYPred = datasetYPred.map(lambda x: (x[0:-1][-1:,:]))\n","  datasetYPred = datasetYPred.batch(batch_size,drop_remainder=True).prefetch(1)\n","\n","\n","  dataset = tf.data.Dataset.zip((datasetX,datasetY))\n","  dataset = tf.data.Dataset.zip((dataset,datasetYPred))\n","\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghhUzmxdlj0g"},"source":["# Définition des caractéristiques du dataset que l'on souhaite créer\n","batch_size = 512\n","longueur_sequence = 80\n","longueur_sortie = 1\n","shift=1\n","\n","# Création du dataset\n","dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n","dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)\n","\n","dataset_PID = prepare_dataset_XY(serie_entrainement_PID_X_norm[:,0:-1],serie_entrainement_PID_X_norm[:,-1:], longueur_sequence,longueur_sortie,batch_size,shift)\n","dataset_PID_val = prepare_dataset_XY(serie_test_PID_X_norm[:,0:-1],serie_test_PID_X_norm[:,-1:],longueur_sequence,longueur_sortie,batch_size,shift)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mJX_otLmJ7w"},"source":["print(len(list(dataset.as_numpy_iterator())))\n","for element in dataset.take(1):\n","  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n","  print(element[0][1].shape)            # (Y1,Y2,...,YT)\n","  print(element[1].shape)               # YT+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5qKsAMMRn2JI"},"source":["print(len(list(dataset_val.as_numpy_iterator())))\n","for element in dataset_val.take(1):\n","  print(element[0][0].shape)            # ((X1),(X2),...) = ((X1_1,X1_2,...,X1_T),(X2_1,X2_2,...,X2_T),...)\n","  print(element[0][1].shape)            # Y1,Y2,...,YT\n","  print(element[1].shape)               # YT+1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RuecK3H6GUeX"},"source":["**3. Préparation des X/Y**"]},{"cell_type":"code","metadata":{"id":"MpCqWrvonaB3"},"source":["def Create_train(dataset):\n","  X1 = []\n","  X2 = []\n","\n","  # Extrait les X,Y du dataset\n","  x,y = tuple(zip(*dataset))              # x=43x((BS,10,3),(BS,9,1))\n","                                          # y=43x(BS,1,1)\n","  for i in range(len(x)):\n","    X1.append(x[i][0])          \n","    X2.append(x[i][1])\n","\n","  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n","  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n","\n","  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n","  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n","\n","  # Recombine les données\n","  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n","  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n","  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n","\n","  x_train = [X1,X2]\n","  y_train = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n","\n","  return x_train,y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVbVOFDwERm-"},"source":["x_train, y_train = Create_train(dataset)\n","print(x_train[0].shape)\n","print(x_train[1].shape)\n","print(y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnDnemp8NujA"},"source":["def Create_val(dataset_val):\n","  X1 = []\n","  X2 = []\n","\n","  # Extrait les X,Y du dataset\n","  x,y = tuple(zip(*dataset_val))              # x=43x((BS,10,3),(BS,9,1))\n","                                          # y=43x(BS,1,1)\n","  for i in range(len(x)):\n","    X1.append(x[i][0])          \n","    X2.append(x[i][1])\n","\n","  X1 = tf.convert_to_tensor(X1)           # (43,BS,10,3)\n","  X2 = tf.convert_to_tensor(X2)           # (43,BS,9,1)\n","\n","  X1 = np.asarray(X1,dtype=np.float32)    # (43,BS,10,3)\n","  X2 = np.asarray(X2,dtype=np.float32)    # (43,BS,10,3)   \n","\n","  # Recombine les données\n","  y = np.asarray(y,dtype=np.float32)      # 43x(BS,1,1) => (43xBS,1,1)\n","  X1 = np.reshape(X1,(X1.shape[0]*X1.shape[1],X1.shape[2],X1.shape[3]))   # (43,BS,10,3) => (43xBS,10,3)\n","  X2 = np.reshape(X2,(X2.shape[0]*X2.shape[1],X2.shape[2],X2.shape[3]))   # (43,BS,9,1) => (43*BS,9,1)\n","\n","  x_val = [X1,X2]\n","  y_val = np.asarray(tf.reshape(y,shape=(y.shape[0]*y.shape[1],longueur_sortie,y.shape[3])))\n","\n","  return x_val,y_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTzC0c34Enxr"},"source":["x_val,y_val = Create_val(dataset_val)\n","print(x_val[0].shape)\n","print(x_val[1].shape)\n","print(y_val.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KvuPUL1nXpX"},"source":["# Création du modèle HRHN ENC/DEC"]},{"cell_type":"markdown","metadata":{"id":"LBvAzS8KtFlp"},"source":["Le modèle HRHN est décrit dans ce document de recherche : [Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction](https://arxiv.org/pdf/1806.00685)"]},{"cell_type":"markdown","metadata":{"id":"gKq0mqg2ts2w"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Mod%C3%A8leHRHN1.png?raw=true' width=700>"]},{"cell_type":"markdown","metadata":{"id":"IrCCnwy8nXp4"},"source":["**1. Création de l'encodeur**"]},{"cell_type":"markdown","metadata":{"id":"9tn5xnfnuehY"},"source":["L'encodeur a pour but de créer des représentations cachées des séries exogènes qui prennent en compte les relations spatiales entre ces séries ainsi que les relations temporelles.  \n","Les relations spatiales sont extraitent à l'aide d'un ensemble de réseaux de convolution qui produisent des représentations w1, w2... w(T-1).  \n","Ces représentations sont ensuites codées par un réseau RHN à 3 couches afin d'en extraire les relations temporelles. En sortie de ce réseau RHN, on extrait 3 tenseurs dont chacun contient les (T-1) états cachés de chaque couche du réseau RHN."]},{"cell_type":"markdown","metadata":{"id":"I0mq5BSauQUq"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"]},{"cell_type":"markdown","metadata":{"id":"IJ-boowSGDp3"},"source":["***a. Création des CNN parallèlisés***"]},{"cell_type":"markdown","metadata":{"id":"s6dF5Cp9vzWB"},"source":["La structure d'un réseau de convolution est composée de trois couches CNN-1D + Max-pooling :"]},{"cell_type":"markdown","metadata":{"id":"jga5_ZzKv6CI"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN1.png?raw=true'>"]},{"cell_type":"markdown","metadata":{"id":"0CMXl2tgwJ76"},"source":["L'intégration de caque réseau dans Keras est parallélisée :"]},{"cell_type":"markdown","metadata":{"id":"gtR-u7ZqwjTL"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_CNN2.png?raw=true'>"]},{"cell_type":"code","metadata":{"id":"5ETqqvAIGKLi"},"source":["# Arguments de la méthode __init__\n","#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n","#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n","#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n","\n","class Encodeur_CNN(tf.keras.layers.Layer):\n","  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling,dim_motif):\n","    self.dim_filtres_cnn = dim_filtres_cnn\n","    self.nbr_filtres_cnn = nbr_filtres_cnn\n","    self.dim_max_pooling = dim_max_pooling\n","    self.dim_motif = dim_motif\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  # Création de Tin réseaux de convolution + max_pooling en //\n","  ############################################################\n","  def build(self,input_shape):\n","    convs = []\n","    input_cnns = []\n","\n","    # Création des Tin entrées des réseaux CNN\n","    for i in range(input_shape[1]):\n","        input_cnns.append(tf.keras.Input(shape=(input_shape[2],1)))       # input = Tin*(batch_size,#dim,1)\n","\n","    # Création des Tin réseaux CNN\n","    for i in range(input_shape[1]):\n","      conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[0],      # conv : (batch_size,#dim,16)\n","                                    kernel_size=self.dim_filtres_cnn[0],\n","                                    activation='relu',\n","                                    padding='same',\n","                                    strides=1)(input_cnns[i])\n","      conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[0],      # conv : (batch_size,#pooling1,16)\n","                                       padding='same')(conv)\n","      for n in range(1,len(self.dim_filtres_cnn)):\n","        conv = tf.keras.layers.Conv1D(filters=self.nbr_filtres_cnn[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n","                                      kernel_size=self.dim_filtres_cnn[n],\n","                                      activation='relu',\n","                                      padding='same',\n","                                      strides=1)(conv)\n","        conv = tf.keras.layers.MaxPool1D(pool_size=self.dim_max_pooling[n],    # conv : (batch_size,#pooling_x,dim_filtres_cnn[n])\n","                                         padding='same')(conv)\n","      convs.append(conv)\n","    \n","    # Création de la sortie concaténée des Tin réseaux CNN\n","    out = tf.convert_to_tensor(convs)                                     # out : (Tin,batch_size,#pooling,64)\n","    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,Tin,#pooling,64)\n","    out = tf.keras.layers.Reshape(                                        # out : (batch_size,Tin,#pooling*64)\n","        target_shape=(out.shape[1],out.shape[2]*out.shape[3]))(out)\n","\n","    if self.dim_motif == 0:\n","      out = tf.keras.layers.Dense(units=out.shape[2])(out)                  # out : (batch_size,Tin,dim_motif = #pooling*64) \n","    else:\n","      out = tf.keras.layers.Dense(units=self.dim_motif)(out)                # out : (batch_size,Tin,dim_motif) \n","\n","    # Création du modèle global\n","    self.conv_model = tf.keras.Model(inputs=input_cnns,outputs=out)\n","\n","    super().build(input_shape)        # Appel de la méthode build()\n","    \n","  # Entrées :\n","  #     input:  Entrée séries exogènes  : (batch_size,Tin,#dim)\n","  # Sorties :\n","  #     w:      Sorties des motifs CNN  : (batch_size,Tin,#dim_motif)\n","  #                                       (taille dernier filtre=64)\n","  def call(self, input):\n","    # Coupes temporelles sur les séries exogènes\n","    # au format : Tin*(batch_size,#dim,1)\n","    input_list = []\n","    for i in range(input.shape[1]):\n","      input_list.append(tf.transpose(input[:,i:i+1,:],perm=[0,2,1]))      # (batch_size,#dim,1)\n","    # Convolutions spatiales des séries exogènes\n","    w = self.conv_model(input_list)                                       # (batch_size,Tin,dim_motif)\n","    return w"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_y68mmiRpR1"},"source":["***b. Création des cellules RHN***"]},{"cell_type":"markdown","metadata":{"id":"2FQ47zOsxHpx"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_RHN.png?raw=true'>"]},{"cell_type":"markdown","metadata":{"id":"lvNrS-wQ1B5C"},"source":["On crée une cellule RHN en reprenant le code précédent auquel :  \n","- On ajoute la possibilité de retourner tous les états cachés de chaque couche\n","- On ajoute la prise en compte de la dimension d'entrée correspondant à la dimension des motifs en sortie des réseaux CNN (dim_motif)"]},{"cell_type":"markdown","metadata":{"id":"CygD9DXbBTDJ"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Structure_RHN4.png?raw=true'>"]},{"cell_type":"code","metadata":{"id":"9HldCwl9z3fY"},"source":["class Cellule_RHN(tf.keras.layers.Layer):\n","  def __init__(self, dim_RHN, nbr_couches, return_all_states = False, dim_input=1):\n","    self.dim_RHN = dim_RHN\n","    self.nbr_couches = nbr_couches\n","    self.dim_input = dim_input\n","    self.return_all_states = return_all_states\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  def build(self,input_shape):\n","    self.Wh = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wh\")       # (#dim, #RHN)\n","    self.Wt = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wt\")       # (#dim, #RHN)\n","    self.Wc = self.add_weight(shape=(input_shape[2],self.dim_RHN),initializer=\"normal\",name=\"Wc\")       # (#dim, #RHN)\n","\n","    self.Rh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rh\")      # (n_couches,#RHN, #RHN)\n","    self.Rt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rt\")      # (n_couches,#RHN, #RHN)\n","    self.Rc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,self.dim_RHN),initializer=\"normal\",name=\"Rc\")      # (n_couches,#RHN, #RHN)\n","\n","    self.bh = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bh\")        # (n_couches,#RHN, 1)\n","    self.bt = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bt\")        # (n_couches,#RHN, 1)\n","    self.bc = self.add_weight(shape=(self.nbr_couches,self.dim_RHN,1),initializer=\"normal\",name=\"bc\")        # (n_couches,#RHN, 1)\n","\n","    super().build(input_shape)        # Appel de la méthode build()\n","\n","    # Initialisation des masques de dropout\n","  def InitMasquesDropout(self,drop=0.0):\n","    self.Wh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n","    self.Wt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n","    self.Wc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.dim_input,1)),dtype=tf.float32)                 # (#dim,1)\n","    self.Rh_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n","    self.Rt_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n","    self.Rc_ = tf.convert_to_tensor(np.random.binomial(n=1,p=1.0-drop,size=(self.nbr_couches,self.dim_RHN,1)),dtype=tf.float32)  # (n_couches,#RHN,1)\n","\n","  # Entrées :\n","  #     input:          Entrées X[t]        : (batch_size,1,#dim)\n","  #     init_hidden:    Etat caché Init.    : (batch_size,#RHN)\n","  # Sorties :\n","  #     sL:             Etat caché de la dernière couche       : (batch_size,#RHN) \n","  #           ou        Etats cachés de chaque couche SL[t]    : (batch_size,nbr_couches,#RHN)\n","  def call(self, input, init_hidden=None):\n","    # Construction d'un vecteur d'état nul si besoin\n","    if init_hidden == None:\n","      init_hidden = tf.matmul(tf.zeros(shape=(self.dim_RHN,input.shape[2])), # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n","                              tf.transpose(input,perm=[0,2,1]))\n","      init_hidden = tf.squeeze(init_hidden,-1)                               # (batch_size,#RHN,1) => (batch_size,#RHN)\n","  \n","    liste_sl = []                                                            # Liste pour  enregistrer les états cachés de chaque couche\n","    # Calcul de hl, tl et cl\n","    for i in range(self.nbr_couches):\n","      if i==0:\n","        # Applique le masque aux poids\n","        Rh = tf.multiply(self.Rh_[0,:,:],self.Rh[0,:,:])                      # (#RHN,1)_x_(#RHN,#RHN) = (#RHN,#RHN)\n","        Rt = tf.multiply(self.Rt_[0,:,:],self.Rt[0,:,:])\n","        Rc = tf.multiply(self.Rc_[0,:,:],self.Rc[0,:,:])\n","\n","        Wh = tf.multiply(self.Wh_,self.Wh)                                    # (#dim,1)_x_(#dim,#RHN) = (#dim,#RHN)\n","        Wt = tf.multiply(self.Wt_,self.Wt)\n","        Wc = tf.multiply(self.Wc_,self.Wc)\n","   \n","        # Calcul de hl\n","        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n","        hl = hl + self.bh[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n","        hl = hl + tf.matmul(tf.transpose(Wh),\n","                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n","        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n","        hl = K.tanh(hl)\n","\n","        # Calcul de tl\n","        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n","        tl = tl + self.bt[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n","        tl = tl + tf.matmul(tf.transpose(Wt),\n","                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n","        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n","        tl = tf.keras.activations.sigmoid(tl)\n","\n","        # Calcul de cl\n","        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n","        cl = cl + self.bc[0,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n","        cl = cl + tf.matmul(tf.transpose(Wc),\n","                            tf.transpose(input,perm=[0,2,1]))               # (#RHN,#dim)X(batch_size,#dim,1) = (batch_size,#RHN,1)\n","        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n","        cl = tf.keras.activations.sigmoid(cl)\n","\n","      else:\n","        # Applique le masque aux poids\n","        Rh = tf.multiply(self.Rh_[i,:,:],self.Rh[i,:,:])\n","        Rt = tf.multiply(self.Rt_[i,:,:],self.Rt[i,:,:])\n","        Rc = tf.multiply(self.Rc_[i,:,:],self.Rc[i,:,:])\n","\n","        # Calcul de hl\n","        hl = tf.matmul(Rh,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n","        hl = hl + self.bh[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n","        hl = tf.squeeze(hl,-1)                                              # (batch_size,#RHN)\n","        hl = K.tanh(hl)\n","\n","        # Calcul de tl\n","        tl = tf.matmul(Rt,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n","        tl = tl + self.bt[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n","        tl = tf.squeeze(tl,-1)                                              # (batch_size,#RHN)\n","        tl = tf.keras.activations.sigmoid(tl)\n","\n","        # Calcul de cl\n","        cl = tf.matmul(Rc,tf.expand_dims(init_hidden,-1))                   # (#RHN,#RHN)X(batch_size,#RHN,1) = (batch_size,#RHN,1)\n","        cl = cl + self.bc[i,:,:]                                            # (batch_size,#RHN,1) + (#RHN,1) = (batch_size,#RHN,1)\n","        cl = tf.squeeze(cl,-1)                                              # (batch_size,#RHN)\n","        cl = tf.keras.activations.sigmoid(cl)\n","      \n","      # Calcul de sl\n","      sl = tf.keras.layers.multiply([hl,tl])                                # (batch_size,#RHN)\n","      sl = sl + tf.keras.layers.multiply([init_hidden,cl])                  # (batch_size,#RHN)\n","      liste_sl.append(sl)       # Sauvegarde l'état caché de la couche courante\n","      init_hidden = sl\n","    if self.return_all_states == False:\n","      return sl\n","    else:\n","      liste_sl = tf.convert_to_tensor(liste_sl)                             # (nbr_couches,batch_size,#RHN)\n","      liste_sl = tf.transpose(liste_sl,perm=[1,0,2])                        # (batch_size,nbr_couches,#RHN)\n","      return liste_sl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJY-TY7W55ZF"},"source":["***c. Création de l'encodeur : Convolutions + RHN***"]},{"cell_type":"markdown","metadata":{"id":"wMT8C8-UVe2O"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_Encodeur_VueEnsemble.png?raw=true'>"]},{"cell_type":"code","metadata":{"id":"HYQu67IfBdel"},"source":["# Arguments de la méthode __init__\n","#   dim_filtres_cnn   :   liste dimension des filtres ex: [3,3,3]\n","#   nbr_filtres_cnn   :   liste nbr de filtre sur chaque couche ex: [16,32,64]\n","#   dim_max_pooling   :   liste dimension max pooling après chaque couche ex: [3,3,3]\n","#   dim_motif         :   dimension du motif en sortie du CNN\n","#   dim_RHN_enc       :   dimension du vecteur caché RHN\n","#   nbr_couches_RHN   :   nombre de couches du RHN\n","#   dropout           :   dropout variationnel pour le RHN ex: [0.1]\n","\n","class Encodeur(tf.keras.layers.Layer):\n","  def __init__(self, dim_filtres_cnn, nbr_filtres_cnn, dim_max_pooling, dim_motif,dim_RHN_enc,nbr_couches_RHN, dropout=0.0):\n","    self.dim_filtres_cnn = dim_filtres_cnn\n","    self.nbr_filtres_cnn = nbr_filtres_cnn\n","    self.dim_max_pooling = dim_max_pooling\n","    self.dim_motif = dim_motif\n","    self.dim_RHN_enc = dim_RHN_enc\n","    self.nbr_couches_RHN = nbr_couches_RHN\n","    self.dropout = dropout\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  def build(self,input_shape):\n","    self.encodeur_cnn = Encodeur_CNN(dim_filtres_cnn=self.dim_filtres_cnn,nbr_filtres_cnn=self.nbr_filtres_cnn,dim_max_pooling=self.dim_max_pooling,dim_motif=self.dim_motif)\n","    self.RHN = Cellule_RHN(dim_RHN=self.dim_RHN_enc,nbr_couches=self.nbr_couches_RHN,return_all_states=True,dim_input=self.dim_motif)\n","    super().build(input_shape)        # Appel de la méthode build()\n","    \n","  # Entrées :\n","  #     input:          Entrées X         : (batch_size,Tin,#dim)\n","  # Sorties :\n","  #     hidden_states   Vecteurs cachés   : (batch_size,nbr_couches,Tin,#RHN)\n","  def call(self, input):\n","    # Convolutions spatiales des séries exogènes\n","    w = self.encodeur_cnn(input)      #  (batch_size,Tin,dim_motif)\n","\n","    # Encodage des motifs CNN avec les cellules RHN\n","    sequence = []\n","    hidden = None\n","\n","    # Initialisation des masques de dropout pour tous les pas de temps\n","    self.RHN.InitMasquesDropout(self.dropout)\n","\n","    # Applique la cellule RHN à chaque pas de temps\n","    for i in range(input.shape[1]):\n","      hidden = self.RHN(w[:,i:i+1,:],hidden)          # Envoie (batch_size,1,dim_motif)\n","      sequence.append(hidden)                         # Sauve (batch_size,nbr_couches,#RHN)\n","\n","      # Le premier état caché du prochain instant\n","      # est l'état caché de la dernière couche précédente\n","      hidden = hidden[:,self.nbr_couches_RHN-1,:]       # (batch_size,#RHN)\n","\n","    # Traite le format des vecteurs cachés de l'encodeur\n","    sequence = tf.convert_to_tensor(sequence)               # (Tin,batch_size,nbr_couches,#RHN)\n","    hidden_states = tf.transpose(sequence,perm=[1,2,0,3])   # (batch_size,nbr_couches,Tin,#RHN)  \n","\n","    return hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__CJ4O7yJne3"},"source":["**2. Création du décodeur**"]},{"cell_type":"markdown","metadata":{"id":"Lt2yWQeaJwNn"},"source":["Le décodeur prend en entrée et à chaque pas de temps :  \n","- Le tenseur en sortie de l'encodeur RHN qui contient l'ensemble des vecteurs cachés des différentes couches : (batch_size,Nbr_couches,Tin,#RHN)\n","- L'état caché de la dernière couche du décodeur RHN précédent : (batch_size,#RHN)\n","- La valeur de la série cible à l'instant courant : (batch_size,1,1)"]},{"cell_type":"markdown","metadata":{"id":"DtYLxAoIK8Xn"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_VueEnsembleDecodeur2.png?raw=true'>"]},{"cell_type":"markdown","metadata":{"id":"vjHiRZZbLief"},"source":["**a. Création de la couche d'attention hiérarchique**"]},{"cell_type":"markdown","metadata":{"id":"9JX5hGeWNN8w"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/HRHN_AttentionHierarchique.png?raw=true'>"]},{"cell_type":"markdown","metadata":{"id":"l9p7ylHmY6gS"},"source":["On commence par créer la fonction permettant de calculer les scores. Cette fonction sera appelée avec la méthode TimeDistributed de Keras."]},{"cell_type":"code","metadata":{"id":"MvaGAb0uY1XL"},"source":["class CalculScore(tf.keras.layers.Layer):\n","  def __init__(self,dim_RHN_dec):\n","    self.dim_RHN_dec = dim_RHN_dec\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  def build(self,input_shape):\n","    self.T = self.add_weight(shape=(input_shape[1],self.dim_RHN_dec),initializer=\"normal\",name=\"T\")   # (#RHN_enc, #RHN_dec)\n","    self.U = self.add_weight(shape=(input_shape[1],input_shape[1]),initializer=\"normal\",name=\"U\")     # (#RHN_enc, #RHN_enc)\n","    self.b = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"b\")                  # (#RHN_enc, 1)\n","    self.v = self.add_weight(shape=(input_shape[1],1),initializer=\"normal\",name=\"v\")                  # (#RHN_enc, 1)\n","    super().build(input_shape)        # Appel de la méthode build()\n","\n","  #     hid_state:  Etat initial RHN          : (batch_size,#RHN_dec)\n","  def SetInitState(self,hid_state):\n","    self.hid_state = hid_state\n","\n","  def compute_output_shape(self,input_shape):\n","    return(input_shape[0],1)\n","\n","  # Entrées :\n","  #     input:      1 sortie encodeur RHN     : (batch_size,#RHN_enc)\n","  # Sorties :\n","  #     score:      score                     : (batch_size,1,1)\n","  def call(self, input):\n","    score = tf.matmul(self.U,tf.expand_dims(input,-1))                      # (#RHN_enc,#RHN_enc)x(batch_size,#RHN_enc,1) = (batch_size,#RHN_enc,1)\n","    score = score + tf.matmul(self.T,tf.expand_dims(self.hid_state,-1))     # (#RHN_enc,#RHN_dec)(batch_size,#RHN_dec,1) = (batch_size,#RHN_enc,1)\n","    score = score + self.b                                                  # (batch_size,#RHN_enc,1)\n","    score = K.tanh(score)\n","    score = tf.matmul(tf.transpose(self.v),score)                           # (1,#RHN_enc)x(batch_size,#RHN_enc,1) = (batch_size,1,1)\n","    return tf.squeeze(score,-1)                                             # (batch_size,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pF02ysdbxWU"},"source":["On crée maintenant la couche d'attention hiérarchique :"]},{"cell_type":"code","metadata":{"id":"8kxnR9fSVXDC"},"source":["class AttentionHierarchique(tf.keras.layers.Layer):\n","  def __init__(self,dim_RHN_dec):\n","    self.dim_RHN_dec = dim_RHN_dec\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  def build(self,input_shape):\n","    self.couche_score = CalculScore(dim_RHN_dec=self.dim_RHN_dec)\n","    super().build(input_shape)        # Appel de la méthode build()\n","    \n","  # Entrées :\n","  #     input:      Sorties d'une couche encodeur RHN       : (batch_size,Tin,#RHN_enc)\n","  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN_dec)\n","  # Sorties :\n","  #     vc:         SousVecteur contexte                    : (batch_size,1,RHN_enc)\n","  def call(self, input, hid_state):\n","    # Calcul des scores\n","    self.couche_score.SetInitState(hid_state)\n","    scores = tf.keras.layers.TimeDistributed(self.couche_score)(input)        # (batch_size,Tin,#RHN_enc) : Timestep = Tin\n","                                                                              # (batch_size,#RHN_enc) envoyé Tin fois\n","                                                                              # (batch_size,Tin,1) retourné\n","    scores = tf.keras.activations.softmax(scores,axis=1)                      # (batch_size,Tin,1)\n","\n","    # Applique les scores aux sorties de la couche RHN\n","    poids = tf.multiply(input,scores)             # (batch_size,Tin,#RHN_enc)_x_(batch_size,Tin,1) = (batch_size,Tin,#RHN_enc)\n","\n","    # Calcul le sous-vecteur contexte\n","    vc = K.sum(poids,axis=1)                      # (batch_size,#RHN_enc)\n","    return tf.expand_dims(vc,1)                   # (batch_size,1,#RHN_enc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slCSUTmyifEY"},"source":["**b. Création du décodeur**"]},{"cell_type":"markdown","metadata":{"id":"l_efbOikfRwt"},"source":["Dans le décodeur, on parallélise autant de couches d'attention que nécessaire afin de créer un modèle d'attention multi-entrées."]},{"cell_type":"markdown","metadata":{"id":"wCElCxBcnUHj"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/ParaDecodeur.png?raw=true'>"]},{"cell_type":"code","metadata":{"id":"k2nZG3Rrjv3O"},"source":["class Decodeur(tf.keras.layers.Layer):\n","  def __init__(self,dim_RHN_dec,nbr_couches_RHN,dropout=0.0):\n","    self.dim_RHN_dec = dim_RHN_dec\n","    self.nbr_couches_RHN = nbr_couches_RHN\n","    self.dropout = dropout\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  def build(self,input_shape):\n","    attentions = []\n","    inputs_attention = []\n","\n","    # Création des \"nbr_couches\" entrées des attentions\n","    # Chaque entrée est une liste : [input,init_state] = [((batch_size,Tin,#RHN_enc)),((batch_size,#RHN_dec))]\n","    for i in range(input_shape[1]):\n","      inputs_attention.append([tf.keras.Input(shape=(input_shape[2],input_shape[3])),          # input = \"nbr_couches\"*(batch_size,Tin,#RHN_enc)\n","                                 tf.keras.Input(shape=(self.dim_RHN_dec))])                    # init_state = \"nbr_couches\"*(batch_size,#RHN_dec)\n","\n","    # Création des \"nbr_couches\" couches d'attentions hierarchiques\n","    for i in range(input_shape[1]):\n","      att = AttentionHierarchique(dim_RHN_dec=self.dim_RHN_dec)(\n","          inputs_attention[i][0],                 # inputs_attention[i][0] : (batch_size,Tin,#RHN_enc)\n","          inputs_attention[i][1])                 # inputs_attention[i][1] : (batch_size,#RHN_dec)\n","      attentions.append(att)\n","\n","    # Création de la sortie concaténée des \"nbr_couches\" couches d'attentions\n","    out = tf.convert_to_tensor(attentions)                                # out : (nbr_couches,batch_size,1,#RHN_enc)\n","    out = tf.transpose(out,perm=[1,0,2,3])                                # out : (batch_size,nbr_couches,1,#RHN_enc)\n","\n","    # Création du modèle global\n","    self.att_model = tf.keras.Model(inputs=inputs_attention,outputs=out)\n","\n","    # Création des poids\n","    self.Wtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n","    self.Vtilda = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n","\n","    # Création du décodeur RHN\n","    self.dec_RHN = Cellule_RHN(dim_RHN=self.dim_RHN_dec,nbr_couches=self.nbr_couches_RHN,return_all_states=False,dim_input=1)\n","   \n","    super().build(input_shape)        # Appel de la méthode build()\n","    \n","  # Entrées :\n","  #     input:      Sorties des couches de l'encodeur RHN   : (batch_size,nbr_couches,Tin,#RHN_enc)\n","  #     hid_state:  Etat initial RHN                        : (batch_size,#RHN_dec)\n","  #     Y:          Valeur de la série cible                : (batch_size,1)\n","  #     only_att    Si =True ne calcul que le vecteur ctx   : True/False\n","  # Sorties :\n","  #     d:          Vecteur contexte                        : (batch_size,nbr_couches*RHN_enc)\n","  #     s:          Vecteur caché décodeur RHN              : (batch_size,#RHN_dec)\n","  def call(self, input, hid_state, Y,only_att):\n","    # Initialisation de l'état caché à 0 si besoin\n","    # Construit le tenseur nul au format (batch_size,#RHN)\n","    if hid_state == None:\n","      coef = tf.expand_dims(input[:,0,0,0],-1)                          # (batch_size,1)\n","      coef = tf.expand_dims(coef,-1)                                    # (batch_size,1,1)\n","      hid_state = tf.matmul(coef,tf.zeros(shape=(1,self.dim_RHN_dec)))  # (batch_size,1,1)X(1,#RHN_dec) = (batch_size,1,#RHN_dec)\n","      hid_state = tf.squeeze(hid_state,axis=1)                          # (batch_size,#RHN_dec)\n","\n","    # Construction de l'entrée du modèle\n","    # nbr_couches*[((batch_size,Tin,#RHN_enc)),((batch_size,#RHN_dec))]\n","    input_model = []\n","    for i in range(input.shape[1]):\n","      input_model.append([input[:,i,:,:],hid_state])    # [((batch_size,Tin,#RHN_enc)),((batch_size,#RHN_dec))]\n","    \n","    # Calcul des sous-vecteurs contextes\n","    # avec le modèle d'attention hiérarchique parallélisé\n","    d = self.att_model(input_model)                     # d : (batch_size,nbr_couches,1,#RHN_enc)\n","\n","    # Concaténation des sous-vecteurs contextes\n","    d = tf.squeeze(d,axis=2)                            # (batch_size,nbr_couches,#RHN_enc)\n","    d = tf.keras.layers.Flatten()(d)                    # (batch_size,nbr_couches*RHN_enc)\n","\n","    if only_att == False :\n","      # Calcul de y_tilda\n","      ytilda = self.Wtilda(Y)                             # (batch_size,1)\n","      ytilda = ytilda + self.Vtilda(d)                    # (batch_size,1)\n","\n","      # Initialisation des masques de dropout pour tous les pas de temps\n","      self.dec_RHN.InitMasquesDropout(self.dropout)\n","\n","      # Décodage avec le réseau RHN\n","      s = self.dec_RHN(tf.expand_dims(ytilda,-1),hid_state)                  # (batch_size,#RHN_dec)\n","      return d,s\n","    else:\n","      return d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UYOTdM7fZT65"},"source":["**3. Création de la couche HRHN**"]},{"cell_type":"markdown","metadata":{"id":"UhML2b5bFsZB"},"source":["<img src='https://github.com/AlexandreBourrieau/FICHIERS/blob/main/Series_Temporelles/Multi/images/Gene_HRHN.png?raw=true'>"]},{"cell_type":"code","metadata":{"id":"8PCEHUDEZ1bt"},"source":["class Net_HRHN(tf.keras.layers.Layer):\n","  def __init__(self,encodeur,decodeur,longueur_sequence, regul=0.0, drop = 0.0):\n","    self.encodeur = encodeur\n","    self.decodeur = decodeur\n","    self.longueur_sequence = longueur_sequence\n","    self.regul = regul\n","    self.drop = drop\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  def build(self,input_shape):\n","    self.W = tf.keras.layers.Dense(units=1,activation=None,use_bias=None)\n","    self.V = tf.keras.layers.Dense(units=1,activation=None,use_bias=True)\n","    super().build(input_shape)        # Appel de la méthode build()\n","\n","  # Entrées :\n","  #     input:          Entrées X           : (batch_size,Tin,#dim)\n","  #     output_seq:     Sortie séquence Y   : (batch_size,Tin,1)\n","  # Sorties :\n","  #     sortie:         Prédiction Y        : (batch_size,1,1)\n","  def call(self,input,output_seq):\n","    # Appel de l'encodeur\n","    # Récupère l'ensemble des états cachés de l'encodeur RHN\n","    H = self.encodeur(input)                # (batch_size,nbr_couches,Tin,#RHN_enc)\n","\n","    # Décodage\n","    hidden_state = None\n","    for t in range(input.shape[1]):\n","      vc, hidden_state = self.decodeur(H,hidden_state,output_seq[:,t:t+1,0],only_att = False)\n","    \n","    # Couche d'attention finale\n","    vc = self.decodeur(H,hidden_state,output_seq[:,0,0],only_att=True)    # (batch_size,#RHN_dec)\n","\n","    # Génération de la prédiction\n","    sortie = self.W(hidden_state) + self.V(vc)        # (batch_size,1)\n","    return tf.expand_dims(sortie,-1)                  # (batch_size,1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I8_PgjEpdC8z"},"source":["#Mise en place du modèle HRHN ENC/DEC"]},{"cell_type":"code","metadata":{"id":"sFrdcJiSRbS-"},"source":["def HRHN_model(config):\n","  dim_cnn_ = config['dim_filtres_cnn']\n","  nbr_cnn_ = config['nbr_filtres_cnn']\n","  max_pool_ = config['max_pooling_cnn']\n","\n","  dim_cnn=[]\n","  nbr_cnn=[]\n","  max_pool=[]\n","  val = dim_cnn_[0].split(',')\n","  for c in val:\n","    dim_cnn.append(int(c))\n","    \n","  val = nbr_cnn_[0].split(',')\n","  for c in val:\n","    nbr_cnn.append(int(c))\n","\n","  val = max_pool_[0].split(',')\n","  for c in val:\n","    max_pool.append(int(c))\n","\n","\n","  entrees_sequences = tf.keras.layers.Input(shape=(config['longueur_sequence'],x_train[0].shape[2]))\n","  sorties_sequence = tf.keras.layers.Input(shape=(config['longueur_sequence'],1))\n","\n","  dim_motif = Encodeur_CNN(dim_filtres_cnn=dim_cnn,nbr_filtres_cnn=nbr_cnn,dim_max_pooling=max_pool,dim_motif=0)(x_train[0][0:1,:,:]).shape[2]\n","\n","  encodeur = Encodeur(dim_filtres_cnn=dim_cnn,nbr_filtres_cnn=nbr_cnn,dim_max_pooling=max_pool,dim_motif=dim_motif,dim_RHN_enc=config['dim_RHN_enc'],nbr_couches_RHN=config['nbr_couches_RHN_enc'],dropout=config['drop'])\n","  decodeur = Decodeur(dim_RHN_dec=config['dim_RHN_dec'],nbr_couches_RHN=config['nbr_couches_RHN_dec'],dropout=config['drop'])\n","\n","  sortie = Net_HRHN(encodeur,decodeur,longueur_sequence=config['longueur_sequence'],drop=config['drop'])(entrees_sequences,sorties_sequence)\n","\n","  model = tf.keras.Model([entrees_sequences,sorties_sequence],sortie)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uEWxDydrYgzi"},"source":["# Ajout du correcteur"]},{"cell_type":"markdown","metadata":{"id":"zQtt8UB0WhQj"},"source":["**1. Création du correcteur PID**"]},{"cell_type":"code","metadata":{"id":"qofproTuWmlW"},"source":["class Correcteur_PID(tf.keras.layers.Layer):\n","  def __init__(self,batch_size,config):\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","    self.erreur_1 = tf.Variable(shape=(batch_size,1,1),trainable=False,initial_value=tf.zeros(shape=(batch_size,1,1)),name=\"erreur_1\")               # (BS,1,1)\n","    self.num_iteration = tf.Variable(shape=(batch_size,1,1),trainable=False,initial_value=tf.ones(shape=(batch_size,1,1)),name=\"iteration\")          # (BS,1,1)\n","    self.config = config\n","  \n","  def build(self,input_shape):\n","    self.Couche_Dense_P = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Ones(),name=\"CoucheP\")\n","    self.Couche_Dense_I = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Zeros(),name=\"CoucheI\")\n","    self.Couche_Dense_D = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Zeros(),name=\"CoucheD\")\n","    self.Couche_Dense = tf.keras.layers.Dense(units=1,use_bias=False,kernel_initializer=tf.keras.initializers.Ones(),trainable=False,activation=self.config[\"couche\"],name=\"CouchePID\")\n","    super().build(input_shape)        # Appel de la méthode build()\n","  \n","  # Entrées\n","  #     y_reel :    Valeur cible réelle à (t-1)   :   (batch_size,1,1)\n","  #     y_pred :    Prédiction à (t-1)            :   (batch_size,1,1)\n","  # Sorties\n","  #     PID(erreur) : Erreur traitée par PID      :   (batch_size,1,1)\n","  def call(self, y_reel,y_pred):\n","    # Calcul de l'erreur courante\n","    erreur = y_reel-y_pred                                                        # (batch_size,1,1)\n","\n","    # Calcul de la partie proportionnelle\n","    P = self.Couche_Dense_P(erreur)                                               # (batch_size,1,1)\n","\n","    # Calcul de la partie intégrale\n","    I = (1/self.num_iteration)*self.Couche_Dense_I(self.erreur_1+erreur)          # (batch_size,1,1)\n","\n","    # Calcul de la partie dérivée\n","    D = self.Couche_Dense_D(erreur-self.erreur_1)                                 # (batch_size,1,1)\n","    \n","    # Sauvegarde des erreurs et #itérations\n","    self.num_iteration.assign(self.num_iteration+1)                               # (batch_size,1,1)\n","    self.erreur_1.assign(erreur + self.erreur_1)                                  # (batch_size,1,1)\n","\n","    return self.Couche_Dense(tf.keras.layers.Concatenate(axis=2)([P,I,D]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8TEN31tZbfu"},"source":["**2. Couche du prédicteur**"]},{"cell_type":"code","metadata":{"id":"-ouU_2jLZbfu"},"source":["class Predicteur(tf.keras.layers.Layer):\n","  def __init__(self,predicteur):\n","    self.predicteur = predicteur\n","    super().__init__()                # Appel du __init__() de la classe Layer\n","  \n","  def build(self,input_shape):\n","    super().build(input_shape)        # Appel de la méthode build()\n","  \n","  # input:    input[0] :  Séries exogènes     : (batch_size,Tin,#dim)\n","  #           input[1] :  Cible               : (batch_size,Tin,1)\n","  def call(self, input):\n","    return self.predicteur(input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rh5MO6QCZbfv"},"source":["#Création du modèle global"]},{"cell_type":"code","metadata":{"id":"NKu1vn97Zbfv"},"source":["class CustomModel(keras.Model):\n","    def __init__(self,predicteur,correcteur,batch_size):\n","      super(CustomModel, self).__init__()\n","      self.predicteur = predicteur\n","      self.Correcteur = correcteur\n","      self.y_pred_ent_1 = tf.Variable(shape=(batch_size,1,1),trainable=False,initial_value=tf.zeros(shape=(batch_size,1,1)),name=\"ypred_1\")          # (BS,1, 1)\n","      self.y_pred_ent_1.assign(tf.zeros(shape=(batch_size,1,1)))\n","\n","    # inputs:   inputs[0] : Séries exogènes     : (Tin,#dim)\n","    #           inputs[1] : Cible               : (Tin,1)\n","    def call(self, inputs, training=True):\n","      y_pred = self.predicteur([inputs[0],inputs[1]])\n","      y_1 = inputs[1][:,-1:,:]                                # Vraie valeur précédente : (batch_size,1,1)\n","      y_pred_ent_1 = self.y_pred_ent_1\n","      offset = self.Correcteur(y_1,y_pred_ent_1)              # Calcul de l'offset : (batch_size,1,1)\n","      y_pred = y_pred + offset                                # Ajoute l'offset à la prédiction\n","      self.y_pred_ent_1.assign(y_pred)                        # Enregistre la prédiction\n","      return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GdSoHUDnkpgu"},"source":["# Configuration de l'optimiseur"]},{"cell_type":"markdown","metadata":{"id":"gQdgXWaTkx0w"},"source":["**1. Espace des hyperparamètres**"]},{"cell_type":"code","metadata":{"id":"gWSesAqRoH22"},"source":["liste_dim_filtres_cnn = [\"1\",\"2\",\"3\",\"4\"]\n","liste_nbr_filtres_cnn = [\"2\",\"4\",\"8\",\"16\",\"32\",\"64\",\"128\",\"256\"]\n","liste_rapport_max_pooling_cnn = [\"1\",\"2\",\"3\",\"4\"]\n","\n","def create_filtres_x2():\n","  dim_filtres_cnn_x2 = []\n","  nbr_filtres_cnn_x2 = []\n","  liste_rapport_max_pooling_cnn_x2 = []\n","\n","  for i in liste_dim_filtres_cnn:\n","    for j in liste_dim_filtres_cnn:\n","      dim_filtres_cnn_x2.append([i,j])\n","  for i in liste_nbr_filtres_cnn:\n","    for j in liste_nbr_filtres_cnn:\n","      nbr_filtres_cnn_x2.append([i,j])\n","  for i in liste_rapport_max_pooling_cnn:\n","    for j in liste_rapport_max_pooling_cnn:\n","      liste_rapport_max_pooling_cnn_x2.append([i,j])\n","  \n","  return dim_filtres_cnn_x2,nbr_filtres_cnn_x2,liste_rapport_max_pooling_cnn_x2\n","\n","def create_filtres_x3():\n","  dim_filtres_cnn_x3 = []\n","  nbr_filtres_cnn_x3 = []\n","  liste_rapport_max_pooling_cnn_x3 = []\n","\n","  for i in liste_dim_filtres_cnn:\n","    for j in liste_dim_filtres_cnn:\n","      for k in liste_dim_filtres_cnn:\n","        dim_filtres_cnn_x3.append([i,j,k])\n","  for i in liste_nbr_filtres_cnn:\n","    for j in liste_nbr_filtres_cnn:\n","      for k in liste_nbr_filtres_cnn:\n","        nbr_filtres_cnn_x3.append([i,j,k])\n","  for i in liste_rapport_max_pooling_cnn:\n","    for j in liste_rapport_max_pooling_cnn:\n","      for k in liste_rapport_max_pooling_cnn:\n","        liste_rapport_max_pooling_cnn_x3.append([i,j,k])\n","  \n","  return dim_filtres_cnn_x3,nbr_filtres_cnn_x3,liste_rapport_max_pooling_cnn_x3\n","\n","def create_liste_filtres():\n","  all_dim_filtres_cnn = [[],[]]\n","  all_nbr_filtres_cnn = [[],[]]\n","  all_max_pooling_cnn = [[],[]]\n","\n","  liste_dim_filtres_x2,liste_nbr_filtres_x2,liste_rapport_max_pooling_cnn_x2 = create_filtres_x2()\n","  liste_dim_filtres_x3,liste_nbr_filtres_x3,liste_rapport_max_pooling_cnn_x3 = create_filtres_x3()\n","\n","  for i in liste_dim_filtres_x2:\n","      all_dim_filtres_cnn[0].append(i)\n","\n","  for i in liste_dim_filtres_x3:\n","      all_dim_filtres_cnn[1].append(i)\n","\n","  for i in liste_nbr_filtres_x2:\n","      all_nbr_filtres_cnn[0].append(i)\n","\n","  for i in liste_nbr_filtres_x3:\n","      all_nbr_filtres_cnn[1].append(i)\n","\n","  for i in liste_rapport_max_pooling_cnn_x2:\n","      all_max_pooling_cnn[0].append(i)\n","\n","  for i in liste_rapport_max_pooling_cnn_x3:\n","      all_max_pooling_cnn[1].append(i)\n","\n","\n","  return all_dim_filtres_cnn,all_nbr_filtres_cnn,all_max_pooling_cnn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnsY-xbvaOCp"},"source":["all_dim_filtres_cnn = [[],[]]\n","all_nbr_filtres_cnn = [[],[]]\n","all_max_pooling_cnn = [[],[]]\n","\n","\n","all_dim_filtres_cnn_, all_nbr_filtres_cnn_ , all_max_pooling_cnn_= create_liste_filtres()\n","\n","val_str = \"\"\n","\n","for i in all_dim_filtres_cnn_[0]:\n","  for j in i:\n","    val_str = val_str+str(j)+\",\"\n","  val_str = val_str[:-1]\n","  all_dim_filtres_cnn[0].append(val_str)\n","  val_str = \"\"\n","\n","for i in all_nbr_filtres_cnn_[0]:\n","  for j in i:\n","    val_str = val_str+j+\",\"\n","  val_str = val_str[:-1]\n","  all_nbr_filtres_cnn[0].append(val_str)\n","  val_str = \"\"\n","\n","for i in all_dim_filtres_cnn_[1]:\n","  for j in i:\n","    val_str = val_str+str(j)+\",\"\n","  val_str = val_str[:-1]\n","  all_dim_filtres_cnn[1].append(val_str)\n","  val_str = \"\"\n","\n","for i in all_nbr_filtres_cnn_[1]:\n","  for j in i:\n","    val_str = val_str+j+\",\"\n","  val_str = val_str[:-1]\n","  all_nbr_filtres_cnn[1].append(val_str)\n","  val_str = \"\"\n","\n","for i in all_max_pooling_cnn_[0]:\n","  for j in i:\n","    val_str = val_str+str(j)+\",\"\n","  val_str = val_str[:-1]\n","  all_max_pooling_cnn[0].append(val_str)\n","  val_str = \"\"\n","\n","for i in all_max_pooling_cnn_[1]:\n","  for j in i:\n","    val_str = val_str+str(j)+\",\"\n","  val_str = val_str[:-1]\n","  all_max_pooling_cnn[1].append(val_str)\n","  val_str = \"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8r8pgY-FBm8l"},"source":["from ray import tune\n","\n","def create_search_space():\n","  config = {\n","      \"longueur_sequence\": tune.choice([5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,90,100]),\n","      \"dim_filtres_cnn\": tune.choice(all_dim_filtres_cnn[1][:]),\n","      \"nbr_filtres_cnn\": tune.choice(all_nbr_filtres_cnn[1][:]),\n","      \"max_pooling_cnn\": tune.choice(all_max_pooling_cnn[1][:]),\n","      \"dim_RHN_enc\": tune.choice([16,32,64,128,256]),\n","      \"dim_RHN_dec\": tune.choice([16,32,64,128,256]),\n","      \"nbr_couches_RHN_enc\": tune.choice([1,2,3,4]),\n","      \"nbr_couches_RHN_dec\": tune.choice([1,2,3,4]),\n","      \"drop\": tune.choice([0.0,0.01,0.1,0.3,0.6]),\n","      \"batch_size\": tune.choice([128,256,512]),\n","      \"lr\": tune.loguniform(1e-4, 1e-1),\n","      \"couche\": tune.choice([\"tanh\",\"relu\",\"linear\"])\n","      }\n","  \n","  initial_best_config = [{\n","      \"longueur_sequence\":10,\n","      \"dim_filtres_cnn\": \"3,3,3\",\n","      \"nbr_filtres_cnn\": \"16,32,64\",\n","      \"max_pooling_cnn\": \"3,3,3\",\n","      \"dim_RHN_enc\": 128,\n","      \"dim_RHN_dec\": 128,\n","      \"nbr_couches_RHN_enc\": 3,\n","      \"nbr_couches_RHN_dec\": 3,\n","      \"drop\": 0.0,\n","      \"batch_size\": 128,\n","      \"lr\": 1e-3,\n","      \"couche\": \"tanh\"\n","      }]\n","\n","  return config,initial_best_config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylc12PPtPGIv"},"source":["class TuneReporter(tf.keras.callbacks.Callback):\n","    def __init__(self, reporter=None, freq=\"batch\", logs=None):\n","        self.iteration = 0\n","        logs = logs or {}\n","        if freq not in [\"batch\", \"epoch\"]:\n","            raise ValueError(\"{} not supported as a frequency.\".format(freq))\n","        self.freq = freq\n","        super(TuneReporter, self).__init__()\n","\n","    def on_batch_end(self, batch, logs=None):\n","        from ray import tune\n","        logs = logs or {}\n","        if not self.freq == \"batch\":\n","            return\n","        self.iteration += 1\n","        for metric in list(logs):\n","            if \"loss\" in metric and \"neg_\" not in metric:\n","                logs[\"neg_\" + metric] = -logs[metric]\n","        if \"acc\" in logs:\n","            tune.report(keras_info=logs, mean_accuracy=logs[\"acc\"])\n","        else:\n","            tune.report(keras_info=logs, mean_accuracy=logs.get(\"accuracy\"))\n","    \n","    def on_epoch_end(self, batch, logs=None):\n","        from ray import tune\n","        logs = logs or {}\n","        if not self.freq == \"epoch\":\n","            return\n","        self.iteration += 1\n","        for metric in list(logs):\n","            if \"loss\" in metric and \"neg_\" not in metric:\n","                logs[\"neg_\" + metric] = -logs[metric]\n","        if \"acc\" in logs:\n","            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs[\"acc\"])\n","        else:\n","            tune.report(keras_info=logs, val_loss=logs['val_loss'], mean_accuracy=logs.get(\"accuracy\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1llAfcqwOjpR"},"source":["def create_callbacks():\n","    callbacks = []\n","    tune_reporter = TuneReporter(freq=\"epoch\")\n","    callbacks.append(tune_reporter)\n","    return callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLyOgPctj76d"},"source":["#def train_model(config,checkpoint_dir=None):\n","def train_model(config):\n","  dataset = prepare_dataset_XY(serie_entrainement_X_norm[:,0:-1],serie_entrainement_X_norm[:,-1:], config['longueur_sequence'],longueur_sortie,config['batch_size'],shift)\n","  dataset_val = prepare_dataset_XY(serie_test_X_norm[:,0:-1],serie_test_X_norm[:,-1:],config['longueur_sequence'],longueur_sortie,config['batch_size'],shift)\n","  \n","  x_train, y_train = Create_train(dataset)\n","  x_val, y_val = Create_train(dataset_val)\n","\n","  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n","  tf.config.experimental_connect_to_cluster(resolver)\n","  tf.tpu.experimental.initialize_tpu_system(resolver)\n","  strategy = tf.distribute.TPUStrategy(resolver)\n","\n","  with strategy.scope():\n","    model_HRHN = HRHN_model(config)\n","    model_HRHN.build(input_shape=([(int(config['batch_size']/8),config['longueur_sequence'],x_train[0].shape[2]),(int(config['batch_size']/8),config['longueur_sequence'],1)]))\n","    model_HRHN.trainable = True\n","    predicteur = Predicteur(model_HRHN)\n","    correcteur = Correcteur_PID(int(config['batch_size']/8),config)\n","    model = CustomModel(predicteur,correcteur,int(config['batch_size']/8))\n","    model.build(input_shape=([(int(config['batch_size']/8),config['longueur_sequence'],x_train[0].shape[2]),(int(config['batch_size']/8),config['longueur_sequence'],1)]))\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config['lr']),loss='mse')\n","  \n","  callbacks = create_callbacks()\n","  history = model.fit(x=[x_train[0],x_train[1]],y=y_train,epochs=1000,callbacks=callbacks,validation_data=([x_val[0],x_val[1]],y_val),batch_size=config['batch_size'])\n","  return history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRDR07YI2rQG"},"source":["from ray.tune import Callback\n","import ftplib\n","\n","class SendFileToFTP(Callback):\n","  def on_trial_complete(self,iteration,trials,trial,**info):\n","    ftp= ftplib.FTP()\n","    HOST = \"62.210.208.36\"\n","    PORT = 2122\n","    ftp.connect(HOST,PORT)\n","    ftp.login('rdpdo','passamoi290876')\n","    os.system(\"zip -r /content/ray_results/RayTuneHRHN-PID_SP500.zip /content/ray_results\")\n","    localfile = \"/content/ray_results/RayTuneHRHN-PID_SP500.zip\"\n","    remotefile = \"RayTuneHRHN-PID_SP500.zip\"\n","    with open(localfile,\"rb\") as file:\n","      ftp.storbinary('STOR %s' %remotefile,file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXnzuTH0uiZU"},"source":["# Téléchargement des résultats précédents"]},{"cell_type":"code","metadata":{"id":"IHhMs-fd_ZKW"},"source":["train_dir = os.path.abspath(\"ray_results/train_dir\")\n","val_dir = os.path.abspath(\"ray_results/val_dir\")\n","checkpoint_dir = os.path.abspath(\"ray_results/chackpoint_dir\")\n","results_dir = os.path.abspath(\"ray_results\")\n","\n","!rm -r \"/content/ray_results\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZ6alY_dul3L"},"source":["import ftplib\n","\n","ftp= ftplib.FTP()\n","HOST = \"62.210.208.36\"\n","PORT = 2122\n","ftp.connect(HOST,PORT)\n","ftp.login('rdpdo','passamoi290876')\n","remotefile = \"/home/rdpdo/RayTuneHRHN-PID_SP500.zip\"\n","ftp.retrbinary('RETR %s' %remotefile, open('/content/RayTuneHRHN-PID_SP500.zip',\"wb\").write)\n","os.system(\"unzip /content/RayTuneHRHN-PID_SP500.zip -d /\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yl-0TUJeloJC"},"source":["# Lancement"]},{"cell_type":"code","metadata":{"id":"S6b5eCeMorVe"},"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/ray_results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KiRfpaXTB_YM"},"source":["import ray\n","import os\n","from ray.tune.schedulers import AsyncHyperBandScheduler\n","from ray.tune.suggest import ConcurrencyLimiter\n","from ray.tune.suggest.hyperopt import HyperOptSearch\n","\n","ray.init(configure_logging=False,ignore_reinit_error=True)\n","config, initial_best_config = create_search_space()\n","\n","scheduler = AsyncHyperBandScheduler(time_attr='training_iteration',metric=\"val_loss\",mode=\"min\",grace_period=500,max_t=1000)\n","search_alg = HyperOptSearch(metric=\"val_loss\",mode=\"min\",points_to_evaluate=initial_best_config)\n","search_alg = ConcurrencyLimiter(search_alg, max_concurrent=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FRa7aHc7lvV-"},"source":["analysis = tune.run(train_model, \n","                    local_dir=results_dir,\n","                    name=\"HRHN-PID_SP500\",\n","                    verbose=0,\n","                    num_samples=5000,\n","                    scheduler=scheduler,\n","                    search_alg=search_alg,\n","                    raise_on_failed_trial=False,\n","                    resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n","                    config=config,\n","                    checkpoint_freq = 1,\n","                    checkpoint_at_end=True,\n","                    resume=True,\n","                    callbacks=[SendFileToFTP()])\n"],"execution_count":null,"outputs":[]}]}